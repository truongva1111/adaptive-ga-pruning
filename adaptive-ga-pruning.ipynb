{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af4321d",
   "metadata": {},
   "source": [
    "# Layer-wise Adaptive Structured Pruning via Genetic Algorithms with Taylor-based Proxy Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27896b7b",
   "metadata": {},
   "source": [
    "## 1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229b19ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.10/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch) (59.6.0)\n",
      "Requirement already satisfied: numpy in /home/vdlung/.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /home/vdlung/.local/lib/python3.10/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/vdlung/.local/lib/python3.10/site-packages (from opencv-python) (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/vdlung/.local/lib/python3.10/site-packages (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch_pruning in /home/vdlung/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (2.8.0)\n",
      "Requirement already satisfied: numpy in /home/vdlung/.local/lib/python3.10/site-packages (from torch_pruning) (2.2.6)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (12.8.93)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (3.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (11.3.3.83)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (0.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (3.19.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (2.27.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (4.15.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (12.5.8.93)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (12.8.4.1)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->torch_pruning) (11.7.3.90)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch>=2.0->torch_pruning) (59.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.0->torch_pruning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->torch_pruning) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio  # PyTorch\n",
    "%pip install opencv-python                 # OpenCV\n",
    "%pip install numpy                         # NumPy\n",
    "%pip install --upgrade torch_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54316c4e",
   "metadata": {},
   "source": [
    "## 2. Setup VGG16 + Config + Common function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd510ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import VGG16_Weights\n",
    "import torch_pruning as tp\n",
    "from operator import itemgetter\n",
    "from heapq import nsmallest, nlargest\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dataset_class = 'CIFAR10'  # or 'CIFAR100'\n",
    "    classifier_type = 'C'  # A, B, or C\n",
    "    batch_size = 128\n",
    "    num_workers = 2\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    prune_method = 'min'  # 'min', 'max', or 'avg'\n",
    "    prune_local = False  # Per-layer or global pruning\n",
    "    best_acc = 0\n",
    "\n",
    "\n",
    "# ==================== Utility Functions ====================\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds to human-readable string\"\"\"\n",
    "    days = int(seconds / 86400)\n",
    "    seconds %= 86400\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds %= 3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds %= 60\n",
    "\n",
    "    parts = []\n",
    "    if days > 0: parts.append(f\"{days}D\")\n",
    "    if hours > 0: parts.append(f\"{hours}h\")\n",
    "    if minutes > 0: parts.append(f\"{minutes}m\")\n",
    "    if seconds > 0: parts.append(f\"{int(seconds)}s\")\n",
    "\n",
    "    return ''.join(parts) if parts else '0s'\n",
    "\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    \"\"\"Display progress bar\"\"\"\n",
    "    bar_length = 50\n",
    "    progress = current / total\n",
    "    filled = int(bar_length * progress)\n",
    "\n",
    "    bar = '=' * filled + '>' + '.' * (bar_length - filled - 1)\n",
    "    status = f\"\\r[{bar}] {current + 1}/{total}\"\n",
    "\n",
    "    if msg:\n",
    "        status += f\" | {msg}\"\n",
    "\n",
    "    sys.stdout.write(status)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if current >= total - 1:\n",
    "        print()\n",
    "\n",
    "\n",
    "# ==================== Layer Replacement ====================\n",
    "def replace_layers(model, i, indexes, layers):\n",
    "    \"\"\"Replace layers at specified indexes\"\"\"\n",
    "    if i in indexes:\n",
    "        return layers[indexes.index(i)]\n",
    "    return model[i]\n",
    "\n",
    "\n",
    "# ==================== VGG16 Pruning Function ====================\n",
    "def prune_vgg16_conv_layer(model, layer_index, filter_index, use_cuda=False):\n",
    "    \"\"\"\n",
    "    Prune a single filter from a convolutional layer in VGG16\n",
    "\n",
    "    Args:\n",
    "        model: VGG16 model\n",
    "        layer_index: Index of conv layer to prune\n",
    "        filter_index: Index of filter to remove\n",
    "        use_cuda: Whether to use CUDA\n",
    "\n",
    "    Returns:\n",
    "        Modified model with pruned filter\n",
    "    \"\"\"\n",
    "    # Get current conv layer\n",
    "    _, conv = list(model.features._modules.items())[layer_index]\n",
    "\n",
    "    # Find next conv layer\n",
    "    next_conv = None\n",
    "    offset = 1\n",
    "    while layer_index + offset < len(model.features._modules.items()):\n",
    "        res = list(model.features._modules.items())[layer_index + offset]\n",
    "        if isinstance(res[1], nn.Conv2d):\n",
    "            _, next_conv = res\n",
    "            break\n",
    "        offset += 1\n",
    "\n",
    "    # Create new conv layer with one less output channel\n",
    "    new_conv = nn.Conv2d(\n",
    "        in_channels=conv.in_channels,\n",
    "        out_channels=conv.out_channels - 1,\n",
    "        kernel_size=conv.kernel_size,\n",
    "        stride=conv.stride,\n",
    "        padding=conv.padding,\n",
    "        dilation=conv.dilation,\n",
    "        groups=conv.groups,\n",
    "        bias=(conv.bias is not None)\n",
    "    )\n",
    "\n",
    "    # Copy weights, excluding the pruned filter\n",
    "    old_weights = conv.weight.data.cpu().numpy()\n",
    "    new_weights = new_conv.weight.data.cpu().numpy()\n",
    "\n",
    "    new_weights[:filter_index, :, :, :] = old_weights[:filter_index, :, :, :]\n",
    "    new_weights[filter_index:, :, :, :] = old_weights[filter_index + 1:, :, :, :]\n",
    "\n",
    "    new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "    if use_cuda:\n",
    "        new_conv.weight.data = new_conv.weight.data.cuda()\n",
    "\n",
    "    # Copy bias\n",
    "    if conv.bias is not None:\n",
    "        bias_numpy = conv.bias.data.cpu().numpy()\n",
    "        bias = np.concatenate([bias_numpy[:filter_index], bias_numpy[filter_index + 1:]])\n",
    "        new_conv.bias.data = torch.from_numpy(bias)\n",
    "        if use_cuda:\n",
    "            new_conv.bias.data = new_conv.bias.data.cuda()\n",
    "\n",
    "    # Update next conv layer if it exists\n",
    "    if next_conv is not None:\n",
    "        next_new_conv = nn.Conv2d(\n",
    "            in_channels=next_conv.in_channels - 1,\n",
    "            out_channels=next_conv.out_channels,\n",
    "            kernel_size=next_conv.kernel_size,\n",
    "            stride=next_conv.stride,\n",
    "            padding=next_conv.padding,\n",
    "            dilation=next_conv.dilation,\n",
    "            groups=next_conv.groups,\n",
    "            bias=(next_conv.bias is not None)\n",
    "        )\n",
    "\n",
    "        old_weights = next_conv.weight.data.cpu().numpy()\n",
    "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
    "\n",
    "        new_weights[:, :filter_index, :, :] = old_weights[:, :filter_index, :, :]\n",
    "        new_weights[:, filter_index:, :, :] = old_weights[:, filter_index + 1:, :, :]\n",
    "\n",
    "        next_new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "        if use_cuda:\n",
    "            next_new_conv.weight.data = next_new_conv.weight.data.cuda()\n",
    "\n",
    "        next_new_conv.bias.data = next_conv.bias.data\n",
    "\n",
    "        # Replace both layers\n",
    "        features = nn.Sequential(\n",
    "            *(replace_layers(model.features, i, [layer_index, layer_index + offset],\n",
    "                           [new_conv, next_new_conv])\n",
    "              for i, _ in enumerate(model.features))\n",
    "        )\n",
    "        model.features = features\n",
    "    else:\n",
    "        # Pruning last conv layer - update classifier\n",
    "        model.features = nn.Sequential(\n",
    "            *(replace_layers(model.features, i, [layer_index], [new_conv])\n",
    "              for i, _ in enumerate(model.features))\n",
    "        )\n",
    "\n",
    "        # Update first linear layer in classifier\n",
    "        layer_index = 0\n",
    "        old_linear_layer = None\n",
    "        for _, module in model.classifier._modules.items():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                old_linear_layer = module\n",
    "                break\n",
    "            layer_index += 1\n",
    "\n",
    "        if old_linear_layer is None:\n",
    "            raise ValueError(\"No linear layer found in classifier\")\n",
    "\n",
    "        params_per_input_channel = old_linear_layer.in_features // conv.out_channels\n",
    "\n",
    "        new_linear_layer = nn.Linear(\n",
    "            old_linear_layer.in_features - params_per_input_channel,\n",
    "            old_linear_layer.out_features\n",
    "        )\n",
    "\n",
    "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
    "        new_weights = new_linear_layer.weight.data.cpu().numpy()\n",
    "\n",
    "        new_weights[:, :filter_index * params_per_input_channel] = \\\n",
    "            old_weights[:, :filter_index * params_per_input_channel]\n",
    "        new_weights[:, filter_index * params_per_input_channel:] = \\\n",
    "            old_weights[:, (filter_index + 1) * params_per_input_channel:]\n",
    "\n",
    "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
    "        new_linear_layer.weight.data = torch.from_numpy(new_weights)\n",
    "\n",
    "        if use_cuda:\n",
    "            new_linear_layer.weight.data = new_linear_layer.weight.data.cuda()\n",
    "\n",
    "        classifier = nn.Sequential(\n",
    "            *(replace_layers(model.classifier, i, [layer_index], [new_linear_layer])\n",
    "              for i, _ in enumerate(model.classifier))\n",
    "        )\n",
    "        model.classifier = classifier\n",
    "\n",
    "    return model\n",
    "\n",
    "def prune_vgg16_conv_layer_BN(model, layer_index, filter_index, use_cuda=False):\n",
    "    # Get current conv layer\n",
    "    _, conv = list(model.features._modules.items())[layer_index]\n",
    "\n",
    "    # Find next conv layer and next batchnorm\n",
    "    next_conv = None\n",
    "    next_bn = None\n",
    "    offset = 1\n",
    "    while layer_index + offset < len(model.features._modules.items()):\n",
    "        res = list(model.features._modules.items())[layer_index + offset]\n",
    "        if isinstance(res[1], nn.Conv2d) and next_conv is None:\n",
    "            _, next_conv = res\n",
    "        if isinstance(res[1], nn.BatchNorm2d) and next_bn is None:\n",
    "            _, next_bn = res\n",
    "        if next_conv and next_bn:\n",
    "            break\n",
    "        offset += 1\n",
    "\n",
    "    # Create new conv layer with one less output channel\n",
    "    new_conv = nn.Conv2d(\n",
    "        in_channels=conv.in_channels,\n",
    "        out_channels=conv.out_channels - 1,\n",
    "        kernel_size=conv.kernel_size,\n",
    "        stride=conv.stride,\n",
    "        padding=conv.padding,\n",
    "        dilation=conv.dilation,\n",
    "        groups=conv.groups,\n",
    "        bias=(conv.bias is not None)\n",
    "    )\n",
    "    # Copy weights, excluding the pruned filter\n",
    "    old_weights = conv.weight.data.cpu().numpy()\n",
    "    new_weights = new_conv.weight.data.cpu().numpy()\n",
    "    new_weights[:filter_index, :, :, :] = old_weights[:filter_index, :, :, :]\n",
    "    new_weights[filter_index:, :, :, :] = old_weights[filter_index + 1:, :, :, :]\n",
    "    new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "    if use_cuda:\n",
    "        new_conv.weight.data = new_conv.weight.data.cuda()\n",
    "    # Copy bias\n",
    "    if conv.bias is not None:\n",
    "        bias_numpy = conv.bias.data.cpu().numpy()\n",
    "        bias = np.concatenate([bias_numpy[:filter_index], bias_numpy[filter_index + 1:]])\n",
    "        new_conv.bias.data = torch.from_numpy(bias)\n",
    "        if use_cuda:\n",
    "            new_conv.bias.data = new_conv.bias.data.cuda()\n",
    "\n",
    "    # === Prune BatchNorm2d ngay sau Conv2d ===\n",
    "    # TÃ¬m BatchNorm2d ngay sau Conv2d (náº¿u cÃ³)\n",
    "    bn_idx = layer_index + 1\n",
    "    while bn_idx < len(model.features):\n",
    "        if isinstance(model.features[bn_idx], nn.BatchNorm2d):\n",
    "            break\n",
    "        bn_idx += 1\n",
    "    if bn_idx < len(model.features) and isinstance(model.features[bn_idx], nn.BatchNorm2d):\n",
    "        old_bn = model.features[bn_idx]\n",
    "        new_bn = nn.BatchNorm2d(conv.out_channels - 1)\n",
    "        # Copy weights/bias/running stats, bá» filter_index\n",
    "        keep = list(range(conv.out_channels))\n",
    "        keep.pop(filter_index)\n",
    "        new_bn.weight.data = old_bn.weight.data[keep].clone()\n",
    "        new_bn.bias.data = old_bn.bias.data[keep].clone()\n",
    "        new_bn.running_mean = old_bn.running_mean[keep].clone()\n",
    "        new_bn.running_var = old_bn.running_var[keep].clone()\n",
    "        if use_cuda:\n",
    "            new_bn.weight.data = new_bn.weight.data.cuda()\n",
    "            new_bn.bias.data = new_bn.bias.data.cuda()\n",
    "            new_bn.running_mean = new_bn.running_mean.cuda()\n",
    "            new_bn.running_var = new_bn.running_var.cuda()\n",
    "        # Thay tháº¿ Conv2d vÃ  BatchNorm2d\n",
    "        features = list(model.features)\n",
    "        features[layer_index] = new_conv\n",
    "        features[bn_idx] = new_bn\n",
    "        model.features = nn.Sequential(*features)\n",
    "    else:\n",
    "        # KhÃ´ng cÃ³ BatchNorm ngay sau, chá»‰ thay Conv2d\n",
    "        features = list(model.features)\n",
    "        features[layer_index] = new_conv\n",
    "        model.features = nn.Sequential(*features)\n",
    "\n",
    "    # Update next conv layer if it exists\n",
    "    if next_conv is not None:\n",
    "        next_new_conv = nn.Conv2d(\n",
    "            in_channels=next_conv.in_channels - 1,\n",
    "            out_channels=next_conv.out_channels,\n",
    "            kernel_size=next_conv.kernel_size,\n",
    "            stride=next_conv.stride,\n",
    "            padding=next_conv.padding,\n",
    "            dilation=next_conv.dilation,\n",
    "            groups=next_conv.groups,\n",
    "            bias=(next_conv.bias is not None)\n",
    "        )\n",
    "\n",
    "        old_weights = next_conv.weight.data.cpu().numpy()\n",
    "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
    "\n",
    "        new_weights[:, :filter_index, :, :] = old_weights[:, :filter_index, :, :]\n",
    "        new_weights[:, filter_index:, :, :] = old_weights[:, filter_index + 1:, :, :]\n",
    "\n",
    "        next_new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "        if use_cuda:\n",
    "            next_new_conv.weight.data = next_new_conv.weight.data.cuda()\n",
    "\n",
    "        next_new_conv.bias.data = next_conv.bias.data\n",
    "\n",
    "        # Replace both layers\n",
    "        features = nn.Sequential(\n",
    "            *(replace_layers(model.features, i, [layer_index, layer_index + offset],\n",
    "                           [new_conv, next_new_conv])\n",
    "              for i, _ in enumerate(model.features))\n",
    "        )\n",
    "        model.features = features\n",
    "    else:\n",
    "        # Pruning last conv layer - update classifier\n",
    "        model.features = nn.Sequential(\n",
    "            *(replace_layers(model.features, i, [layer_index], [new_conv])\n",
    "              for i, _ in enumerate(model.features))\n",
    "        )\n",
    "\n",
    "        # Update first linear layer in classifier\n",
    "        layer_index = 0\n",
    "        old_linear_layer = None\n",
    "        for _, module in model.classifier._modules.items():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                old_linear_layer = module\n",
    "                break\n",
    "            layer_index += 1\n",
    "\n",
    "        if old_linear_layer is None:\n",
    "            raise ValueError(\"No linear layer found in classifier\")\n",
    "\n",
    "        params_per_input_channel = old_linear_layer.in_features // conv.out_channels\n",
    "\n",
    "        new_linear_layer = nn.Linear(\n",
    "            old_linear_layer.in_features - params_per_input_channel,\n",
    "            old_linear_layer.out_features\n",
    "        )\n",
    "\n",
    "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
    "        new_weights = new_linear_layer.weight.data.cpu().numpy()\n",
    "\n",
    "        new_weights[:, :filter_index * params_per_input_channel] = \\\n",
    "            old_weights[:, :filter_index * params_per_input_channel]\n",
    "        new_weights[:, filter_index * params_per_input_channel:] = \\\n",
    "            old_weights[:, (filter_index + 1) * params_per_input_channel:]\n",
    "\n",
    "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
    "        new_linear_layer.weight.data = torch.from_numpy(new_weights)\n",
    "\n",
    "        if use_cuda:\n",
    "            new_linear_layer.weight.data = new_linear_layer.weight.data.cuda()\n",
    "\n",
    "        classifier = nn.Sequential(\n",
    "            *(replace_layers(model.classifier, i, [layer_index], [new_linear_layer])\n",
    "              for i, _ in enumerate(model.classifier))\n",
    "        )\n",
    "        model.classifier = classifier\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "# ==================== Modified VGG16 Model ====================\n",
    "class ModifiedVGG16Model(nn.Module):\n",
    "    \"\"\"VGG16 with custom classifier for CIFAR datasets\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ModifiedVGG16Model, self).__init__()\n",
    "\n",
    "        # Load pretrained VGG16\n",
    "        model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(\n",
    "            # 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 5\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # 6\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # 7\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 8\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # 9\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # 10\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 11\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # 12\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # 13\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "        # # Freeze feature layers initially\n",
    "        # for param in self.features.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # Determine number of classes\n",
    "        num_classes = 10 if config.dataset_class == 'CIFAR10' else 100\n",
    "\n",
    "        # Create classifier based on type\n",
    "        if config.classifier_type == 'A':\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        elif config.classifier_type == 'B':\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Linear(512, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, num_classes)\n",
    "            )\n",
    "        elif config.classifier_type == 'C':\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Linear(25088, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid classifier type: {config.classifier_type}\")\n",
    "\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ==================== Filter Pruner ====================\n",
    "class FilterPruner:\n",
    "    \"\"\"Ranks filters based on Taylor expansion importance\"\"\"\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset filter rankings\"\"\"\n",
    "        self.filter_ranks = {}\n",
    "        self.activations = []\n",
    "        self.gradients = []\n",
    "        self.grad_index = 0\n",
    "        self.activation_to_layer = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with activation recording\"\"\"\n",
    "        self.activations = []\n",
    "        self.activation_to_layer = {}\n",
    "        self.grad_index = 0  # reset má»—i láº§n forward\n",
    "        activation_index = 0\n",
    "\n",
    "        for layer, (name, module) in enumerate(self.model.features._modules.items()):\n",
    "            x = module(x)\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                x.register_hook(self.compute_rank)\n",
    "                self.activations.append(x)\n",
    "                self.activation_to_layer[activation_index] = layer\n",
    "                activation_index += 1\n",
    "\n",
    "        return self.model.classifier(x.view(x.size(0), -1))\n",
    "\n",
    "    def compute_rank(self, grad):\n",
    "        \"\"\"Compute Taylor importance score\"\"\"\n",
    "        if len(self.activations) == 0:\n",
    "            print(\"âš ï¸ Warning: No activations collected. Skipping Taylor importance computation.\")\n",
    "            return\n",
    "    \n",
    "        activation_index = len(self.activations) - self.grad_index - 1\n",
    "        \n",
    "        # Äáº£m báº£o khÃ´ng vÆ°á»£t giá»›i háº¡n\n",
    "        if activation_index < 0 or activation_index >= len(self.activations):\n",
    "            print(f\"âš ï¸ Skipping invalid activation index: {activation_index} / {len(self.activations)}\")\n",
    "            return\n",
    "        \n",
    "        activation = self.activations[activation_index]\n",
    "\n",
    "        # Taylor expansion: importance = activation * gradient\n",
    "        taylor = activation * grad\n",
    "        taylor = taylor.mean(dim=(0, 2, 3)).data\n",
    "\n",
    "        if activation_index not in self.filter_ranks:\n",
    "            self.filter_ranks[activation_index] = torch.zeros(activation.size(1))\n",
    "            if self.config.use_cuda:\n",
    "                self.filter_ranks[activation_index] = self.filter_ranks[activation_index].to(self.config.device)\n",
    "\n",
    "        self.filter_ranks[activation_index] += taylor\n",
    "        self.grad_index += 1\n",
    "\n",
    "    def normalize_ranks_per_layer(self):\n",
    "        \"\"\"L2 normalize filter ranks per layer\"\"\"\n",
    "        for i in self.filter_ranks:\n",
    "            v = torch.abs(self.filter_ranks[i])\n",
    "            v_cpu = v.cpu()\n",
    "            norm = torch.sqrt(torch.sum(v_cpu * v_cpu))\n",
    "            if norm != 0:\n",
    "                v_cpu = v_cpu / norm\n",
    "            self.filter_ranks[i] = v_cpu\n",
    "\n",
    "    def get_pruning_plan(self, num_filters_to_prune):\n",
    "        \"\"\"Get list of (layer, filter) pairs to prune\"\"\"\n",
    "        # Collect filter data\n",
    "        data = []\n",
    "        for i in sorted(self.filter_ranks.keys()):\n",
    "            for j in range(self.filter_ranks[i].size(0)):\n",
    "                data.append((\n",
    "                    self.activation_to_layer[i],\n",
    "                    j,\n",
    "                    self.filter_ranks[i][j]\n",
    "                ))\n",
    "\n",
    "        # Select filters based on pruning method\n",
    "        if self.config.prune_method == 'min':\n",
    "            filters_to_prune = nsmallest(num_filters_to_prune, data, itemgetter(2))\n",
    "        elif self.config.prune_method == 'max':\n",
    "            filters_to_prune = nlargest(num_filters_to_prune, data, itemgetter(2))\n",
    "        elif self.config.prune_method == 'avg':\n",
    "            data_sorted = sorted(data, key=itemgetter(2))\n",
    "            mid = len(data_sorted) // 2\n",
    "            start = max(0, mid - num_filters_to_prune // 2)\n",
    "            end = min(len(data_sorted), start + num_filters_to_prune)\n",
    "            filters_to_prune = data_sorted[start:end]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid prune method: {self.config.prune_method}\")\n",
    "\n",
    "        # Organize by layer and adjust indices\n",
    "        filters_per_layer = {}\n",
    "        for layer, filt, _ in filters_to_prune:\n",
    "            if layer not in filters_per_layer:\n",
    "                filters_per_layer[layer] = []\n",
    "            filters_per_layer[layer].append(filt)\n",
    "\n",
    "        # Sort and adjust for sequential pruning\n",
    "        for layer in filters_per_layer:\n",
    "            filters_per_layer[layer].sort()\n",
    "            for i in range(len(filters_per_layer[layer])):\n",
    "                filters_per_layer[layer][i] -= i\n",
    "\n",
    "        # Flatten to list of tuples\n",
    "        result = []\n",
    "        for layer in filters_per_layer:\n",
    "            for filt in filters_per_layer[layer]:\n",
    "                result.append((layer, filt))\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# ==================== Main Pruning Class ====================\n",
    "class VGG16Pruner:\n",
    "    \"\"\"Main class for VGG16 pruning and fine-tuning\"\"\"\n",
    "\n",
    "    def __init__(self, config, model, save_name='ckpt.pth'):\n",
    "        self.config = config\n",
    "        self.save_name = save_name\n",
    "        self.setup_data_loaders()\n",
    "        self.model = model.to(config.device)\n",
    "\n",
    "        if config.device == 'cuda':\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.pruner = FilterPruner(self.model, config)\n",
    "\n",
    "    def setup_data_loaders(self):\n",
    "        \"\"\"Setup CIFAR data loaders\"\"\"\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "        if self.config.classifier_type == 'C':\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        else:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "\n",
    "        # Load dataset\n",
    "        dataset_fn = (torchvision.datasets.CIFAR10 if self.config.dataset_class == 'CIFAR10'\n",
    "                     else torchvision.datasets.CIFAR100)\n",
    "\n",
    "        trainset = dataset_fn(root='./data', train=True, download=True, transform=transform_train)\n",
    "        testset = dataset_fn(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            trainset, batch_size=self.config.batch_size,\n",
    "            shuffle=True, num_workers=self.config.num_workers\n",
    "        )\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            testset, batch_size=self.config.batch_size,\n",
    "            shuffle=False, num_workers=self.config.num_workers\n",
    "        )\n",
    "\n",
    "    def train(self, optimizer=None, epoches=10):\n",
    "        if optimizer is None:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        for i in range(epoches):\n",
    "            print(\"Epoch: \", i)\n",
    "            self.train_epoch(optimizer)\n",
    "            self.test(i)\n",
    "            scheduler.step()\n",
    "        print(\"Finished fine tuning.\")\n",
    "\n",
    "    def train_epoch(self, optimizer=None, rank_filters=False):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "            inputs, targets = inputs.to(self.config.device), targets.to(self.config.device)\n",
    "\n",
    "            if rank_filters:\n",
    "                output = self.pruner.forward(inputs)\n",
    "                self.criterion(output, targets).backward()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                progress_bar(batch_idx, len(self.train_loader),\n",
    "                           f'Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}%')\n",
    "\n",
    "    def test(self, epoch):\n",
    "        \"\"\"Test model accuracy\"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.test_loader):\n",
    "                inputs, targets = inputs.to(self.config.device), targets.to(self.config.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                progress_bar(batch_idx, len(self.test_loader),\n",
    "                           f'Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}%')\n",
    "\n",
    "        acc = 100. * correct / total\n",
    "        if acc > self.config.best_acc:\n",
    "            print('\\nSaving checkpoint...')\n",
    "            os.makedirs('checkpoint', exist_ok=True)\n",
    "            torch.save({\n",
    "                'model': self.model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch\n",
    "            }, f'./checkpoint/{self.save_name}')\n",
    "            self.config.best_acc = acc\n",
    "\n",
    "        return acc\n",
    "    \n",
    "    def reShapeAfterPrune(self, input_size):\n",
    "        \"\"\"Reshape model after pruning\"\"\"\n",
    "        # Update model's features after pruning\n",
    "        self.model.features = nn.Sequential(*list(self.model.features.children()))\n",
    "\n",
    "        # === Rebuild classifier if needed ===\n",
    "        # Láº¥y sá»‘ channel output cuá»‘i cÃ¹ng sau khi prune\n",
    "        with torch.no_grad():\n",
    "            dummy_input = input_size\n",
    "            if isinstance(dummy_input, torch.Tensor):\n",
    "                dummy_input = dummy_input.to(self.config.device)\n",
    "            feat = self.model.features(dummy_input)\n",
    "            feat_shape = feat.shape  # (batch, channels, H, W)\n",
    "            new_in_features = feat_shape[1] * feat_shape[2] * feat_shape[3]\n",
    "\n",
    "        # Táº¡o láº¡i classifier phÃ¹ há»£p vá»›i loáº¡i báº¡n chá»n\n",
    "        num_classes = 10 if self.config.dataset_class == 'CIFAR10' else 100\n",
    "        if self.config.classifier_type == 'A':\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Linear(feat_shape[1], 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        elif self.config.classifier_type == 'B':\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Linear(feat_shape[1], 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, num_classes)\n",
    "            )\n",
    "        elif self.config.classifier_type == 'C':\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Linear(new_in_features, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid classifier type: {self.config.classifier_type}\")\n",
    "\n",
    "        self.model.classifier = classifier.to(self.config.device)\n",
    "\n",
    "    def prune_all_in_one(self, prune_targets, input_size):\n",
    "        # Gom cÃ¡c filter cáº§n prune theo tá»«ng layer\n",
    "        filters_per_layer = {}\n",
    "        for layer_idx, filt_idx in prune_targets:\n",
    "            if layer_idx not in filters_per_layer:\n",
    "                filters_per_layer[layer_idx] = []\n",
    "            filters_per_layer[layer_idx].append(filt_idx)\n",
    "\n",
    "        print(\"\\nPruning filters using TorchPruning (batch per layer)...\")\n",
    "        self.model = self.model.to(self.config.device)\n",
    "\n",
    "        for layer_idx, filt_list in filters_per_layer.items():\n",
    "            print(f\"ðŸŸ¢ layer {layer_idx}, filters {filt_list}\")\n",
    "            DG = tp.DependencyGraph().build_dependency(self.model, example_inputs=input_size)\n",
    "            conv_layers = [m for m in self.model.features if isinstance(m, torch.nn.Conv2d)]\n",
    "            if layer_idx >= len(conv_layers):\n",
    "                continue\n",
    "            conv = conv_layers[layer_idx]\n",
    "            try:\n",
    "                plan = DG.get_pruning_plan(conv, tp.prune_conv_out_channel, idxs=filt_list)\n",
    "                plan.exec()\n",
    "                self.model = DG.model.to(self.config.device)\n",
    "                # Rebuild láº¡i features Ä‘á»ƒ cáº­p nháº­t sá»‘ channel\n",
    "                self.model.features = nn.Sequential(*list(self.model.features.children()))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Skip layer {layer_idx}, filters {filt_list}: {e}\")\n",
    "    \n",
    "    def prune(self, num_filters_to_prune, epochs_after_prune=50):\n",
    "        \"\"\"Execute pruning procedure\"\"\"\n",
    "        print(\"\\n==> Starting pruning process...\")\n",
    "\n",
    "        # Get baseline metrics\n",
    "        input_size = (torch.randn(1, 3, 224, 224) if self.config.classifier_type == 'C'\n",
    "                     else torch.randn(1, 3, 32, 32)).cuda()\n",
    "\n",
    "        base_macs, base_params = tp.utils.count_ops_and_params(self.model, input_size)\n",
    "        print(f\"Base MACs: {base_macs/1e9:.2f}G | Params: {base_params/1e6:.2f}M\")\n",
    "\n",
    "        # Test before pruning\n",
    "        print(\"\\nTesting before pruning:\")\n",
    "        self.test(0)\n",
    "\n",
    "        # Unfreeze all layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Rank filters\n",
    "        print(\"\\nRanking filters...\")\n",
    "        self.pruner.reset()\n",
    "        self.train_epoch(rank_filters=True)\n",
    "        self.pruner.normalize_ranks_per_layer()\n",
    "\n",
    "        # Get pruning plan\n",
    "        prune_targets = self.pruner.get_pruning_plan(num_filters_to_prune)\n",
    "\n",
    "        layers_pruned = {}\n",
    "        for layer, _ in prune_targets:\n",
    "            layers_pruned[layer] = layers_pruned.get(layer, 0) + 1\n",
    "\n",
    "        print(f\"\\nLayers to prune: {dict(sorted(layers_pruned.items()))}\")\n",
    "        for m in self.model.features:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                print(m)\n",
    "        # Execute pruning\n",
    "        print(\"\\nPruning filters...\")\n",
    "        self.model = self.model.cpu()\n",
    "        for layer_idx, filter_idx in prune_targets:\n",
    "            self.model = prune_vgg16_conv_layer_BN(\n",
    "                self.model, layer_idx, filter_idx,\n",
    "                use_cuda=self.config.use_cuda\n",
    "            )\n",
    "\n",
    "        self.model = self.model.to(self.config.device)\n",
    "        \n",
    "        for m in self.model.features:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                print(m)\n",
    "        # Fine-tune\n",
    "        print(\"\\nFine-tuning after pruning...\")\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs_after_prune)\n",
    "\n",
    "        for epoch in range(epochs_after_prune):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs_after_prune}\")\n",
    "            self.train_epoch(optimizer)\n",
    "            self.test(epoch)\n",
    "            scheduler.step()\n",
    "\n",
    "        # Final metrics\n",
    "        pruned_macs, pruned_params = tp.utils.count_ops_and_params(self.model, input_size)\n",
    "\n",
    "        print(\"\\n==> Pruning Summary:\")\n",
    "        print(f\"Params: {base_params/1e6:.2f}M => {pruned_params/1e6:.2f}M \"\n",
    "              f\"({100*(1-pruned_params/base_params):.1f}% reduction)\")\n",
    "        print(f\"MACs: {base_macs/1e9:.2f}G => {pruned_macs/1e9:.2f}G \"\n",
    "              f\"({100*(1-pruned_macs/base_macs):.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b958b79",
   "metadata": {},
   "source": [
    "## 3. Training mÃ´ hÃ¬nh gá»‘c (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef037102",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ef86708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "[=================================================>] 391/391 | Loss: 1.525 | Acc: 43.308%\n",
      "[=================================================>] 79/79 | Loss: 1.217 | Acc: 55.740%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  1\n",
      "[=================================================>] 391/391 | Loss: 0.986 | Acc: 65.348%\n",
      "[=================================================>] 79/79 | Loss: 1.066 | Acc: 64.840%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  2\n",
      "[=================================================>] 391/391 | Loss: 0.775 | Acc: 73.394%\n",
      "[=================================================>] 79/79 | Loss: 0.818 | Acc: 72.120%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  3\n",
      "[=================================================>] 391/391 | Loss: 0.657 | Acc: 77.798%\n",
      "[=================================================>] 79/79 | Loss: 0.706 | Acc: 76.870%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  4\n",
      "[=================================================>] 391/391 | Loss: 0.581 | Acc: 80.370%\n",
      "[=================================================>] 79/79 | Loss: 0.652 | Acc: 78.010%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  5\n",
      "[=================================================>] 391/391 | Loss: 0.525 | Acc: 82.210%\n",
      "[=================================================>] 79/79 | Loss: 0.576 | Acc: 80.910%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  6\n",
      "[=================================================>] 391/391 | Loss: 0.475 | Acc: 83.982%\n",
      "[=================================================>] 79/79 | Loss: 0.492 | Acc: 83.630%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  7\n",
      "[=================================================>] 391/391 | Loss: 0.442 | Acc: 85.054%\n",
      "[=================================================>] 79/79 | Loss: 0.526 | Acc: 82.600%\n",
      "Epoch:  8\n",
      "[=================================================>] 391/391 | Loss: 0.407 | Acc: 86.202%\n",
      "[=================================================>] 79/79 | Loss: 0.671 | Acc: 78.880%\n",
      "Epoch:  9\n",
      "[=================================================>] 391/391 | Loss: 0.382 | Acc: 87.094%\n",
      "[=================================================>] 79/79 | Loss: 0.529 | Acc: 82.980%\n",
      "Epoch:  10\n",
      "[=================================================>] 391/391 | Loss: 0.354 | Acc: 88.046%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 87.050%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  11\n",
      "[=================================================>] 391/391 | Loss: 0.340 | Acc: 88.452%\n",
      "[=================================================>] 79/79 | Loss: 0.410 | Acc: 86.290%\n",
      "Epoch:  12\n",
      "[=================================================>] 391/391 | Loss: 0.315 | Acc: 89.386%\n",
      "[=================================================>] 79/79 | Loss: 0.423 | Acc: 86.220%\n",
      "Epoch:  13\n",
      "[=================================================>] 391/391 | Loss: 0.298 | Acc: 89.966%\n",
      "[=================================================>] 79/79 | Loss: 0.422 | Acc: 86.090%\n",
      "Epoch:  14\n",
      "[=================================================>] 391/391 | Loss: 0.287 | Acc: 90.202%\n",
      "[=================================================>] 79/79 | Loss: 0.459 | Acc: 84.390%\n",
      "Epoch:  15\n",
      "[=================================================>] 391/391 | Loss: 0.274 | Acc: 90.694%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 88.150%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  16\n",
      "[=================================================>] 391/391 | Loss: 0.254 | Acc: 91.482%\n",
      "[=================================================>] 79/79 | Loss: 0.455 | Acc: 85.680%\n",
      "Epoch:  17\n",
      "[=================================================>] 391/391 | Loss: 0.244 | Acc: 91.860%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 87.440%\n",
      "Epoch:  18\n",
      "[=================================================>] 391/391 | Loss: 0.244 | Acc: 91.744%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 87.870%\n",
      "Epoch:  19\n",
      "[=================================================>] 391/391 | Loss: 0.225 | Acc: 92.414%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 88.860%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  20\n",
      "[=================================================>] 391/391 | Loss: 0.214 | Acc: 92.824%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 87.900%\n",
      "Epoch:  21\n",
      "[=================================================>] 391/391 | Loss: 0.212 | Acc: 92.844%\n",
      "[=================================================>] 79/79 | Loss: 0.437 | Acc: 86.390%\n",
      "Epoch:  22\n",
      "[=================================================>] 391/391 | Loss: 0.199 | Acc: 93.224%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 88.160%\n",
      "Epoch:  23\n",
      "[=================================================>] 391/391 | Loss: 0.192 | Acc: 93.452%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 88.900%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  24\n",
      "[=================================================>] 391/391 | Loss: 0.183 | Acc: 93.818%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 88.190%\n",
      "Epoch:  25\n",
      "[=================================================>] 391/391 | Loss: 0.174 | Acc: 94.158%\n",
      "[=================================================>] 79/79 | Loss: 0.438 | Acc: 87.180%\n",
      "Epoch:  26\n",
      "[=================================================>] 391/391 | Loss: 0.172 | Acc: 94.282%\n",
      "[=================================================>] 79/79 | Loss: 0.422 | Acc: 87.870%\n",
      "Epoch:  27\n",
      "[=================================================>] 391/391 | Loss: 0.162 | Acc: 94.494%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 89.170%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  28\n",
      "[=================================================>] 391/391 | Loss: 0.159 | Acc: 94.618%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 88.340%\n",
      "Epoch:  29\n",
      "[=================================================>] 391/391 | Loss: 0.152 | Acc: 94.790%\n",
      "[=================================================>] 79/79 | Loss: 0.323 | Acc: 90.180%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  30\n",
      "[=================================================>] 391/391 | Loss: 0.143 | Acc: 95.084%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 88.660%\n",
      "Epoch:  31\n",
      "[=================================================>] 391/391 | Loss: 0.144 | Acc: 95.090%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 89.440%\n",
      "Epoch:  32\n",
      "[=================================================>] 391/391 | Loss: 0.140 | Acc: 95.134%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 89.430%\n",
      "Epoch:  33\n",
      "[=================================================>] 391/391 | Loss: 0.132 | Acc: 95.410%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 89.020%\n",
      "Epoch:  34\n",
      "[=================================================>] 391/391 | Loss: 0.129 | Acc: 95.584%\n",
      "[=================================================>] 79/79 | Loss: 0.335 | Acc: 90.050%\n",
      "Epoch:  35\n",
      "[=================================================>] 391/391 | Loss: 0.124 | Acc: 95.798%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 89.430%\n",
      "Epoch:  36\n",
      "[=================================================>] 391/391 | Loss: 0.122 | Acc: 95.862%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 89.960%\n",
      "Epoch:  37\n",
      "[=================================================>] 391/391 | Loss: 0.118 | Acc: 96.026%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 89.730%\n",
      "Epoch:  38\n",
      "[=================================================>] 391/391 | Loss: 0.115 | Acc: 96.022%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.080%\n",
      "Epoch:  39\n",
      "[=================================================>] 391/391 | Loss: 0.114 | Acc: 96.110%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 89.670%\n",
      "Epoch:  40\n",
      "[=================================================>] 391/391 | Loss: 0.109 | Acc: 96.354%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 89.660%\n",
      "Epoch:  41\n",
      "[=================================================>] 391/391 | Loss: 0.104 | Acc: 96.418%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 89.390%\n",
      "Epoch:  42\n",
      "[=================================================>] 391/391 | Loss: 0.104 | Acc: 96.486%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 88.990%\n",
      "Epoch:  43\n",
      "[=================================================>] 391/391 | Loss: 0.099 | Acc: 96.650%\n",
      "[=================================================>] 79/79 | Loss: 0.329 | Acc: 90.410%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  44\n",
      "[=================================================>] 391/391 | Loss: 0.095 | Acc: 96.848%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 89.650%\n",
      "Epoch:  45\n",
      "[=================================================>] 391/391 | Loss: 0.097 | Acc: 96.736%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 89.690%\n",
      "Epoch:  46\n",
      "[=================================================>] 391/391 | Loss: 0.096 | Acc: 96.800%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 90.100%\n",
      "Epoch:  47\n",
      "[=================================================>] 391/391 | Loss: 0.090 | Acc: 96.986%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 89.380%\n",
      "Epoch:  48\n",
      "[=================================================>] 391/391 | Loss: 0.088 | Acc: 97.016%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 89.160%\n",
      "Epoch:  49\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 96.972%\n",
      "[=================================================>] 79/79 | Loss: 0.331 | Acc: 90.770%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  50\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.122%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 89.840%\n",
      "Epoch:  51\n",
      "[=================================================>] 391/391 | Loss: 0.083 | Acc: 97.182%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 90.100%\n",
      "Epoch:  52\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.274%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 89.690%\n",
      "Epoch:  53\n",
      "[=================================================>] 391/391 | Loss: 0.075 | Acc: 97.388%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 89.760%\n",
      "Epoch:  54\n",
      "[=================================================>] 391/391 | Loss: 0.076 | Acc: 97.462%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 90.610%\n",
      "Epoch:  55\n",
      "[=================================================>] 391/391 | Loss: 0.076 | Acc: 97.396%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 89.900%\n",
      "Epoch:  56\n",
      "[=================================================>] 391/391 | Loss: 0.078 | Acc: 97.418%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 89.350%\n",
      "Epoch:  57\n",
      "[=================================================>] 391/391 | Loss: 0.072 | Acc: 97.548%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 89.320%\n",
      "Epoch:  58\n",
      "[=================================================>] 391/391 | Loss: 0.070 | Acc: 97.616%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 90.330%\n",
      "Epoch:  59\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.626%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 90.950%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  60\n",
      "[=================================================>] 391/391 | Loss: 0.069 | Acc: 97.716%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 90.470%\n",
      "Epoch:  61\n",
      "[=================================================>] 391/391 | Loss: 0.069 | Acc: 97.706%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 90.680%\n",
      "Epoch:  62\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.776%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 89.590%\n",
      "Epoch:  63\n",
      "[=================================================>] 391/391 | Loss: 0.067 | Acc: 97.688%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 90.020%\n",
      "Epoch:  64\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.806%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 89.910%\n",
      "Epoch:  65\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.822%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 90.340%\n",
      "Epoch:  66\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.926%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 89.970%\n",
      "Epoch:  67\n",
      "[=================================================>] 391/391 | Loss: 0.062 | Acc: 97.882%\n",
      "[=================================================>] 79/79 | Loss: 0.335 | Acc: 90.830%\n",
      "Epoch:  68\n",
      "[=================================================>] 391/391 | Loss: 0.058 | Acc: 98.052%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 90.400%\n",
      "Epoch:  69\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.142%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 90.500%\n",
      "Epoch:  70\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.054%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 90.090%\n",
      "Epoch:  71\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.112%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 90.630%\n",
      "Epoch:  72\n",
      "[=================================================>] 391/391 | Loss: 0.054 | Acc: 98.222%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 90.950%\n",
      "Epoch:  73\n",
      "[=================================================>] 391/391 | Loss: 0.054 | Acc: 98.242%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 90.740%\n",
      "Epoch:  74\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.308%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 90.360%\n",
      "Epoch:  75\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.190%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 90.350%\n",
      "Epoch:  76\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.240%\n",
      "[=================================================>] 79/79 | Loss: 0.423 | Acc: 89.340%\n",
      "Epoch:  77\n",
      "[=================================================>] 391/391 | Loss: 0.055 | Acc: 98.138%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 90.950%\n",
      "Epoch:  78\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.490%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 90.560%\n",
      "Epoch:  79\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.336%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 90.640%\n",
      "Epoch:  80\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.364%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 90.100%\n",
      "Epoch:  81\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.370%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 90.180%\n",
      "Epoch:  82\n",
      "[=================================================>] 391/391 | Loss: 0.045 | Acc: 98.494%\n",
      "[=================================================>] 79/79 | Loss: 0.375 | Acc: 90.550%\n",
      "Epoch:  83\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.546%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 90.800%\n",
      "Epoch:  84\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.478%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 91.150%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  85\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.478%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 90.640%\n",
      "Epoch:  86\n",
      "[=================================================>] 391/391 | Loss: 0.044 | Acc: 98.532%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.290%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  87\n",
      "[=================================================>] 391/391 | Loss: 0.041 | Acc: 98.626%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 90.230%\n",
      "Epoch:  88\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.606%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 91.000%\n",
      "Epoch:  89\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.696%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 91.270%\n",
      "Epoch:  90\n",
      "[=================================================>] 391/391 | Loss: 0.037 | Acc: 98.740%\n",
      "[=================================================>] 79/79 | Loss: 0.409 | Acc: 90.100%\n",
      "Epoch:  91\n",
      "[=================================================>] 391/391 | Loss: 0.038 | Acc: 98.688%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.140%\n",
      "Epoch:  92\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.680%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 90.650%\n",
      "Epoch:  93\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.704%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 90.990%\n",
      "Epoch:  94\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.908%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 91.120%\n",
      "Epoch:  95\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.906%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 91.250%\n",
      "Epoch:  96\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.950%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 90.620%\n",
      "Epoch:  97\n",
      "[=================================================>] 391/391 | Loss: 0.030 | Acc: 99.014%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 91.400%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  98\n",
      "[=================================================>] 391/391 | Loss: 0.036 | Acc: 98.766%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 91.150%\n",
      "Epoch:  99\n",
      "[=================================================>] 391/391 | Loss: 0.031 | Acc: 98.990%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 91.320%\n",
      "Epoch:  100\n",
      "[=================================================>] 391/391 | Loss: 0.029 | Acc: 98.982%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 91.160%\n",
      "Epoch:  101\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.064%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 91.650%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  102\n",
      "[=================================================>] 391/391 | Loss: 0.029 | Acc: 99.058%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 91.650%\n",
      "Epoch:  103\n",
      "[=================================================>] 391/391 | Loss: 0.026 | Acc: 99.142%\n",
      "[=================================================>] 79/79 | Loss: 0.420 | Acc: 90.100%\n",
      "Epoch:  104\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.142%\n",
      "[=================================================>] 79/79 | Loss: 0.375 | Acc: 90.920%\n",
      "Epoch:  105\n",
      "[=================================================>] 391/391 | Loss: 0.031 | Acc: 98.992%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 90.950%\n",
      "Epoch:  106\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.108%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 91.560%\n",
      "Epoch:  107\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.094%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 90.930%\n",
      "Epoch:  108\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.236%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 91.870%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  109\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.306%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 91.540%\n",
      "Epoch:  110\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.184%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.610%\n",
      "Epoch:  111\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.254%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 91.250%\n",
      "Epoch:  112\n",
      "[=================================================>] 391/391 | Loss: 0.019 | Acc: 99.360%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 91.740%\n",
      "Epoch:  113\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.292%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 91.240%\n",
      "Epoch:  114\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.448%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 91.280%\n",
      "Epoch:  115\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.432%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 91.680%\n",
      "Epoch:  116\n",
      "[=================================================>] 391/391 | Loss: 0.019 | Acc: 99.360%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 91.800%\n",
      "Epoch:  117\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.482%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 91.860%\n",
      "Epoch:  118\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.540%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.060%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  119\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.560%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 92.030%\n",
      "Epoch:  120\n",
      "[=================================================>] 391/391 | Loss: 0.016 | Acc: 99.522%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.170%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  121\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.612%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 91.800%\n",
      "Epoch:  122\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.648%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 91.940%\n",
      "Epoch:  123\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.608%\n",
      "[=================================================>] 79/79 | Loss: 0.380 | Acc: 91.650%\n",
      "Epoch:  124\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.550%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 91.800%\n",
      "Epoch:  125\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.592%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 92.080%\n",
      "Epoch:  126\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.758%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 92.390%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  127\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.704%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 91.600%\n",
      "Epoch:  128\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.684%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 91.820%\n",
      "Epoch:  129\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.696%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 92.530%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  130\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.768%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.510%\n",
      "Epoch:  131\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.798%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 92.510%\n",
      "Epoch:  132\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.830%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.460%\n",
      "Epoch:  133\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.836%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 92.230%\n",
      "Epoch:  134\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.812%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.460%\n",
      "Epoch:  135\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.840%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.780%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  136\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.762%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 92.470%\n",
      "Epoch:  137\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.852%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 92.590%\n",
      "Epoch:  138\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.860%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 92.830%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  139\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.820%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 92.420%\n",
      "Epoch:  140\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.848%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.580%\n",
      "Epoch:  141\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.900%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 92.640%\n",
      "Epoch:  142\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.906%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.570%\n",
      "Epoch:  143\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.904%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.560%\n",
      "Epoch:  144\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.894%\n",
      "[=================================================>] 79/79 | Loss: 0.336 | Acc: 93.000%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  145\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 92.880%\n",
      "Epoch:  146\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.924%\n",
      "[=================================================>] 79/79 | Loss: 0.331 | Acc: 93.050%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  147\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 0.323 | Acc: 93.190%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  148\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 0.329 | Acc: 93.010%\n",
      "Epoch:  149\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.323 | Acc: 93.090%\n",
      "Epoch:  150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 0.325 | Acc: 93.110%\n",
      "Epoch:  151\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 0.332 | Acc: 92.910%\n",
      "Epoch:  152\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.326 | Acc: 93.060%\n",
      "Epoch:  153\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.323 | Acc: 93.110%\n",
      "Epoch:  154\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.328 | Acc: 93.070%\n",
      "Epoch:  155\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.325 | Acc: 92.990%\n",
      "Epoch:  156\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.321 | Acc: 93.160%\n",
      "Epoch:  157\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.324 | Acc: 93.240%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  158\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.323 | Acc: 93.220%\n",
      "Epoch:  159\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%%\n",
      "[=================================================>] 79/79 | Loss: 0.320 | Acc: 93.180%\n",
      "Epoch:  160\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.318 | Acc: 93.290%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  161\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.317 | Acc: 93.350%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  162\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.321 | Acc: 93.250%\n",
      "Epoch:  163\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.990%\n",
      "[=================================================>] 79/79 | Loss: 0.319 | Acc: 93.280%\n",
      "Epoch:  164\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.318 | Acc: 93.400%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  165\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.317 | Acc: 93.350%\n",
      "Epoch:  166\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.317 | Acc: 93.260%\n",
      "Epoch:  167\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.317 | Acc: 93.320%\n",
      "Epoch:  168\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.990%\n",
      "[=================================================>] 79/79 | Loss: 0.317 | Acc: 93.420%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  169\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.317 | Acc: 93.450%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  170\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.318 | Acc: 93.330%\n",
      "Epoch:  171\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.440%\n",
      "Epoch:  172\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%\n",
      "[=================================================>] 79/79 | Loss: 0.317 | Acc: 93.430%\n",
      "Epoch:  173\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.410%\n",
      "Epoch:  174\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.440%\n",
      "Epoch:  175\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.420%\n",
      "Epoch:  176\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.990%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.420%\n",
      "Epoch:  177\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.450%\n",
      "Epoch:  178\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.440%\n",
      "Epoch:  179\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.510%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  180\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.470%\n",
      "Epoch:  181\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.510%\n",
      "Epoch:  182\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.550%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  183\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  184\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.500%\n",
      "Epoch:  185\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.500%\n",
      "Epoch:  186\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.992%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.520%\n",
      "Epoch:  187\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.440%\n",
      "Epoch:  188\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.490%\n",
      "Epoch:  189\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.400%\n",
      "Epoch:  190\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.410%\n",
      "Epoch:  191\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.550%\n",
      "Epoch:  192\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.500%\n",
      "Epoch:  193\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.460%\n",
      "Epoch:  194\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.550%\n",
      "Epoch:  195\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.550%\n",
      "Epoch:  196\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.480%\n",
      "Epoch:  197\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.430%\n",
      "Epoch:  198\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.420%\n",
      "Epoch:  199\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.450%\n",
      "Epoch:  200\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.530%\n",
      "Epoch:  201\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.480%\n",
      "Epoch:  202\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.470%\n",
      "Epoch:  203\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.400%\n",
      "Epoch:  204\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.430%\n",
      "Epoch:  205\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.450%\n",
      "Epoch:  206\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.410%\n",
      "Epoch:  207\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.530%\n",
      "Epoch:  208\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.430%\n",
      "Epoch:  209\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.500%\n",
      "Epoch:  210\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.510%\n",
      "Epoch:  211\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.430%\n",
      "Epoch:  212\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.510%\n",
      "Epoch:  213\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.312 | Acc: 93.560%\n",
      "Epoch:  214\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.530%\n",
      "Epoch:  215\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.520%\n",
      "Epoch:  216\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.510%\n",
      "Epoch:  217\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.410%\n",
      "Epoch:  218\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.470%\n",
      "Epoch:  219\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.500%\n",
      "Epoch:  220\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.550%\n",
      "Epoch:  221\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.480%\n",
      "Epoch:  222\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.500%\n",
      "Epoch:  223\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.470%\n",
      "Epoch:  224\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.430%\n",
      "Epoch:  225\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.430%\n",
      "Epoch:  226\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.510%\n",
      "Epoch:  227\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.420%\n",
      "Epoch:  228\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.500%\n",
      "Epoch:  229\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.570%\n",
      "Epoch:  230\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.490%\n",
      "Epoch:  231\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.312 | Acc: 93.440%\n",
      "Epoch:  232\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 100.000%\n",
      "[=================================================>] 79/79 | Loss: 0.311 | Acc: 93.510%\n",
      "Epoch:  233\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.312 | Acc: 93.500%\n",
      "Epoch:  234\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.312 | Acc: 93.520%\n",
      "Epoch:  235\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.520%\n",
      "Epoch:  236\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.480%\n",
      "Epoch:  237\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.440%\n",
      "Epoch:  238\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.420%\n",
      "Epoch:  239\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.480%\n",
      "Epoch:  240\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.998%%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.430%\n",
      "Epoch:  241\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.410%\n",
      "Epoch:  242\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.460%\n",
      "Epoch:  243\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.450%\n",
      "Epoch:  244\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.315 | Acc: 93.350%\n",
      "Epoch:  245\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.314 | Acc: 93.310%\n",
      "Epoch:  246\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.318 | Acc: 93.350%\n",
      "Epoch:  247\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%%\n",
      "[=================================================>] 79/79 | Loss: 0.316 | Acc: 93.340%\n",
      "Epoch:  248\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%\n",
      "[=================================================>] 79/79 | Loss: 0.322 | Acc: 93.130%\n",
      "Epoch:  249\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%%\n",
      "[=================================================>] 79/79 | Loss: 0.324 | Acc: 93.260%\n",
      "Epoch:  250\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%\n",
      "[=================================================>] 79/79 | Loss: 0.323 | Acc: 93.350%\n",
      "Epoch:  251\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.974%\n",
      "[=================================================>] 79/79 | Loss: 0.330 | Acc: 93.080%\n",
      "Epoch:  252\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.324 | Acc: 93.100%\n",
      "Epoch:  253\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.326 | Acc: 93.130%\n",
      "Epoch:  254\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%\n",
      "[=================================================>] 79/79 | Loss: 0.326 | Acc: 93.190%\n",
      "Epoch:  255\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.990%\n",
      "[=================================================>] 79/79 | Loss: 0.333 | Acc: 93.110%\n",
      "Epoch:  256\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%\n",
      "[=================================================>] 79/79 | Loss: 0.329 | Acc: 93.070%\n",
      "Epoch:  257\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 92.830%\n",
      "Epoch:  258\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.710%\n",
      "Epoch:  259\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.924%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 92.550%\n",
      "Epoch:  260\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.440%\n",
      "Epoch:  261\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.822%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 92.430%\n",
      "Epoch:  262\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.704%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 92.010%\n",
      "Epoch:  263\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.668%\n",
      "[=================================================>] 79/79 | Loss: 0.425 | Acc: 91.160%\n",
      "Epoch:  264\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.296%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 91.080%\n",
      "Epoch:  265\n",
      "[=================================================>] 391/391 | Loss: 0.025 | Acc: 99.188%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 91.690%\n",
      "Epoch:  266\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.280%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 91.480%\n",
      "Epoch:  267\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.236%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 90.880%\n",
      "Epoch:  268\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.088%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 91.640%\n",
      "Epoch:  269\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.096%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 91.140%\n",
      "Epoch:  270\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.120%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 91.550%\n",
      "Epoch:  271\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.172%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 90.790%\n",
      "Epoch:  272\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.922%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 90.520%\n",
      "Epoch:  273\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.932%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 90.670%\n",
      "Epoch:  274\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.642%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 90.110%\n",
      "Epoch:  275\n",
      "[=================================================>] 391/391 | Loss: 0.037 | Acc: 98.824%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 91.200%\n",
      "Epoch:  276\n",
      "[=================================================>] 391/391 | Loss: 0.040 | Acc: 98.692%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 91.400%\n",
      "Epoch:  277\n",
      "[=================================================>] 391/391 | Loss: 0.041 | Acc: 98.742%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 90.540%\n",
      "Epoch:  278\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.488%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 90.890%\n",
      "Epoch:  279\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.500%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 89.920%\n",
      "Epoch:  280\n",
      "[=================================================>] 391/391 | Loss: 0.048 | Acc: 98.452%\n",
      "[=================================================>] 79/79 | Loss: 0.445 | Acc: 89.360%\n",
      "Epoch:  281\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.540%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 90.700%\n",
      "Epoch:  282\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.372%\n",
      "[=================================================>] 79/79 | Loss: 0.391 | Acc: 90.260%\n",
      "Epoch:  283\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.466%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 89.780%\n",
      "Epoch:  284\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.276%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 90.050%\n",
      "Epoch:  285\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.132%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 89.730%\n",
      "Epoch:  286\n",
      "[=================================================>] 391/391 | Loss: 0.052 | Acc: 98.344%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 89.750%\n",
      "Epoch:  287\n",
      "[=================================================>] 391/391 | Loss: 0.052 | Acc: 98.352%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 90.590%\n",
      "Epoch:  288\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.252%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 90.480%\n",
      "Epoch:  289\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.168%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 89.930%\n",
      "Epoch:  290\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.256%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 90.490%\n",
      "Epoch:  291\n",
      "[=================================================>] 391/391 | Loss: 0.055 | Acc: 98.170%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 90.190%\n",
      "Epoch:  292\n",
      "[=================================================>] 391/391 | Loss: 0.059 | Acc: 98.038%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 90.210%\n",
      "Epoch:  293\n",
      "[=================================================>] 391/391 | Loss: 0.055 | Acc: 98.228%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 90.630%\n",
      "Epoch:  294\n",
      "[=================================================>] 391/391 | Loss: 0.060 | Acc: 98.034%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 90.330%\n",
      "Epoch:  295\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.172%\n",
      "[=================================================>] 79/79 | Loss: 0.452 | Acc: 88.740%\n",
      "Epoch:  296\n",
      "[=================================================>] 391/391 | Loss: 0.058 | Acc: 98.122%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 89.590%\n",
      "Epoch:  297\n",
      "[=================================================>] 391/391 | Loss: 0.060 | Acc: 98.030%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 90.550%\n",
      "Epoch:  298\n",
      "[=================================================>] 391/391 | Loss: 0.059 | Acc: 98.084%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 89.360%\n",
      "Epoch:  299\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.166%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 89.910%\n",
      "Finished fine tuning.\n"
     ]
    }
   ],
   "source": [
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "epoch = 300\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "pruner = VGG16Pruner(config, model, save_name='vgg16_cifar10_baseline.pth')\n",
    "pruner.train(epoches=epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c2916",
   "metadata": {},
   "source": [
    "### CIFFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3026291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "[=================================================>] 391/391 | Loss: 4.154 | Acc: 4.938%\n",
      "[=================================================>] 79/79 | Loss: 3.818 | Acc: 9.020%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  1\n",
      "[=================================================>] 391/391 | Loss: 3.505 | Acc: 13.808%\n",
      "[=================================================>] 79/79 | Loss: 3.269 | Acc: 18.320%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  2\n",
      "[=================================================>] 391/391 | Loss: 3.046 | Acc: 21.946%\n",
      "[=================================================>] 79/79 | Loss: 3.004 | Acc: 23.840%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  3\n",
      "[=================================================>] 391/391 | Loss: 2.696 | Acc: 28.836%\n",
      "[=================================================>] 79/79 | Loss: 2.659 | Acc: 30.760%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  4\n",
      "[=================================================>] 391/391 | Loss: 2.424 | Acc: 34.612%\n",
      "[=================================================>] 79/79 | Loss: 2.358 | Acc: 36.410%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  5\n",
      "[=================================================>] 391/391 | Loss: 2.229 | Acc: 39.468%\n",
      "[=================================================>] 79/79 | Loss: 2.234 | Acc: 40.380%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  6\n",
      "[=================================================>] 391/391 | Loss: 2.063 | Acc: 43.480%\n",
      "[=================================================>] 79/79 | Loss: 2.028 | Acc: 44.610%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  7\n",
      "[=================================================>] 391/391 | Loss: 1.930 | Acc: 46.688%\n",
      "[=================================================>] 79/79 | Loss: 2.051 | Acc: 44.440%\n",
      "Epoch:  8\n",
      "[=================================================>] 391/391 | Loss: 1.823 | Acc: 49.154%\n",
      "[=================================================>] 79/79 | Loss: 1.963 | Acc: 47.560%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  9\n",
      "[=================================================>] 391/391 | Loss: 1.723 | Acc: 51.862%\n",
      "[=================================================>] 79/79 | Loss: 1.839 | Acc: 49.950%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  10\n",
      "[=================================================>] 391/391 | Loss: 1.629 | Acc: 54.038%\n",
      "[=================================================>] 79/79 | Loss: 1.868 | Acc: 49.960%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  11\n",
      "[=================================================>] 391/391 | Loss: 1.560 | Acc: 56.012%\n",
      "[=================================================>] 79/79 | Loss: 1.690 | Acc: 53.260%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  12\n",
      "[=================================================>] 391/391 | Loss: 1.490 | Acc: 57.560%\n",
      "[=================================================>] 79/79 | Loss: 1.610 | Acc: 55.780%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  13\n",
      "[=================================================>] 391/391 | Loss: 1.415 | Acc: 59.372%\n",
      "[=================================================>] 79/79 | Loss: 1.744 | Acc: 53.090%\n",
      "Epoch:  14\n",
      "[=================================================>] 391/391 | Loss: 1.365 | Acc: 60.766%\n",
      "[=================================================>] 79/79 | Loss: 1.597 | Acc: 55.800%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  15\n",
      "[=================================================>] 391/391 | Loss: 1.312 | Acc: 62.226%\n",
      "[=================================================>] 79/79 | Loss: 1.633 | Acc: 56.410%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  16\n",
      "[=================================================>] 391/391 | Loss: 1.256 | Acc: 63.598%\n",
      "[=================================================>] 79/79 | Loss: 1.535 | Acc: 58.260%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  17\n",
      "[=================================================>] 391/391 | Loss: 1.204 | Acc: 65.052%\n",
      "[=================================================>] 79/79 | Loss: 1.502 | Acc: 58.510%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  18\n",
      "[=================================================>] 391/391 | Loss: 1.157 | Acc: 66.126%\n",
      "[=================================================>] 79/79 | Loss: 1.514 | Acc: 58.870%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  19\n",
      "[=================================================>] 391/391 | Loss: 1.114 | Acc: 67.624%\n",
      "[=================================================>] 79/79 | Loss: 1.462 | Acc: 60.410%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  20\n",
      "[=================================================>] 391/391 | Loss: 1.074 | Acc: 68.492%\n",
      "[=================================================>] 79/79 | Loss: 1.569 | Acc: 58.740%\n",
      "Epoch:  21\n",
      "[=================================================>] 391/391 | Loss: 1.038 | Acc: 69.574%\n",
      "[=================================================>] 79/79 | Loss: 1.511 | Acc: 59.830%\n",
      "Epoch:  22\n",
      "[=================================================>] 391/391 | Loss: 1.003 | Acc: 70.294%\n",
      "[=================================================>] 79/79 | Loss: 1.526 | Acc: 59.140%\n",
      "Epoch:  23\n",
      "[=================================================>] 391/391 | Loss: 0.964 | Acc: 71.388%\n",
      "[=================================================>] 79/79 | Loss: 1.452 | Acc: 60.900%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  24\n",
      "[=================================================>] 391/391 | Loss: 0.926 | Acc: 72.654%\n",
      "[=================================================>] 79/79 | Loss: 1.442 | Acc: 61.580%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  25\n",
      "[=================================================>] 391/391 | Loss: 0.899 | Acc: 73.150%\n",
      "[=================================================>] 79/79 | Loss: 1.431 | Acc: 62.150%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  26\n",
      "[=================================================>] 391/391 | Loss: 0.874 | Acc: 73.932%\n",
      "[=================================================>] 79/79 | Loss: 1.471 | Acc: 61.170%\n",
      "Epoch:  27\n",
      "[=================================================>] 391/391 | Loss: 0.840 | Acc: 74.548%\n",
      "[=================================================>] 79/79 | Loss: 1.471 | Acc: 61.790%\n",
      "Epoch:  28\n",
      "[=================================================>] 391/391 | Loss: 0.797 | Acc: 75.930%\n",
      "[=================================================>] 79/79 | Loss: 1.496 | Acc: 61.340%\n",
      "Epoch:  29\n",
      "[=================================================>] 391/391 | Loss: 0.785 | Acc: 76.308%\n",
      "[=================================================>] 79/79 | Loss: 1.381 | Acc: 63.660%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  30\n",
      "[=================================================>] 391/391 | Loss: 0.748 | Acc: 77.332%\n",
      "[=================================================>] 79/79 | Loss: 1.447 | Acc: 63.850%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  31\n",
      "[=================================================>] 391/391 | Loss: 0.731 | Acc: 77.836%\n",
      "[=================================================>] 79/79 | Loss: 1.501 | Acc: 61.800%\n",
      "Epoch:  32\n",
      "[=================================================>] 391/391 | Loss: 0.707 | Acc: 78.406%\n",
      "[=================================================>] 79/79 | Loss: 1.499 | Acc: 62.110%\n",
      "Epoch:  33\n",
      "[=================================================>] 391/391 | Loss: 0.682 | Acc: 79.184%\n",
      "[=================================================>] 79/79 | Loss: 1.531 | Acc: 61.850%\n",
      "Epoch:  34\n",
      "[=================================================>] 391/391 | Loss: 0.657 | Acc: 79.928%\n",
      "[=================================================>] 79/79 | Loss: 1.436 | Acc: 63.950%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  35\n",
      "[=================================================>] 391/391 | Loss: 0.633 | Acc: 80.702%\n",
      "[=================================================>] 79/79 | Loss: 1.402 | Acc: 64.170%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  36\n",
      "[=================================================>] 391/391 | Loss: 0.619 | Acc: 81.030%\n",
      "[=================================================>] 79/79 | Loss: 1.483 | Acc: 63.560%\n",
      "Epoch:  37\n",
      "[=================================================>] 391/391 | Loss: 0.592 | Acc: 81.750%\n",
      "[=================================================>] 79/79 | Loss: 1.508 | Acc: 63.240%\n",
      "Epoch:  38\n",
      "[=================================================>] 391/391 | Loss: 0.587 | Acc: 81.836%\n",
      "[=================================================>] 79/79 | Loss: 1.460 | Acc: 64.250%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  39\n",
      "[=================================================>] 391/391 | Loss: 0.560 | Acc: 82.696%\n",
      "[=================================================>] 79/79 | Loss: 1.467 | Acc: 63.930%\n",
      "Epoch:  40\n",
      "[=================================================>] 391/391 | Loss: 0.553 | Acc: 83.060%\n",
      "[=================================================>] 79/79 | Loss: 1.482 | Acc: 64.410%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  41\n",
      "[=================================================>] 391/391 | Loss: 0.524 | Acc: 83.670%\n",
      "[=================================================>] 79/79 | Loss: 1.515 | Acc: 63.630%\n",
      "Epoch:  42\n",
      "[=================================================>] 391/391 | Loss: 0.499 | Acc: 84.676%\n",
      "[=================================================>] 79/79 | Loss: 1.544 | Acc: 63.290%\n",
      "Epoch:  43\n",
      "[=================================================>] 391/391 | Loss: 0.491 | Acc: 84.830%\n",
      "[=================================================>] 79/79 | Loss: 1.560 | Acc: 64.390%\n",
      "Epoch:  44\n",
      "[=================================================>] 391/391 | Loss: 0.474 | Acc: 85.262%\n",
      "[=================================================>] 79/79 | Loss: 1.501 | Acc: 64.350%\n",
      "Epoch:  45\n",
      "[=================================================>] 391/391 | Loss: 0.456 | Acc: 85.776%\n",
      "[=================================================>] 79/79 | Loss: 1.467 | Acc: 65.760%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  46\n",
      "[=================================================>] 391/391 | Loss: 0.454 | Acc: 85.744%\n",
      "[=================================================>] 79/79 | Loss: 1.587 | Acc: 63.620%\n",
      "Epoch:  47\n",
      "[=================================================>] 391/391 | Loss: 0.432 | Acc: 86.450%\n",
      "[=================================================>] 79/79 | Loss: 1.510 | Acc: 64.520%\n",
      "Epoch:  48\n",
      "[=================================================>] 391/391 | Loss: 0.422 | Acc: 86.710%\n",
      "[=================================================>] 79/79 | Loss: 1.560 | Acc: 64.650%\n",
      "Epoch:  49\n",
      "[=================================================>] 391/391 | Loss: 0.410 | Acc: 87.092%\n",
      "[=================================================>] 79/79 | Loss: 1.551 | Acc: 64.580%\n",
      "Epoch:  50\n",
      "[=================================================>] 391/391 | Loss: 0.399 | Acc: 87.514%\n",
      "[=================================================>] 79/79 | Loss: 1.546 | Acc: 64.700%\n",
      "Epoch:  51\n",
      "[=================================================>] 391/391 | Loss: 0.391 | Acc: 87.516%\n",
      "[=================================================>] 79/79 | Loss: 1.497 | Acc: 65.920%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  52\n",
      "[=================================================>] 391/391 | Loss: 0.370 | Acc: 88.458%\n",
      "[=================================================>] 79/79 | Loss: 1.591 | Acc: 63.790%\n",
      "Epoch:  53\n",
      "[=================================================>] 391/391 | Loss: 0.367 | Acc: 88.572%\n",
      "[=================================================>] 79/79 | Loss: 1.548 | Acc: 64.770%\n",
      "Epoch:  54\n",
      "[=================================================>] 391/391 | Loss: 0.354 | Acc: 88.832%\n",
      "[=================================================>] 79/79 | Loss: 1.541 | Acc: 65.510%\n",
      "Epoch:  55\n",
      "[=================================================>] 391/391 | Loss: 0.352 | Acc: 88.982%\n",
      "[=================================================>] 79/79 | Loss: 1.609 | Acc: 64.460%\n",
      "Epoch:  56\n",
      "[=================================================>] 391/391 | Loss: 0.335 | Acc: 89.418%\n",
      "[=================================================>] 79/79 | Loss: 1.542 | Acc: 65.580%\n",
      "Epoch:  57\n",
      "[=================================================>] 391/391 | Loss: 0.331 | Acc: 89.666%\n",
      "[=================================================>] 79/79 | Loss: 1.561 | Acc: 65.250%\n",
      "Epoch:  58\n",
      "[=================================================>] 391/391 | Loss: 0.321 | Acc: 89.774%\n",
      "[=================================================>] 79/79 | Loss: 1.565 | Acc: 65.260%\n",
      "Epoch:  59\n",
      "[=================================================>] 391/391 | Loss: 0.309 | Acc: 90.244%\n",
      "[=================================================>] 79/79 | Loss: 1.535 | Acc: 66.320%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  60\n",
      "[=================================================>] 391/391 | Loss: 0.291 | Acc: 90.832%\n",
      "[=================================================>] 79/79 | Loss: 1.643 | Acc: 64.910%\n",
      "Epoch:  61\n",
      "[=================================================>] 391/391 | Loss: 0.293 | Acc: 90.636%\n",
      "[=================================================>] 79/79 | Loss: 1.586 | Acc: 65.090%\n",
      "Epoch:  62\n",
      "[=================================================>] 391/391 | Loss: 0.291 | Acc: 90.798%\n",
      "[=================================================>] 79/79 | Loss: 1.550 | Acc: 66.090%\n",
      "Epoch:  63\n",
      "[=================================================>] 391/391 | Loss: 0.270 | Acc: 91.494%\n",
      "[=================================================>] 79/79 | Loss: 1.637 | Acc: 65.270%\n",
      "Epoch:  64\n",
      "[=================================================>] 391/391 | Loss: 0.270 | Acc: 91.504%\n",
      "[=================================================>] 79/79 | Loss: 1.647 | Acc: 65.030%\n",
      "Epoch:  65\n",
      "[=================================================>] 391/391 | Loss: 0.264 | Acc: 91.592%\n",
      "[=================================================>] 79/79 | Loss: 1.569 | Acc: 65.630%\n",
      "Epoch:  66\n",
      "[=================================================>] 391/391 | Loss: 0.255 | Acc: 91.826%\n",
      "[=================================================>] 79/79 | Loss: 1.604 | Acc: 65.840%\n",
      "Epoch:  67\n",
      "[=================================================>] 391/391 | Loss: 0.250 | Acc: 92.140%\n",
      "[=================================================>] 79/79 | Loss: 1.641 | Acc: 65.810%\n",
      "Epoch:  68\n",
      "[=================================================>] 391/391 | Loss: 0.249 | Acc: 92.282%\n",
      "[=================================================>] 79/79 | Loss: 1.635 | Acc: 65.410%\n",
      "Epoch:  69\n",
      "[=================================================>] 391/391 | Loss: 0.237 | Acc: 92.496%\n",
      "[=================================================>] 79/79 | Loss: 1.612 | Acc: 65.360%\n",
      "Epoch:  70\n",
      "[=================================================>] 391/391 | Loss: 0.224 | Acc: 93.046%\n",
      "[=================================================>] 79/79 | Loss: 1.718 | Acc: 64.470%\n",
      "Epoch:  71\n",
      "[=================================================>] 391/391 | Loss: 0.229 | Acc: 92.706%\n",
      "[=================================================>] 79/79 | Loss: 1.658 | Acc: 65.220%\n",
      "Epoch:  72\n",
      "[=================================================>] 391/391 | Loss: 0.223 | Acc: 93.028%\n",
      "[=================================================>] 79/79 | Loss: 1.679 | Acc: 65.740%\n",
      "Epoch:  73\n",
      "[=================================================>] 391/391 | Loss: 0.211 | Acc: 93.342%\n",
      "[=================================================>] 79/79 | Loss: 1.624 | Acc: 66.510%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  74\n",
      "[=================================================>] 391/391 | Loss: 0.200 | Acc: 93.894%\n",
      "[=================================================>] 79/79 | Loss: 1.652 | Acc: 65.860%\n",
      "Epoch:  75\n",
      "[=================================================>] 391/391 | Loss: 0.215 | Acc: 93.090%\n",
      "[=================================================>] 79/79 | Loss: 1.718 | Acc: 63.950%\n",
      "Epoch:  76\n",
      "[=================================================>] 391/391 | Loss: 0.191 | Acc: 94.024%\n",
      "[=================================================>] 79/79 | Loss: 1.796 | Acc: 64.430%\n",
      "Epoch:  77\n",
      "[=================================================>] 391/391 | Loss: 0.192 | Acc: 93.946%\n",
      "[=================================================>] 79/79 | Loss: 1.652 | Acc: 66.330%\n",
      "Epoch:  78\n",
      "[=================================================>] 391/391 | Loss: 0.189 | Acc: 93.968%\n",
      "[=================================================>] 79/79 | Loss: 1.685 | Acc: 66.720%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  79\n",
      "[=================================================>] 391/391 | Loss: 0.179 | Acc: 94.284%\n",
      "[=================================================>] 79/79 | Loss: 1.648 | Acc: 67.050%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  80\n",
      "[=================================================>] 391/391 | Loss: 0.184 | Acc: 94.194%\n",
      "[=================================================>] 79/79 | Loss: 1.750 | Acc: 65.290%\n",
      "Epoch:  81\n",
      "[=================================================>] 391/391 | Loss: 0.174 | Acc: 94.588%\n",
      "[=================================================>] 79/79 | Loss: 1.649 | Acc: 66.290%\n",
      "Epoch:  82\n",
      "[=================================================>] 391/391 | Loss: 0.167 | Acc: 94.768%\n",
      "[=================================================>] 79/79 | Loss: 1.758 | Acc: 65.580%\n",
      "Epoch:  83\n",
      "[=================================================>] 391/391 | Loss: 0.162 | Acc: 94.856%\n",
      "[=================================================>] 79/79 | Loss: 1.698 | Acc: 66.280%\n",
      "Epoch:  84\n",
      "[=================================================>] 391/391 | Loss: 0.163 | Acc: 94.920%\n",
      "[=================================================>] 79/79 | Loss: 1.690 | Acc: 66.670%\n",
      "Epoch:  85\n",
      "[=================================================>] 391/391 | Loss: 0.152 | Acc: 95.320%\n",
      "[=================================================>] 79/79 | Loss: 1.712 | Acc: 66.440%\n",
      "Epoch:  86\n",
      "[=================================================>] 391/391 | Loss: 0.154 | Acc: 95.066%\n",
      "[=================================================>] 79/79 | Loss: 1.711 | Acc: 66.640%\n",
      "Epoch:  87\n",
      "[=================================================>] 391/391 | Loss: 0.142 | Acc: 95.590%\n",
      "[=================================================>] 79/79 | Loss: 1.687 | Acc: 66.740%\n",
      "Epoch:  88\n",
      "[=================================================>] 391/391 | Loss: 0.138 | Acc: 95.538%\n",
      "[=================================================>] 79/79 | Loss: 1.734 | Acc: 66.460%\n",
      "Epoch:  89\n",
      "[=================================================>] 391/391 | Loss: 0.138 | Acc: 95.596%\n",
      "[=================================================>] 79/79 | Loss: 1.729 | Acc: 65.840%\n",
      "Epoch:  90\n",
      "[=================================================>] 391/391 | Loss: 0.137 | Acc: 95.636%\n",
      "[=================================================>] 79/79 | Loss: 1.698 | Acc: 66.430%\n",
      "Epoch:  91\n",
      "[=================================================>] 391/391 | Loss: 0.128 | Acc: 96.012%\n",
      "[=================================================>] 79/79 | Loss: 1.706 | Acc: 66.790%\n",
      "Epoch:  92\n",
      "[=================================================>] 391/391 | Loss: 0.123 | Acc: 96.078%\n",
      "[=================================================>] 79/79 | Loss: 1.741 | Acc: 66.450%\n",
      "Epoch:  93\n",
      "[=================================================>] 391/391 | Loss: 0.123 | Acc: 96.214%\n",
      "[=================================================>] 79/79 | Loss: 1.679 | Acc: 67.950%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  94\n",
      "[=================================================>] 391/391 | Loss: 0.117 | Acc: 96.354%\n",
      "[=================================================>] 79/79 | Loss: 1.715 | Acc: 67.330%\n",
      "Epoch:  95\n",
      "[=================================================>] 391/391 | Loss: 0.109 | Acc: 96.646%\n",
      "[=================================================>] 79/79 | Loss: 1.737 | Acc: 67.170%\n",
      "Epoch:  96\n",
      "[=================================================>] 391/391 | Loss: 0.103 | Acc: 96.834%\n",
      "[=================================================>] 79/79 | Loss: 1.698 | Acc: 67.730%\n",
      "Epoch:  97\n",
      "[=================================================>] 391/391 | Loss: 0.101 | Acc: 96.752%\n",
      "[=================================================>] 79/79 | Loss: 1.716 | Acc: 67.340%\n",
      "Epoch:  98\n",
      "[=================================================>] 391/391 | Loss: 0.098 | Acc: 96.932%\n",
      "[=================================================>] 79/79 | Loss: 1.741 | Acc: 67.860%\n",
      "Epoch:  99\n",
      "[=================================================>] 391/391 | Loss: 0.096 | Acc: 97.018%\n",
      "[=================================================>] 79/79 | Loss: 1.714 | Acc: 67.750%\n",
      "Epoch:  100\n",
      "[=================================================>] 391/391 | Loss: 0.098 | Acc: 96.944%\n",
      "[=================================================>] 79/79 | Loss: 1.745 | Acc: 66.770%\n",
      "Epoch:  101\n",
      "[=================================================>] 391/391 | Loss: 0.094 | Acc: 97.110%\n",
      "[=================================================>] 79/79 | Loss: 1.699 | Acc: 67.930%\n",
      "Epoch:  102\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 97.220%\n",
      "[=================================================>] 79/79 | Loss: 1.668 | Acc: 68.360%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  103\n",
      "[=================================================>] 391/391 | Loss: 0.079 | Acc: 97.544%\n",
      "[=================================================>] 79/79 | Loss: 1.686 | Acc: 68.190%\n",
      "Epoch:  104\n",
      "[=================================================>] 391/391 | Loss: 0.078 | Acc: 97.564%\n",
      "[=================================================>] 79/79 | Loss: 1.658 | Acc: 68.970%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  105\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.500%\n",
      "[=================================================>] 79/79 | Loss: 1.699 | Acc: 68.170%\n",
      "Epoch:  106\n",
      "[=================================================>] 391/391 | Loss: 0.073 | Acc: 97.768%\n",
      "[=================================================>] 79/79 | Loss: 1.744 | Acc: 67.740%\n",
      "Epoch:  107\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.790%\n",
      "[=================================================>] 79/79 | Loss: 1.727 | Acc: 68.030%\n",
      "Epoch:  108\n",
      "[=================================================>] 391/391 | Loss: 0.074 | Acc: 97.690%\n",
      "[=================================================>] 79/79 | Loss: 1.700 | Acc: 68.580%\n",
      "Epoch:  109\n",
      "[=================================================>] 391/391 | Loss: 0.073 | Acc: 97.768%\n",
      "[=================================================>] 79/79 | Loss: 1.703 | Acc: 67.870%\n",
      "Epoch:  110\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.960%\n",
      "[=================================================>] 79/79 | Loss: 1.695 | Acc: 68.580%\n",
      "Epoch:  111\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.178%\n",
      "[=================================================>] 79/79 | Loss: 1.721 | Acc: 67.890%\n",
      "Epoch:  112\n",
      "[=================================================>] 391/391 | Loss: 0.055 | Acc: 98.318%\n",
      "[=================================================>] 79/79 | Loss: 1.681 | Acc: 69.240%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  113\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.418%\n",
      "[=================================================>] 79/79 | Loss: 1.694 | Acc: 69.040%\n",
      "Epoch:  114\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.250%\n",
      "[=================================================>] 79/79 | Loss: 1.678 | Acc: 69.070%\n",
      "Epoch:  115\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.548%\n",
      "[=================================================>] 79/79 | Loss: 1.670 | Acc: 68.990%\n",
      "Epoch:  116\n",
      "[=================================================>] 391/391 | Loss: 0.038 | Acc: 98.900%\n",
      "[=================================================>] 79/79 | Loss: 1.679 | Acc: 69.150%\n",
      "Epoch:  117\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.734%\n",
      "[=================================================>] 79/79 | Loss: 1.684 | Acc: 69.310%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  118\n",
      "[=================================================>] 391/391 | Loss: 0.040 | Acc: 98.786%\n",
      "[=================================================>] 79/79 | Loss: 1.673 | Acc: 69.500%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  119\n",
      "[=================================================>] 391/391 | Loss: 0.038 | Acc: 98.846%\n",
      "[=================================================>] 79/79 | Loss: 1.677 | Acc: 69.650%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  120\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 99.054%\n",
      "[=================================================>] 79/79 | Loss: 1.656 | Acc: 69.930%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  121\n",
      "[=================================================>] 391/391 | Loss: 0.031 | Acc: 99.118%\n",
      "[=================================================>] 79/79 | Loss: 1.641 | Acc: 70.080%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  122\n",
      "[=================================================>] 391/391 | Loss: 0.026 | Acc: 99.266%\n",
      "[=================================================>] 79/79 | Loss: 1.676 | Acc: 69.740%\n",
      "Epoch:  123\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.312%\n",
      "[=================================================>] 79/79 | Loss: 1.664 | Acc: 69.890%\n",
      "Epoch:  124\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.308%\n",
      "[=================================================>] 79/79 | Loss: 1.669 | Acc: 70.520%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  125\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.406%\n",
      "[=================================================>] 79/79 | Loss: 1.674 | Acc: 69.660%\n",
      "Epoch:  126\n",
      "[=================================================>] 391/391 | Loss: 0.020 | Acc: 99.436%\n",
      "[=================================================>] 79/79 | Loss: 1.635 | Acc: 70.700%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  127\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.528%\n",
      "[=================================================>] 79/79 | Loss: 1.630 | Acc: 70.500%\n",
      "Epoch:  128\n",
      "[=================================================>] 391/391 | Loss: 0.016 | Acc: 99.582%\n",
      "[=================================================>] 79/79 | Loss: 1.672 | Acc: 69.620%\n",
      "Epoch:  129\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.616%\n",
      "[=================================================>] 79/79 | Loss: 1.653 | Acc: 70.130%\n",
      "Epoch:  130\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.650%\n",
      "[=================================================>] 79/79 | Loss: 1.618 | Acc: 70.650%\n",
      "Epoch:  131\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.702%\n",
      "[=================================================>] 79/79 | Loss: 1.624 | Acc: 70.700%\n",
      "Epoch:  132\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.722%\n",
      "[=================================================>] 79/79 | Loss: 1.609 | Acc: 70.660%\n",
      "Epoch:  133\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.758%\n",
      "[=================================================>] 79/79 | Loss: 1.592 | Acc: 70.870%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  134\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.724%\n",
      "[=================================================>] 79/79 | Loss: 1.600 | Acc: 70.600%\n",
      "Epoch:  135\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.760%\n",
      "[=================================================>] 79/79 | Loss: 1.583 | Acc: 71.010%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  136\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.782%\n",
      "[=================================================>] 79/79 | Loss: 1.596 | Acc: 70.680%\n",
      "Epoch:  137\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.792%\n",
      "[=================================================>] 79/79 | Loss: 1.595 | Acc: 70.970%\n",
      "Epoch:  138\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.806%\n",
      "[=================================================>] 79/79 | Loss: 1.575 | Acc: 71.310%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  139\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.870%\n",
      "[=================================================>] 79/79 | Loss: 1.544 | Acc: 71.670%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  140\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.872%\n",
      "[=================================================>] 79/79 | Loss: 1.551 | Acc: 71.510%\n",
      "Epoch:  141\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.884%\n",
      "[=================================================>] 79/79 | Loss: 1.552 | Acc: 71.240%\n",
      "Epoch:  142\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.852%\n",
      "[=================================================>] 79/79 | Loss: 1.544 | Acc: 71.330%\n",
      "Epoch:  143\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.864%\n",
      "[=================================================>] 79/79 | Loss: 1.538 | Acc: 71.600%\n",
      "Epoch:  144\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.900%\n",
      "[=================================================>] 79/79 | Loss: 1.524 | Acc: 71.810%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  145\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.888%\n",
      "[=================================================>] 79/79 | Loss: 1.522 | Acc: 71.740%\n",
      "Epoch:  146\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.914%\n",
      "[=================================================>] 79/79 | Loss: 1.506 | Acc: 71.910%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  147\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.918%\n",
      "[=================================================>] 79/79 | Loss: 1.503 | Acc: 71.760%\n",
      "Epoch:  148\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.918%\n",
      "[=================================================>] 79/79 | Loss: 1.498 | Acc: 71.930%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  149\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 1.488 | Acc: 71.950%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.936%\n",
      "[=================================================>] 79/79 | Loss: 1.489 | Acc: 71.910%\n",
      "Epoch:  151\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 1.492 | Acc: 71.880%\n",
      "Epoch:  152\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.934%\n",
      "[=================================================>] 79/79 | Loss: 1.481 | Acc: 72.020%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  153\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.942%\n",
      "[=================================================>] 79/79 | Loss: 1.470 | Acc: 71.940%\n",
      "Epoch:  154\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.936%\n",
      "[=================================================>] 79/79 | Loss: 1.471 | Acc: 72.230%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  155\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.924%\n",
      "[=================================================>] 79/79 | Loss: 1.464 | Acc: 72.150%\n",
      "Epoch:  156\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 1.460 | Acc: 71.910%\n",
      "Epoch:  157\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.934%\n",
      "[=================================================>] 79/79 | Loss: 1.459 | Acc: 72.380%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  158\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.920%\n",
      "[=================================================>] 79/79 | Loss: 1.454 | Acc: 72.230%\n",
      "Epoch:  159\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.934%\n",
      "[=================================================>] 79/79 | Loss: 1.452 | Acc: 72.200%\n",
      "Epoch:  160\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.944%\n",
      "[=================================================>] 79/79 | Loss: 1.450 | Acc: 72.220%\n",
      "Epoch:  161\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.443 | Acc: 72.280%\n",
      "Epoch:  162\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.438 | Acc: 72.310%\n",
      "Epoch:  163\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.440 | Acc: 72.300%\n",
      "Epoch:  164\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 1.435 | Acc: 72.290%\n",
      "Epoch:  165\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 1.436 | Acc: 72.480%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  166\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.438 | Acc: 72.440%\n",
      "Epoch:  167\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.942%\n",
      "[=================================================>] 79/79 | Loss: 1.429 | Acc: 72.430%\n",
      "Epoch:  168\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.430 | Acc: 72.350%\n",
      "Epoch:  169\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 1.422 | Acc: 72.330%\n",
      "Epoch:  170\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.431 | Acc: 72.420%\n",
      "Epoch:  171\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.426 | Acc: 72.510%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  172\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.426 | Acc: 72.530%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  173\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.427 | Acc: 72.340%\n",
      "Epoch:  174\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.424 | Acc: 72.400%\n",
      "Epoch:  175\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 1.426 | Acc: 72.580%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  176\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 1.423 | Acc: 72.490%\n",
      "Epoch:  177\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.420 | Acc: 72.410%\n",
      "Epoch:  178\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.419 | Acc: 72.290%\n",
      "Epoch:  179\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.423 | Acc: 72.470%\n",
      "Epoch:  180\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.415 | Acc: 72.420%\n",
      "Epoch:  181\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.418 | Acc: 72.450%\n",
      "Epoch:  182\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.411 | Acc: 72.590%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  183\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.416 | Acc: 72.400%\n",
      "Epoch:  184\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.413 | Acc: 72.510%\n",
      "Epoch:  185\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.411 | Acc: 72.490%\n",
      "Epoch:  186\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.417 | Acc: 72.530%\n",
      "Epoch:  187\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.413 | Acc: 72.380%\n",
      "Epoch:  188\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.410 | Acc: 72.460%\n",
      "Epoch:  189\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.410 | Acc: 72.440%\n",
      "Epoch:  190\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.520%\n",
      "Epoch:  191\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.416 | Acc: 72.140%\n",
      "Epoch:  192\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.409 | Acc: 72.420%\n",
      "Epoch:  193\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 1.413 | Acc: 72.440%\n",
      "Epoch:  194\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.410 | Acc: 72.540%\n",
      "Epoch:  195\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.970%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.470%\n",
      "Epoch:  196\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.412 | Acc: 72.520%\n",
      "Epoch:  197\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.414 | Acc: 72.330%\n",
      "Epoch:  198\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 1.417 | Acc: 72.390%\n",
      "Epoch:  199\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.411 | Acc: 72.330%\n",
      "Epoch:  200\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.550%\n",
      "Epoch:  201\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.411 | Acc: 72.430%\n",
      "Epoch:  202\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.414 | Acc: 72.460%\n",
      "Epoch:  203\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 1.413 | Acc: 72.340%\n",
      "Epoch:  204\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.416 | Acc: 72.390%\n",
      "Epoch:  205\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.409 | Acc: 72.570%\n",
      "Epoch:  206\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.490%\n",
      "Epoch:  207\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 1.414 | Acc: 72.430%\n",
      "Epoch:  208\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.413 | Acc: 72.540%\n",
      "Epoch:  209\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.410 | Acc: 72.390%\n",
      "Epoch:  210\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.411 | Acc: 72.480%\n",
      "Epoch:  211\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.415 | Acc: 72.280%\n",
      "Epoch:  212\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.490%\n",
      "Epoch:  213\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 1.415 | Acc: 72.430%\n",
      "Epoch:  214\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.470%\n",
      "Epoch:  215\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.409 | Acc: 72.490%\n",
      "Epoch:  216\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.410 | Acc: 72.530%\n",
      "Epoch:  217\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.409 | Acc: 72.500%\n",
      "Epoch:  218\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.406 | Acc: 72.520%\n",
      "Epoch:  219\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.409 | Acc: 72.520%\n",
      "Epoch:  220\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 1.411 | Acc: 72.600%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  221\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.360%\n",
      "Epoch:  222\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 1.408 | Acc: 72.540%\n",
      "Epoch:  223\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.412 | Acc: 72.550%\n",
      "Epoch:  224\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 1.405 | Acc: 72.400%\n",
      "Epoch:  225\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 1.399 | Acc: 72.520%\n",
      "Epoch:  226\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 1.407 | Acc: 72.350%\n",
      "Epoch:  227\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.410 | Acc: 72.580%\n",
      "Epoch:  228\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.405 | Acc: 72.350%\n",
      "Epoch:  229\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.399 | Acc: 72.410%\n",
      "Epoch:  230\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.402 | Acc: 72.460%\n",
      "Epoch:  231\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.395 | Acc: 72.390%\n",
      "Epoch:  232\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.398 | Acc: 72.510%\n",
      "Epoch:  233\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 1.399 | Acc: 72.520%\n",
      "Epoch:  234\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.395 | Acc: 72.460%\n",
      "Epoch:  235\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.399 | Acc: 72.450%\n",
      "Epoch:  236\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 1.397 | Acc: 72.490%\n",
      "Epoch:  237\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 1.400 | Acc: 72.640%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  238\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.392 | Acc: 72.780%\n",
      "\n",
      "Saving checkpoint...\n",
      "Epoch:  239\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.395 | Acc: 72.610%\n",
      "Epoch:  240\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.944%\n",
      "[=================================================>] 79/79 | Loss: 1.398 | Acc: 72.450%\n",
      "Epoch:  241\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.389 | Acc: 72.610%\n",
      "Epoch:  242\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.392 | Acc: 72.580%\n",
      "Epoch:  243\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.942%\n",
      "[=================================================>] 79/79 | Loss: 1.390 | Acc: 72.660%\n",
      "Epoch:  244\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.386 | Acc: 72.490%\n",
      "Epoch:  245\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.386 | Acc: 72.520%\n",
      "Epoch:  246\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.381 | Acc: 72.630%\n",
      "Epoch:  247\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.384 | Acc: 72.470%\n",
      "Epoch:  248\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.378 | Acc: 72.660%\n",
      "Epoch:  249\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.375 | Acc: 72.510%\n",
      "Epoch:  250\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.938%\n",
      "[=================================================>] 79/79 | Loss: 1.385 | Acc: 72.560%\n",
      "Epoch:  251\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.384 | Acc: 72.380%\n",
      "Epoch:  252\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 1.385 | Acc: 72.280%\n",
      "Epoch:  253\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.384 | Acc: 72.480%\n",
      "Epoch:  254\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.946%\n",
      "[=================================================>] 79/79 | Loss: 1.379 | Acc: 72.470%\n",
      "Epoch:  255\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 1.377 | Acc: 72.470%\n",
      "Epoch:  256\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.382 | Acc: 72.690%\n",
      "Epoch:  257\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.386 | Acc: 72.210%\n",
      "Epoch:  258\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.383 | Acc: 72.260%\n",
      "Epoch:  259\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 1.383 | Acc: 72.440%\n",
      "Epoch:  260\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.942%\n",
      "[=================================================>] 79/79 | Loss: 1.392 | Acc: 72.430%\n",
      "Epoch:  261\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 1.390 | Acc: 72.170%\n",
      "Epoch:  262\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 1.383 | Acc: 72.170%\n",
      "Epoch:  263\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.936%\n",
      "[=================================================>] 79/79 | Loss: 1.392 | Acc: 72.180%\n",
      "Epoch:  264\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.936%\n",
      "[=================================================>] 79/79 | Loss: 1.396 | Acc: 72.600%\n",
      "Epoch:  265\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.914%\n",
      "[=================================================>] 79/79 | Loss: 1.397 | Acc: 72.390%\n",
      "Epoch:  266\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.912%\n",
      "[=================================================>] 79/79 | Loss: 1.403 | Acc: 72.110%\n",
      "Epoch:  267\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.912%\n",
      "[=================================================>] 79/79 | Loss: 1.414 | Acc: 72.060%\n",
      "Epoch:  268\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.908%\n",
      "[=================================================>] 79/79 | Loss: 1.434 | Acc: 71.830%\n",
      "Epoch:  269\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.864%\n",
      "[=================================================>] 79/79 | Loss: 1.458 | Acc: 71.260%\n",
      "Epoch:  270\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.902%\n",
      "[=================================================>] 79/79 | Loss: 1.446 | Acc: 71.400%\n",
      "Epoch:  271\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.794%\n",
      "[=================================================>] 79/79 | Loss: 1.485 | Acc: 71.470%\n",
      "Epoch:  272\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.816%\n",
      "[=================================================>] 79/79 | Loss: 1.506 | Acc: 70.970%\n",
      "Epoch:  273\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.788%\n",
      "[=================================================>] 79/79 | Loss: 1.562 | Acc: 70.480%\n",
      "Epoch:  274\n",
      "[=================================================>] 391/391 | Loss: 0.044 | Acc: 98.802%\n",
      "[=================================================>] 79/79 | Loss: 1.731 | Acc: 67.500%\n",
      "Epoch:  275\n",
      "[=================================================>] 391/391 | Loss: 0.113 | Acc: 96.722%\n",
      "[=================================================>] 79/79 | Loss: 1.872 | Acc: 64.490%\n",
      "Epoch:  276\n",
      "[=================================================>] 391/391 | Loss: 0.218 | Acc: 93.584%\n",
      "[=================================================>] 79/79 | Loss: 1.793 | Acc: 64.160%\n",
      "Epoch:  277\n",
      "[=================================================>] 391/391 | Loss: 0.226 | Acc: 93.266%\n",
      "[=================================================>] 79/79 | Loss: 1.707 | Acc: 65.090%\n",
      "Epoch:  278\n",
      "[=================================================>] 391/391 | Loss: 0.232 | Acc: 92.994%\n",
      "[=================================================>] 79/79 | Loss: 1.788 | Acc: 64.060%\n",
      "Epoch:  279\n",
      "[=================================================>] 391/391 | Loss: 0.222 | Acc: 93.218%\n",
      "[=================================================>] 79/79 | Loss: 1.847 | Acc: 62.870%\n",
      "Epoch:  280\n",
      "[=================================================>] 391/391 | Loss: 0.222 | Acc: 93.412%\n",
      "[=================================================>] 79/79 | Loss: 1.719 | Acc: 64.990%\n",
      "Epoch:  281\n",
      "[=================================================>] 391/391 | Loss: 0.240 | Acc: 92.722%\n",
      "[=================================================>] 79/79 | Loss: 1.711 | Acc: 64.610%\n",
      "Epoch:  282\n",
      "[=================================================>] 391/391 | Loss: 0.225 | Acc: 93.194%\n",
      "[=================================================>] 79/79 | Loss: 1.639 | Acc: 65.590%\n",
      "Epoch:  283\n",
      "[=================================================>] 391/391 | Loss: 0.225 | Acc: 93.178%\n",
      "[=================================================>] 79/79 | Loss: 1.752 | Acc: 64.040%\n",
      "Epoch:  284\n",
      "[=================================================>] 391/391 | Loss: 0.217 | Acc: 93.404%\n",
      "[=================================================>] 79/79 | Loss: 1.816 | Acc: 64.210%\n",
      "Epoch:  285\n",
      "[=================================================>] 391/391 | Loss: 0.221 | Acc: 93.346%\n",
      "[=================================================>] 79/79 | Loss: 1.778 | Acc: 65.100%\n",
      "Epoch:  286\n",
      "[=================================================>] 391/391 | Loss: 0.227 | Acc: 93.166%\n",
      "[=================================================>] 79/79 | Loss: 1.807 | Acc: 63.250%\n",
      "Epoch:  287\n",
      "[=================================================>] 391/391 | Loss: 0.209 | Acc: 93.756%\n",
      "[=================================================>] 79/79 | Loss: 1.722 | Acc: 64.960%\n",
      "Epoch:  288\n",
      "[=================================================>] 391/391 | Loss: 0.204 | Acc: 93.784%\n",
      "[=================================================>] 79/79 | Loss: 1.741 | Acc: 64.680%\n",
      "Epoch:  289\n",
      "[=================================================>] 391/391 | Loss: 0.222 | Acc: 93.226%\n",
      "[=================================================>] 79/79 | Loss: 1.779 | Acc: 64.330%\n",
      "Epoch:  290\n",
      "[=================================================>] 391/391 | Loss: 0.222 | Acc: 93.268%\n",
      "[=================================================>] 79/79 | Loss: 1.743 | Acc: 64.330%\n",
      "Epoch:  291\n",
      "[=================================================>] 391/391 | Loss: 0.225 | Acc: 93.278%\n",
      "[=================================================>] 79/79 | Loss: 1.722 | Acc: 64.890%\n",
      "Epoch:  292\n",
      "[=================================================>] 391/391 | Loss: 0.209 | Acc: 93.630%\n",
      "[=================================================>] 79/79 | Loss: 1.789 | Acc: 63.950%\n",
      "Epoch:  293\n",
      "[=================================================>] 391/391 | Loss: 0.210 | Acc: 93.556%\n",
      "[=================================================>] 79/79 | Loss: 1.702 | Acc: 65.080%\n",
      "Epoch:  294\n",
      "[=================================================>] 391/391 | Loss: 0.212 | Acc: 93.628%\n",
      "[=================================================>] 79/79 | Loss: 1.713 | Acc: 64.970%\n",
      "Epoch:  295\n",
      "[=================================================>] 391/391 | Loss: 0.219 | Acc: 93.360%\n",
      "[=================================================>] 79/79 | Loss: 1.698 | Acc: 65.170%\n",
      "Epoch:  296\n",
      "[=================================================>] 391/391 | Loss: 0.206 | Acc: 93.820%\n",
      "[=================================================>] 79/79 | Loss: 1.705 | Acc: 65.010%\n",
      "Epoch:  297\n",
      "[=================================================>] 391/391 | Loss: 0.215 | Acc: 93.412%\n",
      "[=================================================>] 79/79 | Loss: 1.728 | Acc: 64.530%\n",
      "Epoch:  298\n",
      "[=================================================>] 391/391 | Loss: 0.207 | Acc: 93.688%\n",
      "[=================================================>] 79/79 | Loss: 1.833 | Acc: 63.380%\n",
      "Epoch:  299\n",
      "[=================================================>] 391/391 | Loss: 0.211 | Acc: 93.572%\n",
      "[=================================================>] 79/79 | Loss: 1.850 | Acc: 63.510%\n",
      "Finished fine tuning.\n"
     ]
    }
   ],
   "source": [
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "epoch = 300\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "pruner = VGG16Pruner(config, model, save_name='vgg16_cifar100_baseline.pth')\n",
    "pruner.train(epoches=epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb63b1",
   "metadata": {},
   "source": [
    "## 4. Global pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a86643",
   "metadata": {},
   "source": [
    "### Global pruning (Archor) - min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c2b1f5",
   "metadata": {},
   "source": [
    "#### CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8787b3",
   "metadata": {},
   "source": [
    "##### ~50% filters pruned (512*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "256165f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 33, 3: 29, 7: 49, 10: 29, 14: 119, 17: 121, 20: 108, 24: 313, 27: 312, 30: 291, 34: 314, 37: 304, 40: 26}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(31, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(35, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(79, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(99, 137, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(137, 135, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(135, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(148, 199, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(199, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(200, 221, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(221, 198, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(198, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(208, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 391/391 | Loss: 0.857 | Acc: 70.906%\n",
      "[=================================================>] 79/79 | Loss: 0.798 | Acc: 74.240%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 391/391 | Loss: 0.535 | Acc: 82.266%\n",
      "[=================================================>] 79/79 | Loss: 0.546 | Acc: 82.240%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 391/391 | Loss: 0.441 | Acc: 85.434%\n",
      "[=================================================>] 79/79 | Loss: 0.495 | Acc: 83.500%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 391/391 | Loss: 0.389 | Acc: 87.200%\n",
      "[=================================================>] 79/79 | Loss: 0.443 | Acc: 85.230%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 391/391 | Loss: 0.349 | Acc: 88.462%\n",
      "[=================================================>] 79/79 | Loss: 0.469 | Acc: 84.900%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 391/391 | Loss: 0.326 | Acc: 89.008%\n",
      "[=================================================>] 79/79 | Loss: 0.442 | Acc: 85.050%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 391/391 | Loss: 0.304 | Acc: 89.878%\n",
      "[=================================================>] 79/79 | Loss: 0.536 | Acc: 83.630%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 391/391 | Loss: 0.278 | Acc: 90.696%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 87.020%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 391/391 | Loss: 0.265 | Acc: 91.120%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 86.850%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 391/391 | Loss: 0.259 | Acc: 91.380%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 88.030%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 391/391 | Loss: 0.240 | Acc: 91.946%\n",
      "[=================================================>] 79/79 | Loss: 0.427 | Acc: 86.480%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 391/391 | Loss: 0.226 | Acc: 92.404%\n",
      "[=================================================>] 79/79 | Loss: 0.432 | Acc: 86.340%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 391/391 | Loss: 0.221 | Acc: 92.666%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 88.510%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 391/391 | Loss: 0.210 | Acc: 93.020%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 87.880%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 391/391 | Loss: 0.201 | Acc: 93.220%\n",
      "[=================================================>] 79/79 | Loss: 0.410 | Acc: 87.020%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 391/391 | Loss: 0.194 | Acc: 93.376%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 87.540%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 391/391 | Loss: 0.195 | Acc: 93.422%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 87.150%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 391/391 | Loss: 0.184 | Acc: 93.816%\n",
      "[=================================================>] 79/79 | Loss: 0.418 | Acc: 87.150%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 391/391 | Loss: 0.176 | Acc: 94.118%\n",
      "[=================================================>] 79/79 | Loss: 0.398 | Acc: 87.880%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 391/391 | Loss: 0.176 | Acc: 94.042%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 89.060%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 391/391 | Loss: 0.166 | Acc: 94.400%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 88.080%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 391/391 | Loss: 0.162 | Acc: 94.546%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 88.650%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 391/391 | Loss: 0.158 | Acc: 94.732%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 87.920%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 391/391 | Loss: 0.159 | Acc: 94.658%\n",
      "[=================================================>] 79/79 | Loss: 0.380 | Acc: 88.770%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 391/391 | Loss: 0.153 | Acc: 94.780%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 88.990%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 391/391 | Loss: 0.146 | Acc: 95.162%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 88.300%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 391/391 | Loss: 0.145 | Acc: 95.158%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 89.270%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 391/391 | Loss: 0.144 | Acc: 95.186%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 89.130%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 391/391 | Loss: 0.138 | Acc: 95.350%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 89.200%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 391/391 | Loss: 0.135 | Acc: 95.482%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 89.020%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 391/391 | Loss: 0.131 | Acc: 95.594%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 88.940%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 391/391 | Loss: 0.135 | Acc: 95.404%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 89.210%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 391/391 | Loss: 0.127 | Acc: 95.654%\n",
      "[=================================================>] 79/79 | Loss: 0.328 | Acc: 90.330%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 391/391 | Loss: 0.125 | Acc: 95.708%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 88.150%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 391/391 | Loss: 0.124 | Acc: 95.782%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 88.420%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 391/391 | Loss: 0.121 | Acc: 95.918%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 89.080%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 391/391 | Loss: 0.116 | Acc: 96.066%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 89.290%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 391/391 | Loss: 0.114 | Acc: 96.138%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 89.410%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 391/391 | Loss: 0.116 | Acc: 96.114%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 89.830%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 391/391 | Loss: 0.112 | Acc: 96.172%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 89.190%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 391/391 | Loss: 0.109 | Acc: 96.320%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 90.040%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 391/391 | Loss: 0.108 | Acc: 96.380%\n",
      "[=================================================>] 79/79 | Loss: 0.391 | Acc: 89.020%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 391/391 | Loss: 0.101 | Acc: 96.586%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 89.720%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 391/391 | Loss: 0.102 | Acc: 96.632%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 90.110%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 391/391 | Loss: 0.099 | Acc: 96.682%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 89.690%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 391/391 | Loss: 0.101 | Acc: 96.562%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 89.750%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 391/391 | Loss: 0.097 | Acc: 96.760%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 89.510%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 391/391 | Loss: 0.096 | Acc: 96.798%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 89.620%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 391/391 | Loss: 0.088 | Acc: 97.036%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 90.030%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 391/391 | Loss: 0.093 | Acc: 96.778%\n",
      "[=================================================>] 79/79 | Loss: 0.334 | Acc: 90.310%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 97.076%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 90.480%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 97.032%\n",
      "[=================================================>] 79/79 | Loss: 0.331 | Acc: 90.660%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 391/391 | Loss: 0.087 | Acc: 97.054%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 89.950%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.224%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 90.460%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 391/391 | Loss: 0.084 | Acc: 97.164%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 90.300%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 391/391 | Loss: 0.080 | Acc: 97.282%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 90.440%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 391/391 | Loss: 0.076 | Acc: 97.452%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 90.440%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 391/391 | Loss: 0.076 | Acc: 97.494%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 90.290%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 391/391 | Loss: 0.074 | Acc: 97.546%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 90.000%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 391/391 | Loss: 0.074 | Acc: 97.472%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 90.090%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.602%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 90.100%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 391/391 | Loss: 0.070 | Acc: 97.658%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.480%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 391/391 | Loss: 0.072 | Acc: 97.560%\n",
      "[=================================================>] 79/79 | Loss: 0.426 | Acc: 89.060%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.754%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 90.400%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.868%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 90.250%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.838%\n",
      "[=================================================>] 79/79 | Loss: 0.380 | Acc: 90.190%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 391/391 | Loss: 0.062 | Acc: 97.942%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 90.220%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.888%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 89.700%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.860%\n",
      "[=================================================>] 79/79 | Loss: 0.398 | Acc: 89.770%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 391/391 | Loss: 0.061 | Acc: 98.024%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 90.810%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.202%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.960%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.224%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.030%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.090%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 89.690%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.276%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 91.130%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 391/391 | Loss: 0.050 | Acc: 98.276%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 90.780%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.344%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 90.350%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.352%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 91.030%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 391/391 | Loss: 0.045 | Acc: 98.504%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 91.140%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.598%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 90.440%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 391/391 | Loss: 0.044 | Acc: 98.390%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 91.070%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.730%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 91.050%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.678%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.460%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 391/391 | Loss: 0.038 | Acc: 98.730%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 91.350%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 391/391 | Loss: 0.035 | Acc: 98.846%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 90.910%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 391/391 | Loss: 0.036 | Acc: 98.818%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 91.180%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 391/391 | Loss: 0.035 | Acc: 98.848%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 91.180%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.994%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 91.950%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 391/391 | Loss: 0.029 | Acc: 99.042%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 90.990%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.904%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 91.790%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.074%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 90.870%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.098%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 91.360%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.094%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 91.050%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.218%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.940%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.330%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 91.470%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.224%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.600%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.332%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 91.640%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 391/391 | Loss: 0.020 | Acc: 99.352%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 91.590%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.460%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 91.620%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.460%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 91.960%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.412%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 91.790%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.396%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 91.650%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 391/391 | Loss: 0.016 | Acc: 99.522%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 91.900%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 391/391 | Loss: 0.016 | Acc: 99.534%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 92.110%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.582%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.260%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.604%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 91.870%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.636%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 91.980%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.754%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.350%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.760%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 92.310%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.708%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.300%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.778%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 92.260%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.790%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 92.320%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.802%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 92.500%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.792%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 92.560%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.812%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 92.240%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.818%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 92.370%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.820%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.370%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.874%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 92.470%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.884%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 92.390%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.860%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 92.490%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.892%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 92.530%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.902%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.440%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.892%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 92.480%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.940%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.520%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.908%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.690%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.914%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 92.680%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 92.790%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.918%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 92.730%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.922%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.860%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.940%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.860%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.830%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.942%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.890%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 92.800%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.944%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.840%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.942%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 93.010%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.938%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 93.000%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.930%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.880%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.900%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 93.040%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.930%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 93.040%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.870%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.960%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.970%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.990%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.970%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.860%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.940%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.890%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.920%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 33.65M => 22.09M (34.3% reduction)\n",
      "MACs: 0.33G => 0.11G (67.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 4\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_min_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fe83b",
   "metadata": {},
   "source": [
    "##### ~36% filters pruned (512*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f904ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 23, 3: 10, 7: 24, 10: 33, 14: 89, 17: 85, 20: 87, 24: 225, 27: 239, 30: 214, 34: 274, 37: 227, 40: 6}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(41, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(54, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(104, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(95, 167, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(167, 171, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(171, 169, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(169, 287, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(287, 273, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(273, 298, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(298, 238, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(238, 285, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(285, 506, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 391/391 | Loss: 0.626 | Acc: 79.648%\n",
      "[=================================================>] 79/79 | Loss: 0.643 | Acc: 79.670%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 391/391 | Loss: 0.394 | Acc: 87.106%\n",
      "[=================================================>] 79/79 | Loss: 0.547 | Acc: 82.490%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 391/391 | Loss: 0.328 | Acc: 89.198%\n",
      "[=================================================>] 79/79 | Loss: 0.516 | Acc: 83.860%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 391/391 | Loss: 0.289 | Acc: 90.382%\n",
      "[=================================================>] 79/79 | Loss: 0.492 | Acc: 84.200%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 391/391 | Loss: 0.261 | Acc: 91.314%\n",
      "[=================================================>] 79/79 | Loss: 0.494 | Acc: 84.050%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 391/391 | Loss: 0.243 | Acc: 91.968%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 87.330%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 391/391 | Loss: 0.222 | Acc: 92.480%\n",
      "[=================================================>] 79/79 | Loss: 0.451 | Acc: 86.480%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 391/391 | Loss: 0.206 | Acc: 93.130%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 88.300%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 391/391 | Loss: 0.196 | Acc: 93.430%\n",
      "[=================================================>] 79/79 | Loss: 0.410 | Acc: 87.500%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 391/391 | Loss: 0.186 | Acc: 93.858%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 88.630%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 391/391 | Loss: 0.179 | Acc: 93.956%\n",
      "[=================================================>] 79/79 | Loss: 0.442 | Acc: 87.130%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 391/391 | Loss: 0.169 | Acc: 94.340%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 88.400%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 391/391 | Loss: 0.164 | Acc: 94.536%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 88.270%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 391/391 | Loss: 0.152 | Acc: 94.878%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 88.560%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 391/391 | Loss: 0.151 | Acc: 94.996%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 88.730%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 391/391 | Loss: 0.147 | Acc: 95.168%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 88.280%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 391/391 | Loss: 0.136 | Acc: 95.522%\n",
      "[=================================================>] 79/79 | Loss: 0.407 | Acc: 88.260%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 391/391 | Loss: 0.135 | Acc: 95.380%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 89.660%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 391/391 | Loss: 0.131 | Acc: 95.510%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 88.610%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 391/391 | Loss: 0.125 | Acc: 95.796%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 89.020%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 391/391 | Loss: 0.125 | Acc: 95.794%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 88.970%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 391/391 | Loss: 0.119 | Acc: 95.998%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 89.390%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 391/391 | Loss: 0.114 | Acc: 96.134%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 89.780%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 391/391 | Loss: 0.111 | Acc: 96.208%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 88.350%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 391/391 | Loss: 0.113 | Acc: 96.240%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 89.480%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 391/391 | Loss: 0.113 | Acc: 96.204%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 88.640%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 391/391 | Loss: 0.104 | Acc: 96.594%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 90.330%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 391/391 | Loss: 0.102 | Acc: 96.664%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 88.810%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 391/391 | Loss: 0.103 | Acc: 96.572%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 89.420%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 391/391 | Loss: 0.100 | Acc: 96.540%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 89.880%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 391/391 | Loss: 0.098 | Acc: 96.764%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 89.260%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 391/391 | Loss: 0.092 | Acc: 96.940%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 90.290%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 391/391 | Loss: 0.096 | Acc: 96.802%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 89.440%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 391/391 | Loss: 0.090 | Acc: 97.010%\n",
      "[=================================================>] 79/79 | Loss: 0.333 | Acc: 90.130%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 96.964%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 90.270%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 97.032%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 89.920%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 391/391 | Loss: 0.091 | Acc: 96.898%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.260%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.378%\n",
      "[=================================================>] 79/79 | Loss: 0.391 | Acc: 89.800%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.276%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 88.920%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 391/391 | Loss: 0.085 | Acc: 97.164%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 90.150%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 391/391 | Loss: 0.077 | Acc: 97.414%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 89.950%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 391/391 | Loss: 0.076 | Acc: 97.430%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 88.780%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.242%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 90.100%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 391/391 | Loss: 0.076 | Acc: 97.424%\n",
      "[=================================================>] 79/79 | Loss: 0.333 | Acc: 90.680%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 391/391 | Loss: 0.075 | Acc: 97.554%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 89.740%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 391/391 | Loss: 0.073 | Acc: 97.486%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 89.410%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 391/391 | Loss: 0.070 | Acc: 97.626%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 90.600%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 391/391 | Loss: 0.073 | Acc: 97.544%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 90.340%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 391/391 | Loss: 0.067 | Acc: 97.792%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 90.500%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.836%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 90.700%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 391/391 | Loss: 0.070 | Acc: 97.602%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 90.690%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 391/391 | Loss: 0.064 | Acc: 97.864%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 90.570%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.918%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 90.890%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 391/391 | Loss: 0.065 | Acc: 97.822%\n",
      "[=================================================>] 79/79 | Loss: 0.409 | Acc: 89.540%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 391/391 | Loss: 0.059 | Acc: 98.018%\n",
      "[=================================================>] 79/79 | Loss: 0.337 | Acc: 91.210%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 391/391 | Loss: 0.059 | Acc: 97.990%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 90.080%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 391/391 | Loss: 0.060 | Acc: 98.040%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 90.370%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.098%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 89.580%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 391/391 | Loss: 0.058 | Acc: 98.050%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 90.470%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 391/391 | Loss: 0.052 | Acc: 98.260%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 90.680%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 391/391 | Loss: 0.054 | Acc: 98.198%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 90.900%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 391/391 | Loss: 0.050 | Acc: 98.316%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 90.450%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 391/391 | Loss: 0.050 | Acc: 98.304%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 90.660%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.296%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 91.120%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.372%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 90.800%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.434%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 90.990%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.508%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 91.240%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.616%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 90.200%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.444%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 90.400%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.514%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 91.160%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.740%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 90.660%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.636%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 91.550%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 391/391 | Loss: 0.040 | Acc: 98.704%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 91.560%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 391/391 | Loss: 0.037 | Acc: 98.820%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 91.370%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 391/391 | Loss: 0.036 | Acc: 98.842%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 91.360%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.922%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 91.100%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 391/391 | Loss: 0.037 | Acc: 98.818%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 91.580%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 391/391 | Loss: 0.030 | Acc: 99.008%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 90.930%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 391/391 | Loss: 0.032 | Acc: 98.980%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 91.350%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 391/391 | Loss: 0.031 | Acc: 99.020%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 91.580%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.114%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 91.240%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 391/391 | Loss: 0.025 | Acc: 99.166%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 91.270%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.090%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 91.790%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.024%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 91.000%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 391/391 | Loss: 0.025 | Acc: 99.176%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 91.650%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.230%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 91.500%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.282%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.090%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.328%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 91.770%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.202%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 91.840%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 391/391 | Loss: 0.020 | Acc: 99.338%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.620%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.420%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 91.790%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 391/391 | Loss: 0.016 | Acc: 99.460%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 91.460%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.442%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 91.660%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.482%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 91.790%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.522%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 91.870%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.582%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 92.000%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.554%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 92.190%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.624%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 92.240%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.658%\n",
      "[=================================================>] 79/79 | Loss: 0.335 | Acc: 92.390%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.670%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 92.460%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.690%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 92.120%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.732%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 92.450%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.730%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.300%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.734%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.370%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.810%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 92.650%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.794%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 92.700%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.800%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.460%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.860%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 92.650%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.860%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 92.570%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.856%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 92.820%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.896%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 92.650%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.866%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.780%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.910%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 92.900%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 93.040%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.906%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 93.030%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.912%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 93.030%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.918%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 92.870%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 93.020%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 93.060%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.170%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 93.030%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 93.120%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 93.120%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 93.060%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.976%%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.130%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.170%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 93.140%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 93.190%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.970%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.260%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.180%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.974%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.210%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.996%%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 93.220%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 93.240%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.160%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.240%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.300%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.210%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.350%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 93.180%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.270%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.250%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.260%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.310%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.974%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.280%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.310%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.200%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.310%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.984%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.290%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.200%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.210%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 33.65M => 24.15M (28.2% reduction)\n",
      "MACs: 0.33G => 0.16G (52.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 3\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_min_30_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bfceb",
   "metadata": {},
   "source": [
    "##### ~72% filters pruned (512*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9159c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 36, 3: 21, 7: 64, 10: 59, 14: 152, 17: 147, 20: 154, 24: 399, 27: 390, 30: 392, 34: 442, 37: 396, 40: 420}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(28, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(43, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(69, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(104, 109, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(109, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(102, 113, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(113, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(122, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(120, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(70, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(116, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 391/391 | Loss: 0.890 | Acc: 70.656%\n",
      "[=================================================>] 79/79 | Loss: 0.942 | Acc: 69.460%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 391/391 | Loss: 0.591 | Acc: 80.392%\n",
      "[=================================================>] 79/79 | Loss: 0.518 | Acc: 82.810%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 391/391 | Loss: 0.497 | Acc: 83.520%\n",
      "[=================================================>] 79/79 | Loss: 0.676 | Acc: 78.030%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 391/391 | Loss: 0.443 | Acc: 85.358%\n",
      "[=================================================>] 79/79 | Loss: 0.513 | Acc: 83.110%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 391/391 | Loss: 0.400 | Acc: 86.786%\n",
      "[=================================================>] 79/79 | Loss: 0.475 | Acc: 84.500%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 391/391 | Loss: 0.374 | Acc: 87.524%\n",
      "[=================================================>] 79/79 | Loss: 0.498 | Acc: 83.650%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 391/391 | Loss: 0.351 | Acc: 88.300%\n",
      "[=================================================>] 79/79 | Loss: 0.468 | Acc: 84.710%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 391/391 | Loss: 0.330 | Acc: 89.012%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 86.420%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 391/391 | Loss: 0.324 | Acc: 89.194%\n",
      "[=================================================>] 79/79 | Loss: 0.555 | Acc: 82.390%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 391/391 | Loss: 0.303 | Acc: 89.922%\n",
      "[=================================================>] 79/79 | Loss: 0.438 | Acc: 85.800%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 391/391 | Loss: 0.284 | Acc: 90.534%\n",
      "[=================================================>] 79/79 | Loss: 0.490 | Acc: 84.460%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 391/391 | Loss: 0.276 | Acc: 90.670%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 87.640%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 391/391 | Loss: 0.265 | Acc: 90.998%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 86.620%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 391/391 | Loss: 0.264 | Acc: 91.162%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 88.000%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 391/391 | Loss: 0.250 | Acc: 91.748%\n",
      "[=================================================>] 79/79 | Loss: 0.407 | Acc: 86.750%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 391/391 | Loss: 0.241 | Acc: 91.790%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 87.200%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 391/391 | Loss: 0.236 | Acc: 92.108%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 88.120%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 391/391 | Loss: 0.229 | Acc: 92.314%\n",
      "[=================================================>] 79/79 | Loss: 0.442 | Acc: 86.880%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 391/391 | Loss: 0.229 | Acc: 92.324%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 87.310%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 391/391 | Loss: 0.217 | Acc: 92.688%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 88.160%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 391/391 | Loss: 0.213 | Acc: 92.694%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 86.900%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 391/391 | Loss: 0.209 | Acc: 92.962%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 88.190%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 391/391 | Loss: 0.199 | Acc: 93.282%\n",
      "[=================================================>] 79/79 | Loss: 0.453 | Acc: 86.330%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 391/391 | Loss: 0.194 | Acc: 93.576%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 88.600%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 391/391 | Loss: 0.199 | Acc: 93.324%\n",
      "[=================================================>] 79/79 | Loss: 0.405 | Acc: 87.010%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 391/391 | Loss: 0.191 | Acc: 93.522%\n",
      "[=================================================>] 79/79 | Loss: 0.451 | Acc: 86.020%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 391/391 | Loss: 0.186 | Acc: 93.742%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 88.020%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 391/391 | Loss: 0.183 | Acc: 93.812%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 88.130%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 391/391 | Loss: 0.177 | Acc: 94.032%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 87.750%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 391/391 | Loss: 0.180 | Acc: 93.982%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 88.510%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 391/391 | Loss: 0.176 | Acc: 94.114%\n",
      "[=================================================>] 79/79 | Loss: 0.445 | Acc: 86.860%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 391/391 | Loss: 0.168 | Acc: 94.148%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 88.550%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 391/391 | Loss: 0.165 | Acc: 94.442%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 88.580%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 391/391 | Loss: 0.166 | Acc: 94.446%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 88.550%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 391/391 | Loss: 0.161 | Acc: 94.540%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 88.590%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 391/391 | Loss: 0.159 | Acc: 94.618%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 88.090%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 391/391 | Loss: 0.157 | Acc: 94.594%\n",
      "[=================================================>] 79/79 | Loss: 0.437 | Acc: 87.290%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 391/391 | Loss: 0.152 | Acc: 94.826%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 89.310%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 391/391 | Loss: 0.151 | Acc: 94.990%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 88.360%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 391/391 | Loss: 0.148 | Acc: 94.962%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 88.610%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 391/391 | Loss: 0.146 | Acc: 95.186%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 88.280%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 391/391 | Loss: 0.143 | Acc: 95.186%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 87.950%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 391/391 | Loss: 0.143 | Acc: 95.124%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 89.300%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 391/391 | Loss: 0.135 | Acc: 95.438%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 88.950%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 391/391 | Loss: 0.140 | Acc: 95.292%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 88.130%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 391/391 | Loss: 0.138 | Acc: 95.314%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 89.370%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 391/391 | Loss: 0.129 | Acc: 95.580%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 89.280%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 391/391 | Loss: 0.135 | Acc: 95.442%\n",
      "[=================================================>] 79/79 | Loss: 0.398 | Acc: 88.490%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 391/391 | Loss: 0.126 | Acc: 95.670%\n",
      "[=================================================>] 79/79 | Loss: 0.456 | Acc: 87.490%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 391/391 | Loss: 0.127 | Acc: 95.690%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 88.970%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 391/391 | Loss: 0.119 | Acc: 96.018%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 88.720%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 391/391 | Loss: 0.124 | Acc: 95.782%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 89.490%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 391/391 | Loss: 0.120 | Acc: 95.942%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 89.240%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 391/391 | Loss: 0.113 | Acc: 96.112%\n",
      "[=================================================>] 79/79 | Loss: 0.420 | Acc: 88.160%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 391/391 | Loss: 0.116 | Acc: 96.048%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 89.160%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 391/391 | Loss: 0.109 | Acc: 96.346%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 89.710%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 391/391 | Loss: 0.107 | Acc: 96.416%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 89.500%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 391/391 | Loss: 0.110 | Acc: 96.230%\n",
      "[=================================================>] 79/79 | Loss: 0.375 | Acc: 89.120%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 391/391 | Loss: 0.109 | Acc: 96.314%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 89.960%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 391/391 | Loss: 0.105 | Acc: 96.498%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 89.860%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 391/391 | Loss: 0.104 | Acc: 96.536%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 89.690%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 391/391 | Loss: 0.101 | Acc: 96.594%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 88.560%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 391/391 | Loss: 0.096 | Acc: 96.754%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 89.650%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 391/391 | Loss: 0.099 | Acc: 96.650%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 89.490%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 391/391 | Loss: 0.096 | Acc: 96.734%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 89.720%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 391/391 | Loss: 0.094 | Acc: 96.810%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 89.140%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 391/391 | Loss: 0.090 | Acc: 96.930%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 89.820%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 391/391 | Loss: 0.088 | Acc: 97.008%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 88.800%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 97.024%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 89.850%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 391/391 | Loss: 0.084 | Acc: 97.074%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 89.140%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 391/391 | Loss: 0.084 | Acc: 97.190%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 89.400%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.212%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 89.750%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.270%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 90.610%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 391/391 | Loss: 0.079 | Acc: 97.360%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 90.470%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 391/391 | Loss: 0.078 | Acc: 97.320%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 90.470%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 391/391 | Loss: 0.076 | Acc: 97.462%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 90.170%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 391/391 | Loss: 0.074 | Acc: 97.504%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 89.600%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 391/391 | Loss: 0.069 | Acc: 97.662%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 90.590%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.536%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 90.180%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.672%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 90.390%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 391/391 | Loss: 0.061 | Acc: 97.884%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 90.350%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 391/391 | Loss: 0.064 | Acc: 97.850%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 90.370%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 391/391 | Loss: 0.061 | Acc: 97.880%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 90.520%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.026%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 90.190%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 391/391 | Loss: 0.054 | Acc: 98.220%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 90.230%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 391/391 | Loss: 0.055 | Acc: 98.166%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 90.270%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 391/391 | Loss: 0.055 | Acc: 98.166%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 90.740%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 391/391 | Loss: 0.050 | Acc: 98.310%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 90.960%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 391/391 | Loss: 0.048 | Acc: 98.382%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 90.570%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.450%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 90.910%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.460%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 90.570%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.474%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 90.690%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.536%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 91.190%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.546%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 90.600%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 391/391 | Loss: 0.036 | Acc: 98.804%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 90.720%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 391/391 | Loss: 0.037 | Acc: 98.780%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 91.030%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.632%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 90.670%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 391/391 | Loss: 0.035 | Acc: 98.842%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 90.970%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.916%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 91.150%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.894%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 91.630%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 391/391 | Loss: 0.029 | Acc: 99.038%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 91.170%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.094%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 90.980%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 391/391 | Loss: 0.029 | Acc: 99.030%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 90.950%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.218%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 91.270%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.248%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 91.460%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.264%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 91.350%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.260%\n",
      "[=================================================>] 79/79 | Loss: 0.375 | Acc: 91.380%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.328%\n",
      "[=================================================>] 79/79 | Loss: 0.375 | Acc: 91.390%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.296%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 91.530%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.444%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 91.730%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.452%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 91.390%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.510%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 91.440%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.564%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 91.770%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.524%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 91.450%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.532%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 91.930%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.614%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 91.610%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.604%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.020%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.644%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 91.630%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.648%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 91.820%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.690%\n",
      "[=================================================>] 79/79 | Loss: 0.375 | Acc: 91.870%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.690%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 92.010%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.670%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 92.060%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.718%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 91.880%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.670%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 92.110%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.762%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 92.130%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.788%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.060%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.796%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 92.210%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.818%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 92.090%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.796%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.290%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.824%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 92.100%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.838%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 92.160%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.822%\n",
      "[=================================================>] 79/79 | Loss: 0.375 | Acc: 92.170%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.834%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 92.240%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.866%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 92.290%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.898%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 92.290%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.866%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.310%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.882%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.320%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.876%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.310%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.908%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.320%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.886%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 92.470%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.866%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.490%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.886%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 92.480%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.878%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 92.390%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.906%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 92.500%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.886%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.420%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.898%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 92.380%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.898%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.480%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.912%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 92.530%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.908%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 92.410%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.894%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 92.510%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 33.65M => 18.15M (46.0% reduction)\n",
      "MACs: 0.33G => 0.07G (79.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 6\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_min_70_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82397936",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4453d97",
   "metadata": {},
   "source": [
    "##### ~50% filters pruned (512*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaeaa122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.790%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 29, 3: 19, 7: 34, 10: 49, 14: 115, 17: 115, 20: 107, 24: 301, 27: 296, 30: 296, 34: 312, 37: 294, 40: 81}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(35, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(45, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(94, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(79, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(141, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(141, 149, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(149, 211, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(211, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(216, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(216, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(200, 218, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(218, 431, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 4.046 | Acc: 6.762%\n",
      "[=================================================>] 157/157 | Loss: 3.587 | Acc: 13.080%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 3.221 | Acc: 18.328%\n",
      "[=================================================>] 157/157 | Loss: 2.935 | Acc: 22.800%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 2.747 | Acc: 27.574%\n",
      "[=================================================>] 157/157 | Loss: 2.575 | Acc: 32.150%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 2.477 | Acc: 33.648%\n",
      "[=================================================>] 157/157 | Loss: 2.657 | Acc: 31.480%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 2.278 | Acc: 38.642%\n",
      "[=================================================>] 157/157 | Loss: 2.446 | Acc: 35.990%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 2.145 | Acc: 41.410%\n",
      "[=================================================>] 157/157 | Loss: 2.192 | Acc: 40.590%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 2.044 | Acc: 43.978%\n",
      "[=================================================>] 157/157 | Loss: 1.994 | Acc: 45.760%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 1.957 | Acc: 46.132%\n",
      "[=================================================>] 157/157 | Loss: 2.284 | Acc: 41.670%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 1.875 | Acc: 48.248%\n",
      "[=================================================>] 157/157 | Loss: 1.953 | Acc: 46.470%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 1.813 | Acc: 49.782%\n",
      "[=================================================>] 157/157 | Loss: 1.825 | Acc: 50.110%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 1.746 | Acc: 51.290%\n",
      "[=================================================>] 157/157 | Loss: 1.946 | Acc: 48.390%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 1.698 | Acc: 52.648%\n",
      "[=================================================>] 157/157 | Loss: 1.813 | Acc: 49.950%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 1.653 | Acc: 53.610%\n",
      "[=================================================>] 157/157 | Loss: 1.940 | Acc: 48.360%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 1.609 | Acc: 55.106%\n",
      "[=================================================>] 157/157 | Loss: 1.677 | Acc: 53.500%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 1.567 | Acc: 56.032%\n",
      "[=================================================>] 157/157 | Loss: 1.694 | Acc: 53.280%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 1.529 | Acc: 57.072%\n",
      "[=================================================>] 157/157 | Loss: 1.774 | Acc: 53.140%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 1.498 | Acc: 57.800%\n",
      "[=================================================>] 157/157 | Loss: 1.674 | Acc: 54.900%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 1.471 | Acc: 58.534%\n",
      "[=================================================>] 157/157 | Loss: 1.847 | Acc: 51.810%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 1.434 | Acc: 59.472%\n",
      "[=================================================>] 157/157 | Loss: 1.716 | Acc: 54.290%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 1.406 | Acc: 60.380%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 54.610%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 1.375 | Acc: 61.206%\n",
      "[=================================================>] 157/157 | Loss: 1.585 | Acc: 57.230%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 1.351 | Acc: 61.670%\n",
      "[=================================================>] 157/157 | Loss: 1.658 | Acc: 56.070%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 1.337 | Acc: 62.312%\n",
      "[=================================================>] 157/157 | Loss: 1.646 | Acc: 56.600%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 1.314 | Acc: 62.664%\n",
      "[=================================================>] 157/157 | Loss: 1.592 | Acc: 57.560%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 1.300 | Acc: 63.130%\n",
      "[=================================================>] 157/157 | Loss: 1.627 | Acc: 56.710%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 1.269 | Acc: 63.870%\n",
      "[=================================================>] 157/157 | Loss: 1.559 | Acc: 57.770%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 1.250 | Acc: 64.594%\n",
      "[=================================================>] 157/157 | Loss: 1.662 | Acc: 57.120%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 1.234 | Acc: 64.894%\n",
      "[=================================================>] 157/157 | Loss: 1.583 | Acc: 58.210%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 1.220 | Acc: 65.300%\n",
      "[=================================================>] 157/157 | Loss: 1.496 | Acc: 59.380%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 1.198 | Acc: 65.860%\n",
      "[=================================================>] 157/157 | Loss: 1.500 | Acc: 59.400%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 1.185 | Acc: 66.162%\n",
      "[=================================================>] 157/157 | Loss: 1.573 | Acc: 58.890%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 1.172 | Acc: 66.722%\n",
      "[=================================================>] 157/157 | Loss: 1.605 | Acc: 57.990%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 1.149 | Acc: 66.962%\n",
      "[=================================================>] 157/157 | Loss: 1.628 | Acc: 57.570%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 1.140 | Acc: 67.312%\n",
      "[=================================================>] 157/157 | Loss: 1.472 | Acc: 60.210%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 1.126 | Acc: 67.636%\n",
      "[=================================================>] 157/157 | Loss: 1.562 | Acc: 58.880%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 1.105 | Acc: 68.394%\n",
      "[=================================================>] 157/157 | Loss: 1.600 | Acc: 58.210%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 1.108 | Acc: 68.116%\n",
      "[=================================================>] 157/157 | Loss: 1.486 | Acc: 60.210%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 1.083 | Acc: 68.874%\n",
      "[=================================================>] 157/157 | Loss: 1.535 | Acc: 59.210%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 1.064 | Acc: 69.536%\n",
      "[=================================================>] 157/157 | Loss: 1.464 | Acc: 60.700%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 1.056 | Acc: 69.480%\n",
      "[=================================================>] 157/157 | Loss: 1.530 | Acc: 60.120%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 1.036 | Acc: 70.064%\n",
      "[=================================================>] 157/157 | Loss: 1.526 | Acc: 60.130%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 1.034 | Acc: 70.088%\n",
      "[=================================================>] 157/157 | Loss: 1.512 | Acc: 60.770%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 1.024 | Acc: 70.522%\n",
      "[=================================================>] 157/157 | Loss: 1.467 | Acc: 61.640%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 1.001 | Acc: 70.924%\n",
      "[=================================================>] 157/157 | Loss: 1.453 | Acc: 61.900%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 0.997 | Acc: 71.214%\n",
      "[=================================================>] 157/157 | Loss: 1.516 | Acc: 60.560%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 0.983 | Acc: 71.414%\n",
      "[=================================================>] 157/157 | Loss: 1.608 | Acc: 59.770%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 0.976 | Acc: 71.888%\n",
      "[=================================================>] 157/157 | Loss: 1.459 | Acc: 61.790%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 0.971 | Acc: 72.048%\n",
      "[=================================================>] 157/157 | Loss: 1.497 | Acc: 61.970%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 0.938 | Acc: 72.900%\n",
      "[=================================================>] 157/157 | Loss: 1.524 | Acc: 61.160%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 0.944 | Acc: 72.472%\n",
      "[=================================================>] 157/157 | Loss: 1.533 | Acc: 60.350%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 0.930 | Acc: 72.884%\n",
      "[=================================================>] 157/157 | Loss: 1.491 | Acc: 61.860%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 0.911 | Acc: 73.444%\n",
      "[=================================================>] 157/157 | Loss: 1.499 | Acc: 62.230%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 0.914 | Acc: 73.180%\n",
      "[=================================================>] 157/157 | Loss: 1.498 | Acc: 61.870%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 0.895 | Acc: 74.026%\n",
      "[=================================================>] 157/157 | Loss: 1.457 | Acc: 62.040%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 0.882 | Acc: 74.154%\n",
      "[=================================================>] 157/157 | Loss: 1.639 | Acc: 59.320%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 0.879 | Acc: 74.334%\n",
      "[=================================================>] 157/157 | Loss: 1.470 | Acc: 61.650%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 0.858 | Acc: 74.972%\n",
      "[=================================================>] 157/157 | Loss: 1.503 | Acc: 61.870%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 0.845 | Acc: 75.176%\n",
      "[=================================================>] 157/157 | Loss: 1.439 | Acc: 62.980%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 0.832 | Acc: 75.482%\n",
      "[=================================================>] 157/157 | Loss: 1.498 | Acc: 61.970%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 0.830 | Acc: 75.746%\n",
      "[=================================================>] 157/157 | Loss: 1.447 | Acc: 63.260%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 0.818 | Acc: 75.876%\n",
      "[=================================================>] 157/157 | Loss: 1.484 | Acc: 62.950%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 0.807 | Acc: 76.204%\n",
      "[=================================================>] 157/157 | Loss: 1.412 | Acc: 63.900%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 0.807 | Acc: 76.100%\n",
      "[=================================================>] 157/157 | Loss: 1.409 | Acc: 64.110%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 0.776 | Acc: 76.938%\n",
      "[=================================================>] 157/157 | Loss: 1.524 | Acc: 62.020%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 0.778 | Acc: 77.042%\n",
      "[=================================================>] 157/157 | Loss: 1.480 | Acc: 62.980%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 0.764 | Acc: 77.466%\n",
      "[=================================================>] 157/157 | Loss: 1.468 | Acc: 63.270%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 0.754 | Acc: 77.822%\n",
      "[=================================================>] 157/157 | Loss: 1.543 | Acc: 62.390%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 0.739 | Acc: 78.202%\n",
      "[=================================================>] 157/157 | Loss: 1.439 | Acc: 63.870%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 0.733 | Acc: 78.318%\n",
      "[=================================================>] 157/157 | Loss: 1.438 | Acc: 63.630%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 0.722 | Acc: 78.494%\n",
      "[=================================================>] 157/157 | Loss: 1.431 | Acc: 63.760%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.714 | Acc: 78.946%\n",
      "[=================================================>] 157/157 | Loss: 1.493 | Acc: 63.350%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.697 | Acc: 79.178%\n",
      "[=================================================>] 157/157 | Loss: 1.397 | Acc: 64.660%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.689 | Acc: 79.532%\n",
      "[=================================================>] 157/157 | Loss: 1.455 | Acc: 63.780%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.672 | Acc: 79.908%\n",
      "[=================================================>] 157/157 | Loss: 1.481 | Acc: 64.000%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.662 | Acc: 79.930%\n",
      "[=================================================>] 157/157 | Loss: 1.462 | Acc: 64.780%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.650 | Acc: 80.454%\n",
      "[=================================================>] 157/157 | Loss: 1.441 | Acc: 64.890%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.647 | Acc: 80.566%\n",
      "[=================================================>] 157/157 | Loss: 1.465 | Acc: 64.580%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.629 | Acc: 81.036%\n",
      "[=================================================>] 157/157 | Loss: 1.446 | Acc: 64.490%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.615 | Acc: 81.496%\n",
      "[=================================================>] 157/157 | Loss: 1.451 | Acc: 64.740%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.611 | Acc: 81.568%\n",
      "[=================================================>] 157/157 | Loss: 1.427 | Acc: 65.100%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.588 | Acc: 82.264%\n",
      "[=================================================>] 157/157 | Loss: 1.444 | Acc: 65.330%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.581 | Acc: 82.620%\n",
      "[=================================================>] 157/157 | Loss: 1.427 | Acc: 65.480%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.577 | Acc: 82.592%\n",
      "[=================================================>] 157/157 | Loss: 1.470 | Acc: 64.750%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.558 | Acc: 83.136%\n",
      "[=================================================>] 157/157 | Loss: 1.540 | Acc: 63.820%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.550 | Acc: 83.272%\n",
      "[=================================================>] 157/157 | Loss: 1.513 | Acc: 63.930%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.534 | Acc: 83.768%\n",
      "[=================================================>] 157/157 | Loss: 1.440 | Acc: 65.870%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.519 | Acc: 84.148%\n",
      "[=================================================>] 157/157 | Loss: 1.466 | Acc: 65.900%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.509 | Acc: 84.484%\n",
      "[=================================================>] 157/157 | Loss: 1.491 | Acc: 64.990%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.514 | Acc: 84.302%\n",
      "[=================================================>] 157/157 | Loss: 1.436 | Acc: 66.650%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.481 | Acc: 85.238%\n",
      "[=================================================>] 157/157 | Loss: 1.488 | Acc: 65.970%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.480 | Acc: 85.330%\n",
      "[=================================================>] 157/157 | Loss: 1.465 | Acc: 66.300%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.461 | Acc: 85.644%\n",
      "[=================================================>] 157/157 | Loss: 1.462 | Acc: 66.510%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.450 | Acc: 86.160%\n",
      "[=================================================>] 157/157 | Loss: 1.499 | Acc: 65.700%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.445 | Acc: 86.334%\n",
      "[=================================================>] 157/157 | Loss: 1.501 | Acc: 65.930%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.424 | Acc: 86.918%\n",
      "[=================================================>] 157/157 | Loss: 1.494 | Acc: 66.040%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.408 | Acc: 87.258%\n",
      "[=================================================>] 157/157 | Loss: 1.486 | Acc: 65.910%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.403 | Acc: 87.562%\n",
      "[=================================================>] 157/157 | Loss: 1.542 | Acc: 66.080%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.384 | Acc: 87.946%\n",
      "[=================================================>] 157/157 | Loss: 1.523 | Acc: 66.640%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.374 | Acc: 88.436%\n",
      "[=================================================>] 157/157 | Loss: 1.502 | Acc: 66.410%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.366 | Acc: 88.564%\n",
      "[=================================================>] 157/157 | Loss: 1.548 | Acc: 66.180%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.360 | Acc: 88.676%\n",
      "[=================================================>] 157/157 | Loss: 1.537 | Acc: 66.320%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.345 | Acc: 89.094%\n",
      "[=================================================>] 157/157 | Loss: 1.517 | Acc: 66.550%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.324 | Acc: 89.958%\n",
      "[=================================================>] 157/157 | Loss: 1.523 | Acc: 66.980%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.313 | Acc: 90.206%\n",
      "[=================================================>] 157/157 | Loss: 1.543 | Acc: 66.930%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.300 | Acc: 90.436%\n",
      "[=================================================>] 157/157 | Loss: 1.561 | Acc: 66.880%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.291 | Acc: 90.784%\n",
      "[=================================================>] 157/157 | Loss: 1.550 | Acc: 67.290%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.274 | Acc: 91.400%\n",
      "[=================================================>] 157/157 | Loss: 1.568 | Acc: 66.770%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.265 | Acc: 91.646%\n",
      "[=================================================>] 157/157 | Loss: 1.603 | Acc: 67.180%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.268 | Acc: 91.554%\n",
      "[=================================================>] 157/157 | Loss: 1.570 | Acc: 67.470%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.248 | Acc: 92.038%\n",
      "[=================================================>] 157/157 | Loss: 1.566 | Acc: 67.820%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.234 | Acc: 92.568%\n",
      "[=================================================>] 157/157 | Loss: 1.623 | Acc: 67.300%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.227 | Acc: 92.778%\n",
      "[=================================================>] 157/157 | Loss: 1.605 | Acc: 67.760%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.217 | Acc: 93.066%\n",
      "[=================================================>] 157/157 | Loss: 1.624 | Acc: 67.350%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.203 | Acc: 93.604%\n",
      "[=================================================>] 157/157 | Loss: 1.617 | Acc: 67.470%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.193 | Acc: 93.866%\n",
      "[=================================================>] 157/157 | Loss: 1.636 | Acc: 67.830%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.181 | Acc: 94.214%\n",
      "[=================================================>] 157/157 | Loss: 1.629 | Acc: 68.180%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.168 | Acc: 94.640%\n",
      "[=================================================>] 157/157 | Loss: 1.616 | Acc: 68.570%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.162 | Acc: 94.816%\n",
      "[=================================================>] 157/157 | Loss: 1.633 | Acc: 68.540%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.151 | Acc: 95.152%\n",
      "[=================================================>] 157/157 | Loss: 1.703 | Acc: 67.960%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.146 | Acc: 95.356%\n",
      "[=================================================>] 157/157 | Loss: 1.674 | Acc: 68.390%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.134 | Acc: 95.696%\n",
      "[=================================================>] 157/157 | Loss: 1.711 | Acc: 68.320%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.127 | Acc: 95.956%\n",
      "[=================================================>] 157/157 | Loss: 1.680 | Acc: 68.410%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.121 | Acc: 96.024%\n",
      "[=================================================>] 157/157 | Loss: 1.699 | Acc: 68.840%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.115 | Acc: 96.268%\n",
      "[=================================================>] 157/157 | Loss: 1.693 | Acc: 68.760%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.108 | Acc: 96.604%\n",
      "[=================================================>] 157/157 | Loss: 1.707 | Acc: 68.440%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.103 | Acc: 96.730%\n",
      "[=================================================>] 157/157 | Loss: 1.721 | Acc: 68.560%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.098 | Acc: 96.902%\n",
      "[=================================================>] 157/157 | Loss: 1.710 | Acc: 68.860%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.090 | Acc: 97.072%\n",
      "[=================================================>] 157/157 | Loss: 1.725 | Acc: 69.110%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.087 | Acc: 97.324%\n",
      "[=================================================>] 157/157 | Loss: 1.738 | Acc: 69.040%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.085 | Acc: 97.370%\n",
      "[=================================================>] 157/157 | Loss: 1.729 | Acc: 69.280%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.082 | Acc: 97.418%\n",
      "[=================================================>] 157/157 | Loss: 1.740 | Acc: 69.240%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.074 | Acc: 97.728%\n",
      "[=================================================>] 157/157 | Loss: 1.752 | Acc: 69.240%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.075 | Acc: 97.548%\n",
      "[=================================================>] 157/157 | Loss: 1.737 | Acc: 69.420%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.068 | Acc: 97.852%\n",
      "[=================================================>] 157/157 | Loss: 1.738 | Acc: 69.500%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.066 | Acc: 97.920%\n",
      "[=================================================>] 157/157 | Loss: 1.758 | Acc: 69.220%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.061 | Acc: 98.106%\n",
      "[=================================================>] 157/157 | Loss: 1.758 | Acc: 69.630%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.061 | Acc: 98.052%\n",
      "[=================================================>] 157/157 | Loss: 1.749 | Acc: 69.890%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.061 | Acc: 98.052%\n",
      "[=================================================>] 157/157 | Loss: 1.770 | Acc: 69.400%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.057 | Acc: 98.270%\n",
      "[=================================================>] 157/157 | Loss: 1.758 | Acc: 69.410%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.056 | Acc: 98.356%\n",
      "[=================================================>] 157/157 | Loss: 1.747 | Acc: 69.770%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.055 | Acc: 98.386%\n",
      "[=================================================>] 157/157 | Loss: 1.772 | Acc: 69.570%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.056 | Acc: 98.310%\n",
      "[=================================================>] 157/157 | Loss: 1.772 | Acc: 69.470%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.054 | Acc: 98.376%\n",
      "[=================================================>] 157/157 | Loss: 1.774 | Acc: 69.310%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.052 | Acc: 98.446%\n",
      "[=================================================>] 157/157 | Loss: 1.781 | Acc: 69.400%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.051 | Acc: 98.468%\n",
      "[=================================================>] 157/157 | Loss: 1.767 | Acc: 69.850%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.053 | Acc: 98.342%\n",
      "[=================================================>] 157/157 | Loss: 1.775 | Acc: 69.380%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.048 | Acc: 98.598%\n",
      "[=================================================>] 157/157 | Loss: 1.777 | Acc: 69.530%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.049 | Acc: 98.510%\n",
      "[=================================================>] 157/157 | Loss: 1.764 | Acc: 69.910%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.053 | Acc: 98.368%\n",
      "[=================================================>] 157/157 | Loss: 1.775 | Acc: 69.630%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.051 | Acc: 98.448%\n",
      "[=================================================>] 157/157 | Loss: 1.771 | Acc: 69.810%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 22.30M (34.5% reduction)\n",
      "MACs: 0.33G => 0.12G (65.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 4\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_min_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1bdfd2",
   "metadata": {},
   "source": [
    "##### ~36% filters pruned (512*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6517e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.790%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 20, 3: 14, 7: 29, 10: 33, 14: 77, 17: 76, 20: 81, 24: 232, 27: 249, 30: 202, 34: 244, 37: 240, 40: 39}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(44, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(50, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(99, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(95, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(179, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(180, 175, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(175, 280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(280, 263, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(263, 310, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(310, 268, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(268, 272, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(272, 473, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 3.478 | Acc: 15.598%\n",
      "[=================================================>] 157/157 | Loss: 3.304 | Acc: 21.250%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 2.364 | Acc: 36.364%\n",
      "[=================================================>] 157/157 | Loss: 2.360 | Acc: 38.700%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 2.023 | Acc: 44.986%\n",
      "[=================================================>] 157/157 | Loss: 2.072 | Acc: 45.560%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 1.842 | Acc: 49.176%\n",
      "[=================================================>] 157/157 | Loss: 1.893 | Acc: 49.550%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 1.698 | Acc: 52.812%\n",
      "[=================================================>] 157/157 | Loss: 1.846 | Acc: 50.920%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 1.598 | Acc: 55.372%\n",
      "[=================================================>] 157/157 | Loss: 1.707 | Acc: 53.960%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 1.511 | Acc: 57.752%\n",
      "[=================================================>] 157/157 | Loss: 1.702 | Acc: 53.000%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 1.439 | Acc: 59.482%\n",
      "[=================================================>] 157/157 | Loss: 1.749 | Acc: 54.220%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 1.386 | Acc: 60.910%\n",
      "[=================================================>] 157/157 | Loss: 1.586 | Acc: 56.430%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 1.336 | Acc: 62.030%\n",
      "[=================================================>] 157/157 | Loss: 1.562 | Acc: 58.210%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 1.295 | Acc: 63.218%\n",
      "[=================================================>] 157/157 | Loss: 1.511 | Acc: 58.950%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 1.248 | Acc: 64.384%\n",
      "[=================================================>] 157/157 | Loss: 1.551 | Acc: 58.750%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 1.207 | Acc: 65.736%\n",
      "[=================================================>] 157/157 | Loss: 1.517 | Acc: 59.690%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 1.182 | Acc: 66.232%\n",
      "[=================================================>] 157/157 | Loss: 1.511 | Acc: 59.320%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 1.155 | Acc: 67.044%\n",
      "[=================================================>] 157/157 | Loss: 1.509 | Acc: 59.850%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 1.122 | Acc: 68.008%\n",
      "[=================================================>] 157/157 | Loss: 1.487 | Acc: 60.540%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 1.098 | Acc: 68.862%\n",
      "[=================================================>] 157/157 | Loss: 1.430 | Acc: 61.840%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 1.084 | Acc: 68.886%\n",
      "[=================================================>] 157/157 | Loss: 1.489 | Acc: 60.700%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 1.053 | Acc: 69.644%\n",
      "[=================================================>] 157/157 | Loss: 1.488 | Acc: 60.660%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 1.039 | Acc: 70.076%\n",
      "[=================================================>] 157/157 | Loss: 1.434 | Acc: 62.030%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 1.011 | Acc: 70.878%\n",
      "[=================================================>] 157/157 | Loss: 1.589 | Acc: 58.960%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 0.996 | Acc: 70.938%\n",
      "[=================================================>] 157/157 | Loss: 1.510 | Acc: 60.450%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 0.988 | Acc: 71.494%\n",
      "[=================================================>] 157/157 | Loss: 1.475 | Acc: 61.390%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 0.967 | Acc: 72.074%\n",
      "[=================================================>] 157/157 | Loss: 1.403 | Acc: 62.850%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 0.943 | Acc: 72.620%\n",
      "[=================================================>] 157/157 | Loss: 1.516 | Acc: 61.570%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 0.935 | Acc: 73.008%\n",
      "[=================================================>] 157/157 | Loss: 1.498 | Acc: 61.700%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 0.926 | Acc: 73.130%\n",
      "[=================================================>] 157/157 | Loss: 1.464 | Acc: 61.310%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 0.902 | Acc: 73.758%\n",
      "[=================================================>] 157/157 | Loss: 1.480 | Acc: 61.670%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 0.895 | Acc: 74.086%\n",
      "[=================================================>] 157/157 | Loss: 1.527 | Acc: 60.720%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 0.875 | Acc: 74.574%\n",
      "[=================================================>] 157/157 | Loss: 1.381 | Acc: 64.310%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 0.862 | Acc: 74.824%\n",
      "[=================================================>] 157/157 | Loss: 1.394 | Acc: 64.120%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 0.862 | Acc: 74.926%\n",
      "[=================================================>] 157/157 | Loss: 1.377 | Acc: 63.970%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 0.833 | Acc: 75.828%\n",
      "[=================================================>] 157/157 | Loss: 1.603 | Acc: 60.240%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 0.836 | Acc: 75.592%\n",
      "[=================================================>] 157/157 | Loss: 1.491 | Acc: 61.770%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 0.818 | Acc: 76.174%\n",
      "[=================================================>] 157/157 | Loss: 1.512 | Acc: 62.260%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 0.808 | Acc: 76.348%\n",
      "[=================================================>] 157/157 | Loss: 1.435 | Acc: 62.910%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 0.794 | Acc: 76.886%\n",
      "[=================================================>] 157/157 | Loss: 1.481 | Acc: 62.560%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 0.785 | Acc: 76.868%\n",
      "[=================================================>] 157/157 | Loss: 1.381 | Acc: 64.400%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 0.783 | Acc: 76.906%\n",
      "[=================================================>] 157/157 | Loss: 1.468 | Acc: 63.440%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 0.765 | Acc: 77.548%\n",
      "[=================================================>] 157/157 | Loss: 1.414 | Acc: 64.100%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 0.748 | Acc: 78.042%\n",
      "[=================================================>] 157/157 | Loss: 1.409 | Acc: 63.780%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 0.743 | Acc: 78.346%\n",
      "[=================================================>] 157/157 | Loss: 1.496 | Acc: 62.640%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 0.729 | Acc: 78.548%\n",
      "[=================================================>] 157/157 | Loss: 1.541 | Acc: 62.570%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 0.731 | Acc: 78.596%\n",
      "[=================================================>] 157/157 | Loss: 1.441 | Acc: 63.580%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 0.714 | Acc: 79.030%\n",
      "[=================================================>] 157/157 | Loss: 1.433 | Acc: 64.120%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 0.711 | Acc: 78.896%\n",
      "[=================================================>] 157/157 | Loss: 1.484 | Acc: 62.870%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 0.696 | Acc: 79.160%\n",
      "[=================================================>] 157/157 | Loss: 1.509 | Acc: 63.680%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 0.695 | Acc: 79.210%\n",
      "[=================================================>] 157/157 | Loss: 1.433 | Acc: 64.330%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 0.677 | Acc: 79.844%\n",
      "[=================================================>] 157/157 | Loss: 1.449 | Acc: 64.110%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 0.659 | Acc: 80.422%\n",
      "[=================================================>] 157/157 | Loss: 1.505 | Acc: 63.270%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 0.653 | Acc: 80.536%\n",
      "[=================================================>] 157/157 | Loss: 1.517 | Acc: 63.180%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 0.642 | Acc: 80.952%\n",
      "[=================================================>] 157/157 | Loss: 1.455 | Acc: 64.200%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 0.637 | Acc: 80.986%\n",
      "[=================================================>] 157/157 | Loss: 1.480 | Acc: 63.540%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 0.634 | Acc: 81.110%\n",
      "[=================================================>] 157/157 | Loss: 1.474 | Acc: 63.640%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 0.615 | Acc: 81.696%\n",
      "[=================================================>] 157/157 | Loss: 1.412 | Acc: 65.150%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 0.602 | Acc: 82.024%\n",
      "[=================================================>] 157/157 | Loss: 1.459 | Acc: 64.820%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 0.601 | Acc: 82.206%\n",
      "[=================================================>] 157/157 | Loss: 1.427 | Acc: 64.840%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 0.592 | Acc: 82.298%\n",
      "[=================================================>] 157/157 | Loss: 1.448 | Acc: 64.330%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 0.586 | Acc: 82.428%\n",
      "[=================================================>] 157/157 | Loss: 1.396 | Acc: 65.640%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 0.570 | Acc: 82.728%\n",
      "[=================================================>] 157/157 | Loss: 1.478 | Acc: 64.050%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 0.554 | Acc: 83.280%\n",
      "[=================================================>] 157/157 | Loss: 1.473 | Acc: 64.950%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 0.553 | Acc: 83.192%\n",
      "[=================================================>] 157/157 | Loss: 1.434 | Acc: 65.820%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 0.551 | Acc: 83.680%\n",
      "[=================================================>] 157/157 | Loss: 1.460 | Acc: 65.650%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 0.530 | Acc: 84.038%\n",
      "[=================================================>] 157/157 | Loss: 1.426 | Acc: 66.090%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 0.517 | Acc: 84.334%\n",
      "[=================================================>] 157/157 | Loss: 1.469 | Acc: 65.840%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 0.511 | Acc: 84.530%\n",
      "[=================================================>] 157/157 | Loss: 1.467 | Acc: 65.350%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 0.502 | Acc: 84.720%\n",
      "[=================================================>] 157/157 | Loss: 1.506 | Acc: 65.070%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 0.488 | Acc: 85.318%\n",
      "[=================================================>] 157/157 | Loss: 1.507 | Acc: 64.870%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 0.483 | Acc: 85.370%\n",
      "[=================================================>] 157/157 | Loss: 1.493 | Acc: 65.040%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 0.469 | Acc: 85.818%\n",
      "[=================================================>] 157/157 | Loss: 1.462 | Acc: 65.080%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.468 | Acc: 85.740%\n",
      "[=================================================>] 157/157 | Loss: 1.482 | Acc: 65.330%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.464 | Acc: 86.134%\n",
      "[=================================================>] 157/157 | Loss: 1.475 | Acc: 65.860%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.440 | Acc: 86.720%\n",
      "[=================================================>] 157/157 | Loss: 1.473 | Acc: 66.290%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.429 | Acc: 86.790%\n",
      "[=================================================>] 157/157 | Loss: 1.488 | Acc: 65.510%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.423 | Acc: 87.116%\n",
      "[=================================================>] 157/157 | Loss: 1.475 | Acc: 66.300%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.413 | Acc: 87.294%\n",
      "[=================================================>] 157/157 | Loss: 1.521 | Acc: 66.260%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.404 | Acc: 87.720%\n",
      "[=================================================>] 157/157 | Loss: 1.458 | Acc: 66.740%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.399 | Acc: 87.714%\n",
      "[=================================================>] 157/157 | Loss: 1.476 | Acc: 66.590%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.387 | Acc: 88.112%\n",
      "[=================================================>] 157/157 | Loss: 1.500 | Acc: 66.010%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.377 | Acc: 88.376%\n",
      "[=================================================>] 157/157 | Loss: 1.432 | Acc: 67.130%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.361 | Acc: 88.712%\n",
      "[=================================================>] 157/157 | Loss: 1.491 | Acc: 66.380%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.361 | Acc: 88.942%\n",
      "[=================================================>] 157/157 | Loss: 1.507 | Acc: 65.700%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.341 | Acc: 89.460%\n",
      "[=================================================>] 157/157 | Loss: 1.455 | Acc: 67.970%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.337 | Acc: 89.628%\n",
      "[=================================================>] 157/157 | Loss: 1.524 | Acc: 66.550%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.325 | Acc: 90.038%\n",
      "[=================================================>] 157/157 | Loss: 1.480 | Acc: 67.680%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.312 | Acc: 90.276%\n",
      "[=================================================>] 157/157 | Loss: 1.554 | Acc: 66.600%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.304 | Acc: 90.504%\n",
      "[=================================================>] 157/157 | Loss: 1.524 | Acc: 66.810%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.290 | Acc: 90.864%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 67.160%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.277 | Acc: 91.472%\n",
      "[=================================================>] 157/157 | Loss: 1.539 | Acc: 67.220%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.272 | Acc: 91.444%\n",
      "[=================================================>] 157/157 | Loss: 1.572 | Acc: 66.880%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.264 | Acc: 91.644%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 67.530%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.257 | Acc: 91.836%\n",
      "[=================================================>] 157/157 | Loss: 1.521 | Acc: 67.840%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.243 | Acc: 92.550%\n",
      "[=================================================>] 157/157 | Loss: 1.593 | Acc: 67.370%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.236 | Acc: 92.734%\n",
      "[=================================================>] 157/157 | Loss: 1.553 | Acc: 67.780%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.218 | Acc: 93.160%\n",
      "[=================================================>] 157/157 | Loss: 1.575 | Acc: 67.580%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.209 | Acc: 93.496%\n",
      "[=================================================>] 157/157 | Loss: 1.568 | Acc: 68.400%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.208 | Acc: 93.564%\n",
      "[=================================================>] 157/157 | Loss: 1.546 | Acc: 68.210%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.186 | Acc: 93.974%\n",
      "[=================================================>] 157/157 | Loss: 1.590 | Acc: 68.200%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.189 | Acc: 94.080%\n",
      "[=================================================>] 157/157 | Loss: 1.564 | Acc: 68.610%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.175 | Acc: 94.536%\n",
      "[=================================================>] 157/157 | Loss: 1.560 | Acc: 68.910%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.167 | Acc: 94.662%\n",
      "[=================================================>] 157/157 | Loss: 1.567 | Acc: 68.830%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.154 | Acc: 95.290%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 68.770%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.149 | Acc: 95.330%\n",
      "[=================================================>] 157/157 | Loss: 1.622 | Acc: 68.900%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.142 | Acc: 95.496%\n",
      "[=================================================>] 157/157 | Loss: 1.614 | Acc: 68.980%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.131 | Acc: 95.810%\n",
      "[=================================================>] 157/157 | Loss: 1.578 | Acc: 69.070%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.128 | Acc: 95.910%\n",
      "[=================================================>] 157/157 | Loss: 1.592 | Acc: 69.010%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.117 | Acc: 96.352%\n",
      "[=================================================>] 157/157 | Loss: 1.593 | Acc: 69.520%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.109 | Acc: 96.522%\n",
      "[=================================================>] 157/157 | Loss: 1.600 | Acc: 69.610%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.107 | Acc: 96.616%\n",
      "[=================================================>] 157/157 | Loss: 1.619 | Acc: 69.360%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.092 | Acc: 97.164%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 69.390%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.088 | Acc: 97.270%\n",
      "[=================================================>] 157/157 | Loss: 1.595 | Acc: 70.130%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.082 | Acc: 97.474%\n",
      "[=================================================>] 157/157 | Loss: 1.617 | Acc: 69.910%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.072 | Acc: 97.796%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 70.140%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.070 | Acc: 97.858%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 70.540%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.071 | Acc: 97.806%\n",
      "[=================================================>] 157/157 | Loss: 1.617 | Acc: 71.030%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.061 | Acc: 98.114%\n",
      "[=================================================>] 157/157 | Loss: 1.632 | Acc: 70.830%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.053 | Acc: 98.322%\n",
      "[=================================================>] 157/157 | Loss: 1.634 | Acc: 70.890%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.050 | Acc: 98.440%\n",
      "[=================================================>] 157/157 | Loss: 1.666 | Acc: 70.590%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.048 | Acc: 98.490%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 71.050%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.046 | Acc: 98.620%\n",
      "[=================================================>] 157/157 | Loss: 1.644 | Acc: 71.110%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.039 | Acc: 98.876%\n",
      "[=================================================>] 157/157 | Loss: 1.658 | Acc: 70.920%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.039 | Acc: 98.798%\n",
      "[=================================================>] 157/157 | Loss: 1.669 | Acc: 70.910%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.037 | Acc: 98.844%\n",
      "[=================================================>] 157/157 | Loss: 1.676 | Acc: 70.930%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.033 | Acc: 99.008%\n",
      "[=================================================>] 157/157 | Loss: 1.658 | Acc: 71.120%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.032 | Acc: 99.056%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 71.150%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.027 | Acc: 99.190%\n",
      "[=================================================>] 157/157 | Loss: 1.642 | Acc: 71.360%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.026 | Acc: 99.238%\n",
      "[=================================================>] 157/157 | Loss: 1.658 | Acc: 71.330%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.026 | Acc: 99.222%\n",
      "[=================================================>] 157/157 | Loss: 1.639 | Acc: 71.720%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.023 | Acc: 99.366%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 71.570%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.021 | Acc: 99.434%\n",
      "[=================================================>] 157/157 | Loss: 1.658 | Acc: 71.560%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.022 | Acc: 99.376%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 71.530%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.019 | Acc: 99.524%\n",
      "[=================================================>] 157/157 | Loss: 1.653 | Acc: 71.640%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.486%\n",
      "[=================================================>] 157/157 | Loss: 1.661 | Acc: 71.650%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.468%\n",
      "[=================================================>] 157/157 | Loss: 1.644 | Acc: 71.790%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.017 | Acc: 99.516%\n",
      "[=================================================>] 157/157 | Loss: 1.652 | Acc: 71.570%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.538%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 71.880%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.586%\n",
      "[=================================================>] 157/157 | Loss: 1.644 | Acc: 72.100%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.015 | Acc: 99.608%\n",
      "[=================================================>] 157/157 | Loss: 1.646 | Acc: 71.950%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.574%\n",
      "[=================================================>] 157/157 | Loss: 1.650 | Acc: 71.830%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.576%\n",
      "[=================================================>] 157/157 | Loss: 1.661 | Acc: 71.900%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.014 | Acc: 99.648%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 71.790%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.552%\n",
      "[=================================================>] 157/157 | Loss: 1.646 | Acc: 71.950%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.015 | Acc: 99.584%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 71.770%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.682%\n",
      "[=================================================>] 157/157 | Loss: 1.655 | Acc: 71.670%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.014 | Acc: 99.646%\n",
      "[=================================================>] 157/157 | Loss: 1.652 | Acc: 72.010%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.632%\n",
      "[=================================================>] 157/157 | Loss: 1.638 | Acc: 71.970%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.014 | Acc: 99.656%\n",
      "[=================================================>] 157/157 | Loss: 1.653 | Acc: 71.940%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.014 | Acc: 99.656%\n",
      "[=================================================>] 157/157 | Loss: 1.650 | Acc: 72.030%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.660%\n",
      "[=================================================>] 157/157 | Loss: 1.649 | Acc: 71.750%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.014 | Acc: 99.614%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 72.030%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 24.42M (28.2% reduction)\n",
      "MACs: 0.33G => 0.16G (51.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 3\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_min_30_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2412ac",
   "metadata": {},
   "source": [
    "##### ~72% filters pruned (512*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53107b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.790%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 43, 3: 27, 7: 61, 10: 55, 14: 165, 17: 161, 20: 159, 24: 393, 27: 386, 30: 390, 34: 397, 37: 397, 40: 438}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(21, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(37, 67, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(67, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(73, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(91, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(95, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(97, 119, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(119, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(126, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(122, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(115, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(115, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 4.036 | Acc: 6.306%\n",
      "[=================================================>] 157/157 | Loss: 3.605 | Acc: 10.950%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 3.228 | Acc: 16.604%\n",
      "[=================================================>] 157/157 | Loss: 3.212 | Acc: 18.540%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 2.835 | Acc: 24.672%\n",
      "[=================================================>] 157/157 | Loss: 2.665 | Acc: 28.410%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 2.573 | Acc: 30.774%\n",
      "[=================================================>] 157/157 | Loss: 2.578 | Acc: 32.570%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 2.398 | Acc: 34.960%\n",
      "[=================================================>] 157/157 | Loss: 2.331 | Acc: 36.720%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 2.269 | Acc: 38.256%\n",
      "[=================================================>] 157/157 | Loss: 2.231 | Acc: 39.940%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 2.156 | Acc: 41.136%\n",
      "[=================================================>] 157/157 | Loss: 2.277 | Acc: 39.880%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 2.074 | Acc: 42.916%\n",
      "[=================================================>] 157/157 | Loss: 2.130 | Acc: 42.790%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 1.998 | Acc: 44.782%\n",
      "[=================================================>] 157/157 | Loss: 1.955 | Acc: 46.120%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 1.932 | Acc: 46.748%\n",
      "[=================================================>] 157/157 | Loss: 2.087 | Acc: 44.370%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 1.877 | Acc: 47.962%\n",
      "[=================================================>] 157/157 | Loss: 2.073 | Acc: 44.810%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 1.840 | Acc: 49.064%\n",
      "[=================================================>] 157/157 | Loss: 1.888 | Acc: 48.370%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 1.793 | Acc: 50.116%\n",
      "[=================================================>] 157/157 | Loss: 1.869 | Acc: 48.970%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 1.743 | Acc: 51.646%\n",
      "[=================================================>] 157/157 | Loss: 1.848 | Acc: 49.690%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 1.721 | Acc: 52.130%\n",
      "[=================================================>] 157/157 | Loss: 1.895 | Acc: 48.960%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 1.680 | Acc: 53.070%\n",
      "[=================================================>] 157/157 | Loss: 1.798 | Acc: 51.160%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 1.651 | Acc: 53.800%\n",
      "[=================================================>] 157/157 | Loss: 1.808 | Acc: 52.360%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 1.620 | Acc: 54.852%\n",
      "[=================================================>] 157/157 | Loss: 1.750 | Acc: 52.090%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 1.600 | Acc: 55.264%\n",
      "[=================================================>] 157/157 | Loss: 1.721 | Acc: 52.710%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 1.577 | Acc: 56.104%\n",
      "[=================================================>] 157/157 | Loss: 1.666 | Acc: 54.330%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 1.553 | Acc: 56.666%\n",
      "[=================================================>] 157/157 | Loss: 1.668 | Acc: 54.520%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 1.526 | Acc: 57.072%\n",
      "[=================================================>] 157/157 | Loss: 1.761 | Acc: 53.100%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 1.519 | Acc: 57.514%\n",
      "[=================================================>] 157/157 | Loss: 1.709 | Acc: 53.960%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 1.499 | Acc: 57.958%\n",
      "[=================================================>] 157/157 | Loss: 1.758 | Acc: 52.840%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 1.479 | Acc: 58.560%\n",
      "[=================================================>] 157/157 | Loss: 1.706 | Acc: 54.360%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 1.463 | Acc: 58.862%\n",
      "[=================================================>] 157/157 | Loss: 1.703 | Acc: 54.670%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 1.443 | Acc: 59.468%\n",
      "[=================================================>] 157/157 | Loss: 1.771 | Acc: 53.250%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 1.438 | Acc: 59.668%\n",
      "[=================================================>] 157/157 | Loss: 1.630 | Acc: 56.040%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 1.418 | Acc: 60.082%\n",
      "[=================================================>] 157/157 | Loss: 1.652 | Acc: 55.610%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 1.397 | Acc: 60.508%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 55.620%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 1.375 | Acc: 61.156%\n",
      "[=================================================>] 157/157 | Loss: 1.625 | Acc: 56.280%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 1.383 | Acc: 61.104%\n",
      "[=================================================>] 157/157 | Loss: 1.672 | Acc: 55.590%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 1.369 | Acc: 61.360%\n",
      "[=================================================>] 157/157 | Loss: 1.613 | Acc: 56.520%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 1.351 | Acc: 61.738%\n",
      "[=================================================>] 157/157 | Loss: 1.673 | Acc: 55.350%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 1.348 | Acc: 61.956%\n",
      "[=================================================>] 157/157 | Loss: 1.589 | Acc: 57.340%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 1.320 | Acc: 62.526%\n",
      "[=================================================>] 157/157 | Loss: 1.647 | Acc: 56.180%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 1.315 | Acc: 62.690%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 55.860%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 1.312 | Acc: 62.918%\n",
      "[=================================================>] 157/157 | Loss: 1.516 | Acc: 58.780%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 1.290 | Acc: 63.450%\n",
      "[=================================================>] 157/157 | Loss: 1.583 | Acc: 56.840%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 1.285 | Acc: 63.750%\n",
      "[=================================================>] 157/157 | Loss: 1.530 | Acc: 58.280%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 1.271 | Acc: 63.910%\n",
      "[=================================================>] 157/157 | Loss: 1.688 | Acc: 55.790%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 1.267 | Acc: 63.974%\n",
      "[=================================================>] 157/157 | Loss: 1.525 | Acc: 58.760%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 1.255 | Acc: 64.538%\n",
      "[=================================================>] 157/157 | Loss: 1.584 | Acc: 58.170%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 1.248 | Acc: 64.648%\n",
      "[=================================================>] 157/157 | Loss: 1.647 | Acc: 56.700%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 1.241 | Acc: 64.722%\n",
      "[=================================================>] 157/157 | Loss: 1.593 | Acc: 57.730%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 1.220 | Acc: 65.182%\n",
      "[=================================================>] 157/157 | Loss: 1.549 | Acc: 58.890%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 1.213 | Acc: 65.376%\n",
      "[=================================================>] 157/157 | Loss: 1.584 | Acc: 58.100%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 1.201 | Acc: 65.670%\n",
      "[=================================================>] 157/157 | Loss: 1.547 | Acc: 58.500%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 1.189 | Acc: 66.290%\n",
      "[=================================================>] 157/157 | Loss: 1.588 | Acc: 58.210%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 1.185 | Acc: 66.182%\n",
      "[=================================================>] 157/157 | Loss: 1.586 | Acc: 58.580%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 1.170 | Acc: 66.840%\n",
      "[=================================================>] 157/157 | Loss: 1.548 | Acc: 58.730%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 1.164 | Acc: 66.738%\n",
      "[=================================================>] 157/157 | Loss: 1.486 | Acc: 60.250%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 1.154 | Acc: 67.168%\n",
      "[=================================================>] 157/157 | Loss: 1.525 | Acc: 59.870%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 1.148 | Acc: 67.260%\n",
      "[=================================================>] 157/157 | Loss: 1.555 | Acc: 59.180%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 1.134 | Acc: 67.666%\n",
      "[=================================================>] 157/157 | Loss: 1.512 | Acc: 59.620%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 1.126 | Acc: 67.658%\n",
      "[=================================================>] 157/157 | Loss: 1.517 | Acc: 59.340%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 1.114 | Acc: 68.104%\n",
      "[=================================================>] 157/157 | Loss: 1.508 | Acc: 59.810%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 1.103 | Acc: 68.332%\n",
      "[=================================================>] 157/157 | Loss: 1.513 | Acc: 60.230%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 1.091 | Acc: 68.776%\n",
      "[=================================================>] 157/157 | Loss: 1.491 | Acc: 60.400%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 1.092 | Acc: 68.744%\n",
      "[=================================================>] 157/157 | Loss: 1.499 | Acc: 60.590%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 1.080 | Acc: 68.932%\n",
      "[=================================================>] 157/157 | Loss: 1.469 | Acc: 61.030%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 1.060 | Acc: 69.576%\n",
      "[=================================================>] 157/157 | Loss: 1.473 | Acc: 61.080%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 1.061 | Acc: 69.472%\n",
      "[=================================================>] 157/157 | Loss: 1.512 | Acc: 60.240%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 1.058 | Acc: 69.678%\n",
      "[=================================================>] 157/157 | Loss: 1.476 | Acc: 61.200%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 1.039 | Acc: 70.122%\n",
      "[=================================================>] 157/157 | Loss: 1.497 | Acc: 61.040%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 1.033 | Acc: 69.934%\n",
      "[=================================================>] 157/157 | Loss: 1.426 | Acc: 62.010%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 1.023 | Acc: 70.446%\n",
      "[=================================================>] 157/157 | Loss: 1.555 | Acc: 59.380%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 1.011 | Acc: 70.660%\n",
      "[=================================================>] 157/157 | Loss: 1.498 | Acc: 61.010%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 1.008 | Acc: 70.904%\n",
      "[=================================================>] 157/157 | Loss: 1.530 | Acc: 60.090%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 0.989 | Acc: 71.182%\n",
      "[=================================================>] 157/157 | Loss: 1.466 | Acc: 61.670%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.991 | Acc: 71.400%\n",
      "[=================================================>] 157/157 | Loss: 1.496 | Acc: 61.120%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.976 | Acc: 71.584%\n",
      "[=================================================>] 157/157 | Loss: 1.522 | Acc: 60.920%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.970 | Acc: 71.894%\n",
      "[=================================================>] 157/157 | Loss: 1.458 | Acc: 62.190%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.960 | Acc: 71.934%\n",
      "[=================================================>] 157/157 | Loss: 1.488 | Acc: 61.520%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.961 | Acc: 72.178%\n",
      "[=================================================>] 157/157 | Loss: 1.484 | Acc: 61.620%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.942 | Acc: 72.414%\n",
      "[=================================================>] 157/157 | Loss: 1.485 | Acc: 61.330%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.921 | Acc: 73.038%\n",
      "[=================================================>] 157/157 | Loss: 1.456 | Acc: 61.710%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.916 | Acc: 73.282%\n",
      "[=================================================>] 157/157 | Loss: 1.433 | Acc: 63.280%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.905 | Acc: 73.660%\n",
      "[=================================================>] 157/157 | Loss: 1.457 | Acc: 62.490%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.894 | Acc: 73.974%\n",
      "[=================================================>] 157/157 | Loss: 1.429 | Acc: 62.530%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.882 | Acc: 74.044%\n",
      "[=================================================>] 157/157 | Loss: 1.445 | Acc: 62.590%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.874 | Acc: 74.286%\n",
      "[=================================================>] 157/157 | Loss: 1.465 | Acc: 62.060%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.859 | Acc: 74.980%\n",
      "[=================================================>] 157/157 | Loss: 1.574 | Acc: 61.150%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.857 | Acc: 74.700%\n",
      "[=================================================>] 157/157 | Loss: 1.442 | Acc: 62.650%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.849 | Acc: 75.238%\n",
      "[=================================================>] 157/157 | Loss: 1.452 | Acc: 62.240%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.832 | Acc: 75.600%\n",
      "[=================================================>] 157/157 | Loss: 1.492 | Acc: 62.140%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.822 | Acc: 75.806%\n",
      "[=================================================>] 157/157 | Loss: 1.394 | Acc: 63.410%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.809 | Acc: 76.126%\n",
      "[=================================================>] 157/157 | Loss: 1.427 | Acc: 63.320%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.800 | Acc: 76.316%\n",
      "[=================================================>] 157/157 | Loss: 1.450 | Acc: 62.740%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.790 | Acc: 76.626%\n",
      "[=================================================>] 157/157 | Loss: 1.447 | Acc: 62.880%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.770 | Acc: 77.158%\n",
      "[=================================================>] 157/157 | Loss: 1.432 | Acc: 63.070%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.770 | Acc: 77.058%\n",
      "[=================================================>] 157/157 | Loss: 1.423 | Acc: 63.730%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.745 | Acc: 77.642%\n",
      "[=================================================>] 157/157 | Loss: 1.448 | Acc: 63.690%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.746 | Acc: 77.894%\n",
      "[=================================================>] 157/157 | Loss: 1.436 | Acc: 63.380%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.736 | Acc: 77.966%\n",
      "[=================================================>] 157/157 | Loss: 1.464 | Acc: 63.230%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.709 | Acc: 78.776%\n",
      "[=================================================>] 157/157 | Loss: 1.433 | Acc: 64.790%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.706 | Acc: 78.952%\n",
      "[=================================================>] 157/157 | Loss: 1.419 | Acc: 64.080%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.703 | Acc: 79.044%\n",
      "[=================================================>] 157/157 | Loss: 1.440 | Acc: 64.180%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.682 | Acc: 79.562%\n",
      "[=================================================>] 157/157 | Loss: 1.441 | Acc: 64.490%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.674 | Acc: 79.712%\n",
      "[=================================================>] 157/157 | Loss: 1.441 | Acc: 63.960%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.664 | Acc: 80.046%\n",
      "[=================================================>] 157/157 | Loss: 1.458 | Acc: 63.610%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.655 | Acc: 80.140%\n",
      "[=================================================>] 157/157 | Loss: 1.423 | Acc: 64.550%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.641 | Acc: 80.550%\n",
      "[=================================================>] 157/157 | Loss: 1.425 | Acc: 64.480%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.631 | Acc: 80.824%\n",
      "[=================================================>] 157/157 | Loss: 1.457 | Acc: 64.240%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.613 | Acc: 81.436%\n",
      "[=================================================>] 157/157 | Loss: 1.452 | Acc: 63.790%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.601 | Acc: 81.796%\n",
      "[=================================================>] 157/157 | Loss: 1.409 | Acc: 64.630%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.580 | Acc: 82.456%\n",
      "[=================================================>] 157/157 | Loss: 1.441 | Acc: 64.660%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.577 | Acc: 82.518%\n",
      "[=================================================>] 157/157 | Loss: 1.477 | Acc: 65.110%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.568 | Acc: 82.652%\n",
      "[=================================================>] 157/157 | Loss: 1.431 | Acc: 65.540%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.555 | Acc: 82.938%\n",
      "[=================================================>] 157/157 | Loss: 1.446 | Acc: 65.430%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.545 | Acc: 83.318%\n",
      "[=================================================>] 157/157 | Loss: 1.421 | Acc: 65.470%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.522 | Acc: 83.968%\n",
      "[=================================================>] 157/157 | Loss: 1.486 | Acc: 64.960%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.522 | Acc: 83.960%\n",
      "[=================================================>] 157/157 | Loss: 1.445 | Acc: 65.580%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.504 | Acc: 84.570%\n",
      "[=================================================>] 157/157 | Loss: 1.472 | Acc: 65.090%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.492 | Acc: 84.902%\n",
      "[=================================================>] 157/157 | Loss: 1.479 | Acc: 65.280%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.483 | Acc: 85.034%\n",
      "[=================================================>] 157/157 | Loss: 1.460 | Acc: 65.200%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.468 | Acc: 85.438%\n",
      "[=================================================>] 157/157 | Loss: 1.464 | Acc: 65.770%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.458 | Acc: 85.766%\n",
      "[=================================================>] 157/157 | Loss: 1.440 | Acc: 65.800%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.447 | Acc: 86.206%\n",
      "[=================================================>] 157/157 | Loss: 1.460 | Acc: 65.890%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.428 | Acc: 86.646%\n",
      "[=================================================>] 157/157 | Loss: 1.484 | Acc: 65.710%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.422 | Acc: 86.874%\n",
      "[=================================================>] 157/157 | Loss: 1.456 | Acc: 65.710%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.409 | Acc: 87.140%\n",
      "[=================================================>] 157/157 | Loss: 1.460 | Acc: 66.620%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.398 | Acc: 87.656%\n",
      "[=================================================>] 157/157 | Loss: 1.489 | Acc: 65.990%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.389 | Acc: 87.910%\n",
      "[=================================================>] 157/157 | Loss: 1.480 | Acc: 66.230%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.377 | Acc: 88.138%\n",
      "[=================================================>] 157/157 | Loss: 1.476 | Acc: 65.850%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.363 | Acc: 88.532%\n",
      "[=================================================>] 157/157 | Loss: 1.505 | Acc: 66.250%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.354 | Acc: 88.762%\n",
      "[=================================================>] 157/157 | Loss: 1.489 | Acc: 66.570%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.349 | Acc: 89.052%\n",
      "[=================================================>] 157/157 | Loss: 1.498 | Acc: 66.570%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.344 | Acc: 89.194%\n",
      "[=================================================>] 157/157 | Loss: 1.508 | Acc: 66.460%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.329 | Acc: 89.610%\n",
      "[=================================================>] 157/157 | Loss: 1.517 | Acc: 66.410%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.323 | Acc: 89.720%\n",
      "[=================================================>] 157/157 | Loss: 1.513 | Acc: 66.530%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.311 | Acc: 90.184%\n",
      "[=================================================>] 157/157 | Loss: 1.524 | Acc: 66.390%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.309 | Acc: 90.332%\n",
      "[=================================================>] 157/157 | Loss: 1.523 | Acc: 66.600%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.301 | Acc: 90.482%\n",
      "[=================================================>] 157/157 | Loss: 1.539 | Acc: 66.580%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.296 | Acc: 90.640%\n",
      "[=================================================>] 157/157 | Loss: 1.547 | Acc: 66.620%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.286 | Acc: 90.980%\n",
      "[=================================================>] 157/157 | Loss: 1.536 | Acc: 66.620%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.283 | Acc: 91.008%\n",
      "[=================================================>] 157/157 | Loss: 1.547 | Acc: 66.640%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.272 | Acc: 91.506%\n",
      "[=================================================>] 157/157 | Loss: 1.548 | Acc: 66.390%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.276 | Acc: 91.240%\n",
      "[=================================================>] 157/157 | Loss: 1.560 | Acc: 66.840%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.266 | Acc: 91.556%\n",
      "[=================================================>] 157/157 | Loss: 1.567 | Acc: 66.640%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.263 | Acc: 91.576%\n",
      "[=================================================>] 157/157 | Loss: 1.558 | Acc: 66.550%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.262 | Acc: 91.752%\n",
      "[=================================================>] 157/157 | Loss: 1.558 | Acc: 66.470%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.267 | Acc: 91.570%\n",
      "[=================================================>] 157/157 | Loss: 1.554 | Acc: 66.680%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.257 | Acc: 92.038%\n",
      "[=================================================>] 157/157 | Loss: 1.551 | Acc: 66.590%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.258 | Acc: 91.826%\n",
      "[=================================================>] 157/157 | Loss: 1.553 | Acc: 66.810%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.253 | Acc: 92.006%\n",
      "[=================================================>] 157/157 | Loss: 1.566 | Acc: 66.680%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.253 | Acc: 92.014%\n",
      "[=================================================>] 157/157 | Loss: 1.573 | Acc: 66.750%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.248 | Acc: 92.114%\n",
      "[=================================================>] 157/157 | Loss: 1.564 | Acc: 66.540%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.246 | Acc: 92.266%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 66.590%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.252 | Acc: 92.034%\n",
      "[=================================================>] 157/157 | Loss: 1.563 | Acc: 66.780%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 18.50M (45.6% reduction)\n",
      "MACs: 0.33G => 0.06G (80.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'min'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 6\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_min_70_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d444a",
   "metadata": {},
   "source": [
    "### Global pruning (Archor) - avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f196e",
   "metadata": {},
   "source": [
    "#### CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559b3f4",
   "metadata": {},
   "source": [
    "##### ~50% filters pruned (512*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cdad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 21, 3: 16, 7: 41, 10: 34, 14: 103, 17: 85, 20: 100, 24: 239, 27: 225, 30: 243, 34: 234, 37: 256, 40: 451}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(43, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(48, 87, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(87, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(94, 153, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(153, 171, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(171, 156, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(156, 273, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(273, 287, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(287, 269, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(269, 278, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(278, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 391/391 | Loss: 0.631 | Acc: 81.136%\n",
      "[=================================================>] 79/79 | Loss: 0.581 | Acc: 80.920%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 391/391 | Loss: 0.412 | Acc: 86.440%\n",
      "[=================================================>] 79/79 | Loss: 0.526 | Acc: 82.830%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 391/391 | Loss: 0.347 | Acc: 88.576%\n",
      "[=================================================>] 79/79 | Loss: 0.498 | Acc: 83.880%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 391/391 | Loss: 0.304 | Acc: 90.062%\n",
      "[=================================================>] 79/79 | Loss: 0.434 | Acc: 85.850%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 391/391 | Loss: 0.281 | Acc: 90.492%\n",
      "[=================================================>] 79/79 | Loss: 0.443 | Acc: 86.210%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 391/391 | Loss: 0.254 | Acc: 91.642%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 88.510%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 391/391 | Loss: 0.240 | Acc: 92.020%\n",
      "[=================================================>] 79/79 | Loss: 0.380 | Acc: 87.590%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 391/391 | Loss: 0.222 | Acc: 92.606%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 87.770%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 391/391 | Loss: 0.211 | Acc: 92.982%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 87.500%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 391/391 | Loss: 0.192 | Acc: 93.592%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 87.660%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 391/391 | Loss: 0.187 | Acc: 93.852%\n",
      "[=================================================>] 79/79 | Loss: 0.436 | Acc: 87.070%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 391/391 | Loss: 0.180 | Acc: 94.014%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 88.800%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 391/391 | Loss: 0.169 | Acc: 94.318%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 88.400%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 391/391 | Loss: 0.166 | Acc: 94.498%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 88.530%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 391/391 | Loss: 0.161 | Acc: 94.618%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 89.000%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 391/391 | Loss: 0.156 | Acc: 94.818%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 88.880%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 391/391 | Loss: 0.150 | Acc: 94.930%\n",
      "[=================================================>] 79/79 | Loss: 0.409 | Acc: 87.550%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 391/391 | Loss: 0.147 | Acc: 95.152%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 89.480%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 391/391 | Loss: 0.141 | Acc: 95.200%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 89.190%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 391/391 | Loss: 0.134 | Acc: 95.372%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 88.760%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 391/391 | Loss: 0.130 | Acc: 95.724%\n",
      "[=================================================>] 79/79 | Loss: 0.407 | Acc: 88.100%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 391/391 | Loss: 0.127 | Acc: 95.812%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 89.670%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 391/391 | Loss: 0.127 | Acc: 95.830%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 88.490%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 391/391 | Loss: 0.119 | Acc: 95.944%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 89.980%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 391/391 | Loss: 0.121 | Acc: 95.940%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 89.070%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 391/391 | Loss: 0.121 | Acc: 96.000%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 89.450%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 391/391 | Loss: 0.117 | Acc: 96.046%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 89.960%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 391/391 | Loss: 0.114 | Acc: 96.258%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 88.750%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 391/391 | Loss: 0.110 | Acc: 96.336%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 88.960%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 391/391 | Loss: 0.109 | Acc: 96.336%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 89.790%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 391/391 | Loss: 0.102 | Acc: 96.540%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 90.370%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 391/391 | Loss: 0.100 | Acc: 96.658%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 89.050%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 391/391 | Loss: 0.101 | Acc: 96.644%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 89.690%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 391/391 | Loss: 0.097 | Acc: 96.758%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 89.960%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 391/391 | Loss: 0.093 | Acc: 96.864%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 89.220%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 391/391 | Loss: 0.093 | Acc: 96.916%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.070%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 391/391 | Loss: 0.092 | Acc: 96.860%\n",
      "[=================================================>] 79/79 | Loss: 0.380 | Acc: 89.360%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 391/391 | Loss: 0.095 | Acc: 96.880%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 89.680%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 391/391 | Loss: 0.087 | Acc: 97.024%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 89.370%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 391/391 | Loss: 0.087 | Acc: 97.056%\n",
      "[=================================================>] 79/79 | Loss: 0.337 | Acc: 90.310%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 391/391 | Loss: 0.084 | Acc: 97.162%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 89.210%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.196%\n",
      "[=================================================>] 79/79 | Loss: 0.336 | Acc: 90.290%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 391/391 | Loss: 0.084 | Acc: 97.194%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 89.140%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.330%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 89.600%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 391/391 | Loss: 0.080 | Acc: 97.322%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 90.200%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 391/391 | Loss: 0.078 | Acc: 97.392%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 89.860%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 391/391 | Loss: 0.080 | Acc: 97.364%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 90.330%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 391/391 | Loss: 0.078 | Acc: 97.426%\n",
      "[=================================================>] 79/79 | Loss: 0.320 | Acc: 91.120%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 391/391 | Loss: 0.073 | Acc: 97.524%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 89.120%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.644%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 89.980%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.590%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 89.810%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.818%\n",
      "[=================================================>] 79/79 | Loss: 0.335 | Acc: 90.980%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 391/391 | Loss: 0.064 | Acc: 97.836%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 89.660%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.774%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 89.730%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 391/391 | Loss: 0.065 | Acc: 97.854%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 89.890%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 391/391 | Loss: 0.068 | Acc: 97.748%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 89.810%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 391/391 | Loss: 0.062 | Acc: 97.938%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 89.750%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 391/391 | Loss: 0.060 | Acc: 97.948%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 89.860%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 391/391 | Loss: 0.060 | Acc: 98.020%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 90.460%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.226%\n",
      "[=================================================>] 79/79 | Loss: 0.367 | Acc: 90.320%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 391/391 | Loss: 0.054 | Acc: 98.204%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 90.400%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.120%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 90.600%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.292%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 90.770%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 391/391 | Loss: 0.052 | Acc: 98.310%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 90.260%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.304%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 90.670%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 391/391 | Loss: 0.050 | Acc: 98.354%\n",
      "[=================================================>] 79/79 | Loss: 0.328 | Acc: 91.320%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 391/391 | Loss: 0.052 | Acc: 98.232%\n",
      "[=================================================>] 79/79 | Loss: 0.450 | Acc: 88.970%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 391/391 | Loss: 0.052 | Acc: 98.238%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 90.820%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 391/391 | Loss: 0.044 | Acc: 98.578%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 91.340%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 391/391 | Loss: 0.045 | Acc: 98.546%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 90.680%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.530%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 90.390%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 391/391 | Loss: 0.044 | Acc: 98.502%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.280%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 391/391 | Loss: 0.044 | Acc: 98.564%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 90.760%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.568%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.980%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.594%\n",
      "[=================================================>] 79/79 | Loss: 0.333 | Acc: 91.830%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 391/391 | Loss: 0.035 | Acc: 98.832%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 91.120%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 391/391 | Loss: 0.036 | Acc: 98.776%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 90.970%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 391/391 | Loss: 0.036 | Acc: 98.844%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 91.100%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.874%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 91.500%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 391/391 | Loss: 0.030 | Acc: 99.018%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 91.470%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 391/391 | Loss: 0.035 | Acc: 98.894%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 90.960%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 391/391 | Loss: 0.030 | Acc: 99.042%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 91.490%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 391/391 | Loss: 0.031 | Acc: 98.978%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 91.280%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.088%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 91.130%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 391/391 | Loss: 0.029 | Acc: 99.036%\n",
      "[=================================================>] 79/79 | Loss: 0.359 | Acc: 91.500%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.034%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 90.740%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 391/391 | Loss: 0.026 | Acc: 99.192%\n",
      "[=================================================>] 79/79 | Loss: 0.366 | Acc: 91.350%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.258%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 91.430%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 391/391 | Loss: 0.025 | Acc: 99.160%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 91.890%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.308%\n",
      "[=================================================>] 79/79 | Loss: 0.327 | Acc: 92.130%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 391/391 | Loss: 0.019 | Acc: 99.382%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 91.860%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.298%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 91.910%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.382%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 92.150%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 391/391 | Loss: 0.020 | Acc: 99.354%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 91.870%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.420%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 91.550%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.562%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 92.170%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.574%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.020%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.476%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 92.230%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.700%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.280%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.602%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 92.060%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.646%\n",
      "[=================================================>] 79/79 | Loss: 0.334 | Acc: 92.690%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.632%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 92.520%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.674%\n",
      "[=================================================>] 79/79 | Loss: 0.354 | Acc: 92.400%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.668%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 92.470%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.724%\n",
      "[=================================================>] 79/79 | Loss: 0.336 | Acc: 92.480%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.794%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.680%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.862%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 92.660%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.840%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 92.800%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.820%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 92.600%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.856%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 92.860%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.852%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 92.800%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.852%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 92.780%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.856%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.850%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.916%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 92.820%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.924%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.000%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.912%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 92.890%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.920%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.910%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.944%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 93.080%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.922%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 92.910%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.912%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.010%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 92.950%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.830%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.946%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 92.910%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 93.030%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.130%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.090%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.080%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.020%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.040%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.970%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.110%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.100%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.250%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.120%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.140%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.030%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.180%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.100%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.100%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.110%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.100%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.984%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.230%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.970%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.200%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.990%%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.190%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.970%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.190%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.200%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.968%\n",
      "[=================================================>] 79/79 | Loss: 0.337 | Acc: 93.210%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.984%\n",
      "[=================================================>] 79/79 | Loss: 0.337 | Acc: 93.190%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.170%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.170%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.080%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 33.65M => 21.06M (37.4% reduction)\n",
      "MACs: 0.33G => 0.14G (58.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'avg'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 4\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_avg_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302aaf59",
   "metadata": {},
   "source": [
    "##### ~36% filters pruned (512*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf4f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 19, 3: 12, 7: 31, 10: 30, 14: 84, 17: 73, 20: 71, 24: 175, 27: 174, 30: 164, 34: 187, 37: 166, 40: 350}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(45, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(52, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(97, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(98, 172, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(172, 183, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(183, 185, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(185, 337, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(337, 338, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(338, 348, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(348, 325, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(325, 346, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(346, 162, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 391/391 | Loss: 0.529 | Acc: 83.436%\n",
      "[=================================================>] 79/79 | Loss: 0.587 | Acc: 82.030%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 391/391 | Loss: 0.360 | Acc: 88.208%\n",
      "[=================================================>] 79/79 | Loss: 0.432 | Acc: 85.920%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 391/391 | Loss: 0.307 | Acc: 89.882%\n",
      "[=================================================>] 79/79 | Loss: 0.534 | Acc: 83.410%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 391/391 | Loss: 0.265 | Acc: 91.150%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 87.200%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 391/391 | Loss: 0.249 | Acc: 91.706%\n",
      "[=================================================>] 79/79 | Loss: 0.440 | Acc: 86.700%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 391/391 | Loss: 0.225 | Acc: 92.572%\n",
      "[=================================================>] 79/79 | Loss: 0.425 | Acc: 86.590%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 391/391 | Loss: 0.204 | Acc: 93.190%\n",
      "[=================================================>] 79/79 | Loss: 0.443 | Acc: 86.710%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 391/391 | Loss: 0.195 | Acc: 93.486%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 88.920%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 391/391 | Loss: 0.180 | Acc: 93.966%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 88.600%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 391/391 | Loss: 0.175 | Acc: 94.190%\n",
      "[=================================================>] 79/79 | Loss: 0.380 | Acc: 88.040%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 391/391 | Loss: 0.162 | Acc: 94.600%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 88.250%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 391/391 | Loss: 0.157 | Acc: 94.742%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 88.470%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 391/391 | Loss: 0.153 | Acc: 94.900%\n",
      "[=================================================>] 79/79 | Loss: 0.391 | Acc: 88.430%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 391/391 | Loss: 0.144 | Acc: 95.252%\n",
      "[=================================================>] 79/79 | Loss: 0.430 | Acc: 87.240%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 391/391 | Loss: 0.134 | Acc: 95.464%\n",
      "[=================================================>] 79/79 | Loss: 0.459 | Acc: 86.970%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 391/391 | Loss: 0.131 | Acc: 95.704%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 88.810%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 391/391 | Loss: 0.128 | Acc: 95.748%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 87.730%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 391/391 | Loss: 0.124 | Acc: 95.836%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 89.760%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 391/391 | Loss: 0.121 | Acc: 95.948%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 89.640%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 391/391 | Loss: 0.120 | Acc: 95.974%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 89.450%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 391/391 | Loss: 0.114 | Acc: 96.144%\n",
      "[=================================================>] 79/79 | Loss: 0.376 | Acc: 88.840%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 391/391 | Loss: 0.113 | Acc: 96.208%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 89.800%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 391/391 | Loss: 0.110 | Acc: 96.310%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 89.140%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 391/391 | Loss: 0.106 | Acc: 96.460%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 88.970%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 391/391 | Loss: 0.101 | Acc: 96.628%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 89.680%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 391/391 | Loss: 0.102 | Acc: 96.592%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 89.300%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 391/391 | Loss: 0.096 | Acc: 96.778%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 90.010%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 391/391 | Loss: 0.101 | Acc: 96.566%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 89.090%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 391/391 | Loss: 0.095 | Acc: 96.814%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 89.400%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 391/391 | Loss: 0.092 | Acc: 96.978%\n",
      "[=================================================>] 79/79 | Loss: 0.368 | Acc: 89.740%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 391/391 | Loss: 0.090 | Acc: 97.008%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 89.730%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 97.026%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 89.830%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 391/391 | Loss: 0.089 | Acc: 96.970%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 90.050%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 391/391 | Loss: 0.084 | Acc: 97.196%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 89.980%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.326%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 89.200%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 391/391 | Loss: 0.085 | Acc: 97.168%\n",
      "[=================================================>] 79/79 | Loss: 0.383 | Acc: 89.590%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 391/391 | Loss: 0.080 | Acc: 97.334%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 90.290%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 391/391 | Loss: 0.084 | Acc: 97.210%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 89.110%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 391/391 | Loss: 0.078 | Acc: 97.416%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 89.610%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 391/391 | Loss: 0.078 | Acc: 97.442%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 89.730%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 391/391 | Loss: 0.073 | Acc: 97.518%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 89.810%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 391/391 | Loss: 0.077 | Acc: 97.412%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 90.730%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.688%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 90.310%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 391/391 | Loss: 0.069 | Acc: 97.682%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.400%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 391/391 | Loss: 0.068 | Acc: 97.786%\n",
      "[=================================================>] 79/79 | Loss: 0.378 | Acc: 90.440%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 391/391 | Loss: 0.072 | Acc: 97.618%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 90.630%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 391/391 | Loss: 0.060 | Acc: 98.060%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 90.480%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.818%\n",
      "[=================================================>] 79/79 | Loss: 0.335 | Acc: 91.140%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 391/391 | Loss: 0.064 | Acc: 97.856%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 90.160%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 391/391 | Loss: 0.066 | Acc: 97.808%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 90.460%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.904%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 91.090%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 391/391 | Loss: 0.062 | Acc: 97.868%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 89.930%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 391/391 | Loss: 0.058 | Acc: 98.086%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 90.430%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 391/391 | Loss: 0.059 | Acc: 98.014%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 90.400%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.960%\n",
      "[=================================================>] 79/79 | Loss: 0.332 | Acc: 91.130%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 391/391 | Loss: 0.053 | Acc: 98.192%\n",
      "[=================================================>] 79/79 | Loss: 0.333 | Acc: 91.150%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.134%\n",
      "[=================================================>] 79/79 | Loss: 0.374 | Acc: 90.340%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.122%\n",
      "[=================================================>] 79/79 | Loss: 0.329 | Acc: 91.000%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.334%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 90.700%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 391/391 | Loss: 0.048 | Acc: 98.390%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 91.080%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.486%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 90.130%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.406%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 90.770%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 391/391 | Loss: 0.048 | Acc: 98.436%\n",
      "[=================================================>] 79/79 | Loss: 0.440 | Acc: 89.520%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.510%\n",
      "[=================================================>] 79/79 | Loss: 0.355 | Acc: 90.910%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.368%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 91.130%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 391/391 | Loss: 0.044 | Acc: 98.542%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 90.090%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 391/391 | Loss: 0.045 | Acc: 98.516%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 91.260%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.712%\n",
      "[=================================================>] 79/79 | Loss: 0.370 | Acc: 90.810%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 391/391 | Loss: 0.040 | Acc: 98.700%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 90.660%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.608%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 91.460%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 391/391 | Loss: 0.037 | Acc: 98.756%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 91.090%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 391/391 | Loss: 0.041 | Acc: 98.642%\n",
      "[=================================================>] 79/79 | Loss: 0.348 | Acc: 91.390%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.912%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 91.070%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.626%\n",
      "[=================================================>] 79/79 | Loss: 0.324 | Acc: 91.390%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.890%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 90.580%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 391/391 | Loss: 0.030 | Acc: 98.988%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 91.730%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.160%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 91.330%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 391/391 | Loss: 0.031 | Acc: 98.990%\n",
      "[=================================================>] 79/79 | Loss: 0.371 | Acc: 91.140%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 391/391 | Loss: 0.026 | Acc: 99.184%\n",
      "[=================================================>] 79/79 | Loss: 0.365 | Acc: 91.560%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 391/391 | Loss: 0.033 | Acc: 98.926%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 91.600%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 391/391 | Loss: 0.027 | Acc: 99.158%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 91.690%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 391/391 | Loss: 0.026 | Acc: 99.196%\n",
      "[=================================================>] 79/79 | Loss: 0.352 | Acc: 91.520%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.190%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 91.630%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.240%\n",
      "[=================================================>] 79/79 | Loss: 0.369 | Acc: 91.610%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.200%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 91.670%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.264%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 91.370%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.322%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 91.380%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.416%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 91.820%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 391/391 | Loss: 0.022 | Acc: 99.302%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 91.940%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 391/391 | Loss: 0.017 | Acc: 99.420%\n",
      "[=================================================>] 79/79 | Loss: 0.364 | Acc: 91.900%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.424%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 92.000%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.520%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 92.160%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.524%\n",
      "[=================================================>] 79/79 | Loss: 0.330 | Acc: 92.270%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.532%\n",
      "[=================================================>] 79/79 | Loss: 0.350 | Acc: 92.160%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.554%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.370%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.616%\n",
      "[=================================================>] 79/79 | Loss: 0.358 | Acc: 92.150%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.640%\n",
      "[=================================================>] 79/79 | Loss: 0.360 | Acc: 92.260%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.610%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 92.360%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.684%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.530%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 391/391 | Loss: 0.012 | Acc: 99.630%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 91.910%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.732%\n",
      "[=================================================>] 79/79 | Loss: 0.353 | Acc: 92.450%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.702%\n",
      "[=================================================>] 79/79 | Loss: 0.333 | Acc: 92.610%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.756%\n",
      "[=================================================>] 79/79 | Loss: 0.332 | Acc: 92.620%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.754%\n",
      "[=================================================>] 79/79 | Loss: 0.357 | Acc: 92.490%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.804%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 92.790%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.850%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 92.570%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.842%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 92.780%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.876%\n",
      "[=================================================>] 79/79 | Loss: 0.356 | Acc: 92.730%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.814%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.880%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.856%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.810%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.888%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.740%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.904%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.980%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.922%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 92.670%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 0.351 | Acc: 92.890%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 0.342 | Acc: 93.070%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 93.100%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.970%\n",
      "[=================================================>] 79/79 | Loss: 0.349 | Acc: 92.990%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 92.930%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 0.343 | Acc: 92.900%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.970%\n",
      "[=================================================>] 79/79 | Loss: 0.347 | Acc: 92.690%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 92.940%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 93.010%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 93.170%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 93.000%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.346 | Acc: 92.990%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.974%\n",
      "[=================================================>] 79/79 | Loss: 0.345 | Acc: 93.040%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.344 | Acc: 93.180%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 92.990%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.240%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.978%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.250%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%%\n",
      "[=================================================>] 79/79 | Loss: 0.337 | Acc: 93.280%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.290%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.336 | Acc: 93.390%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.972%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.170%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.270%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.994%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.130%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.220%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.337 | Acc: 93.190%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.341 | Acc: 93.310%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.990%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.100%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.984%\n",
      "[=================================================>] 79/79 | Loss: 0.337 | Acc: 93.270%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.980%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.220%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.240%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.988%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.160%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.976%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.150%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%\n",
      "[=================================================>] 79/79 | Loss: 0.340 | Acc: 93.150%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.984%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.280%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.982%%\n",
      "[=================================================>] 79/79 | Loss: 0.338 | Acc: 93.160%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.986%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.170%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 391/391 | Loss: 0.001 | Acc: 99.990%\n",
      "[=================================================>] 79/79 | Loss: 0.339 | Acc: 93.160%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 33.65M => 23.57M (29.9% reduction)\n",
      "MACs: 0.33G => 0.17G (47.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'avg'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 3\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_avg_30_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41917101",
   "metadata": {},
   "source": [
    "##### ~72% filters pruned (512*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07c7c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 36, 3: 33, 7: 65, 10: 64, 14: 155, 17: 172, 20: 175, 24: 375, 27: 371, 30: 362, 34: 381, 37: 371, 40: 512}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(28, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(31, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(101, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(84, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(81, 137, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(137, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(141, 150, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(150, 131, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(131, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(141, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 141, 3, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\u001b[39;00m\n\u001b[1;32m     35\u001b[0m pruner \u001b[38;5;241m=\u001b[39m VGG16Pruner(config, model, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpruned_avg_70_ckpt.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mpruner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_filters_to_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_filters_to_prune\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_after_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_after_prune\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 893\u001b[0m, in \u001b[0;36mVGG16Pruner.prune\u001b[0;34m(self, num_filters_to_prune, epochs_after_prune)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_after_prune):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs_after_prune\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(epoch)\n\u001b[1;32m    895\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[3], line 708\u001b[0m, in \u001b[0;36mVGG16Pruner.train_epoch\u001b[0;34m(self, optimizer, rank_filters)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 708\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, targets)\n\u001b[1;32m    710\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 491\u001b[0m, in \u001b[0;36mModifiedVGG16Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 491\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    493\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 141, 3, 3] instead"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'avg'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 6\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_avg_70_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fad7ef",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1837e7c",
   "metadata": {},
   "source": [
    "##### ~50% filters pruned (512*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c46b44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.790%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 17, 3: 18, 7: 45, 10: 37, 14: 98, 17: 98, 20: 98, 24: 235, 27: 268, 30: 210, 34: 248, 37: 245, 40: 431}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(47, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(46, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(83, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(91, 158, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(158, 158, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(158, 158, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(158, 277, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(277, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(244, 302, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(302, 264, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(264, 267, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(267, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 2.076 | Acc: 47.546%\n",
      "[=================================================>] 157/157 | Loss: 2.026 | Acc: 47.480%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 1.695 | Acc: 54.534%\n",
      "[=================================================>] 157/157 | Loss: 2.089 | Acc: 48.410%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 1.544 | Acc: 58.092%\n",
      "[=================================================>] 157/157 | Loss: 1.856 | Acc: 51.850%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 1.447 | Acc: 60.436%\n",
      "[=================================================>] 157/157 | Loss: 1.675 | Acc: 56.050%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 1.369 | Acc: 62.302%\n",
      "[=================================================>] 157/157 | Loss: 1.736 | Acc: 55.520%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 1.309 | Acc: 63.862%\n",
      "[=================================================>] 157/157 | Loss: 1.625 | Acc: 57.420%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 1.254 | Acc: 65.436%\n",
      "[=================================================>] 157/157 | Loss: 1.702 | Acc: 56.250%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 1.203 | Acc: 66.536%\n",
      "[=================================================>] 157/157 | Loss: 1.625 | Acc: 57.680%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 1.163 | Acc: 67.378%\n",
      "[=================================================>] 157/157 | Loss: 1.584 | Acc: 59.200%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 1.134 | Acc: 68.108%\n",
      "[=================================================>] 157/157 | Loss: 1.571 | Acc: 59.170%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 1.095 | Acc: 69.114%\n",
      "[=================================================>] 157/157 | Loss: 1.472 | Acc: 61.310%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 1.066 | Acc: 69.876%\n",
      "[=================================================>] 157/157 | Loss: 1.563 | Acc: 59.520%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 1.033 | Acc: 70.878%\n",
      "[=================================================>] 157/157 | Loss: 1.526 | Acc: 61.050%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 1.013 | Acc: 71.212%\n",
      "[=================================================>] 157/157 | Loss: 1.541 | Acc: 60.660%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 0.986 | Acc: 72.062%\n",
      "[=================================================>] 157/157 | Loss: 1.533 | Acc: 60.720%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 0.971 | Acc: 72.250%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 61.200%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 0.945 | Acc: 72.980%\n",
      "[=================================================>] 157/157 | Loss: 1.530 | Acc: 61.100%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 0.923 | Acc: 73.518%\n",
      "[=================================================>] 157/157 | Loss: 1.505 | Acc: 62.040%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 0.906 | Acc: 73.942%\n",
      "[=================================================>] 157/157 | Loss: 1.599 | Acc: 59.820%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 0.889 | Acc: 74.386%\n",
      "[=================================================>] 157/157 | Loss: 1.527 | Acc: 61.410%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 0.875 | Acc: 74.778%\n",
      "[=================================================>] 157/157 | Loss: 1.515 | Acc: 61.590%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 0.857 | Acc: 75.360%\n",
      "[=================================================>] 157/157 | Loss: 1.553 | Acc: 61.810%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 0.841 | Acc: 75.982%\n",
      "[=================================================>] 157/157 | Loss: 1.487 | Acc: 62.430%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 0.826 | Acc: 76.144%\n",
      "[=================================================>] 157/157 | Loss: 1.615 | Acc: 60.720%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 0.818 | Acc: 76.286%\n",
      "[=================================================>] 157/157 | Loss: 1.448 | Acc: 63.680%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 0.793 | Acc: 76.966%\n",
      "[=================================================>] 157/157 | Loss: 1.459 | Acc: 62.750%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 0.783 | Acc: 77.180%\n",
      "[=================================================>] 157/157 | Loss: 1.588 | Acc: 62.030%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 0.775 | Acc: 77.520%\n",
      "[=================================================>] 157/157 | Loss: 1.599 | Acc: 61.190%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 0.763 | Acc: 77.740%\n",
      "[=================================================>] 157/157 | Loss: 1.512 | Acc: 62.620%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 0.751 | Acc: 78.158%\n",
      "[=================================================>] 157/157 | Loss: 1.438 | Acc: 63.780%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 0.737 | Acc: 78.550%\n",
      "[=================================================>] 157/157 | Loss: 1.537 | Acc: 62.290%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 0.731 | Acc: 78.656%\n",
      "[=================================================>] 157/157 | Loss: 1.586 | Acc: 61.800%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 0.716 | Acc: 79.248%\n",
      "[=================================================>] 157/157 | Loss: 1.521 | Acc: 63.110%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 0.707 | Acc: 79.366%\n",
      "[=================================================>] 157/157 | Loss: 1.498 | Acc: 62.850%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 0.689 | Acc: 79.778%\n",
      "[=================================================>] 157/157 | Loss: 1.553 | Acc: 61.500%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 0.684 | Acc: 79.914%\n",
      "[=================================================>] 157/157 | Loss: 1.436 | Acc: 64.740%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 0.668 | Acc: 80.328%\n",
      "[=================================================>] 157/157 | Loss: 1.594 | Acc: 62.180%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 0.672 | Acc: 80.210%\n",
      "[=================================================>] 157/157 | Loss: 1.494 | Acc: 63.840%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 0.655 | Acc: 80.536%\n",
      "[=================================================>] 157/157 | Loss: 1.571 | Acc: 62.560%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 0.648 | Acc: 80.696%\n",
      "[=================================================>] 157/157 | Loss: 1.465 | Acc: 63.930%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 0.629 | Acc: 81.392%\n",
      "[=================================================>] 157/157 | Loss: 1.511 | Acc: 64.060%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 0.620 | Acc: 81.672%\n",
      "[=================================================>] 157/157 | Loss: 1.482 | Acc: 64.120%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 0.618 | Acc: 81.882%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 61.880%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 0.601 | Acc: 82.060%\n",
      "[=================================================>] 157/157 | Loss: 1.481 | Acc: 63.580%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 0.598 | Acc: 82.394%\n",
      "[=================================================>] 157/157 | Loss: 1.499 | Acc: 64.470%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 0.578 | Acc: 82.830%\n",
      "[=================================================>] 157/157 | Loss: 1.525 | Acc: 64.170%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 0.572 | Acc: 82.956%\n",
      "[=================================================>] 157/157 | Loss: 1.504 | Acc: 64.030%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 0.567 | Acc: 83.072%\n",
      "[=================================================>] 157/157 | Loss: 1.534 | Acc: 63.810%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 0.561 | Acc: 83.310%\n",
      "[=================================================>] 157/157 | Loss: 1.474 | Acc: 64.980%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 0.555 | Acc: 83.464%\n",
      "[=================================================>] 157/157 | Loss: 1.548 | Acc: 63.660%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 0.543 | Acc: 83.926%\n",
      "[=================================================>] 157/157 | Loss: 1.494 | Acc: 64.920%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 0.533 | Acc: 84.098%\n",
      "[=================================================>] 157/157 | Loss: 1.481 | Acc: 64.890%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 0.527 | Acc: 84.086%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 64.770%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 0.516 | Acc: 84.444%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 65.040%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 0.505 | Acc: 84.772%\n",
      "[=================================================>] 157/157 | Loss: 1.569 | Acc: 63.980%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 0.487 | Acc: 85.198%\n",
      "[=================================================>] 157/157 | Loss: 1.604 | Acc: 63.240%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 0.484 | Acc: 85.286%\n",
      "[=================================================>] 157/157 | Loss: 1.504 | Acc: 65.120%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 0.480 | Acc: 85.534%\n",
      "[=================================================>] 157/157 | Loss: 1.499 | Acc: 65.590%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 0.474 | Acc: 85.714%\n",
      "[=================================================>] 157/157 | Loss: 1.576 | Acc: 64.420%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 0.462 | Acc: 85.996%\n",
      "[=================================================>] 157/157 | Loss: 1.505 | Acc: 65.640%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 0.449 | Acc: 86.610%\n",
      "[=================================================>] 157/157 | Loss: 1.558 | Acc: 64.290%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 0.445 | Acc: 86.844%\n",
      "[=================================================>] 157/157 | Loss: 1.526 | Acc: 64.920%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 0.438 | Acc: 86.754%\n",
      "[=================================================>] 157/157 | Loss: 1.534 | Acc: 65.100%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 0.425 | Acc: 87.108%\n",
      "[=================================================>] 157/157 | Loss: 1.551 | Acc: 65.030%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 0.409 | Acc: 87.566%\n",
      "[=================================================>] 157/157 | Loss: 1.527 | Acc: 66.030%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 0.407 | Acc: 87.672%\n",
      "[=================================================>] 157/157 | Loss: 1.567 | Acc: 65.110%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 0.391 | Acc: 88.082%\n",
      "[=================================================>] 157/157 | Loss: 1.532 | Acc: 65.800%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 0.388 | Acc: 88.330%\n",
      "[=================================================>] 157/157 | Loss: 1.577 | Acc: 65.680%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 0.371 | Acc: 88.714%\n",
      "[=================================================>] 157/157 | Loss: 1.606 | Acc: 64.910%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 0.372 | Acc: 88.644%\n",
      "[=================================================>] 157/157 | Loss: 1.573 | Acc: 65.490%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.358 | Acc: 89.092%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 66.090%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.345 | Acc: 89.520%\n",
      "[=================================================>] 157/157 | Loss: 1.577 | Acc: 66.180%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.341 | Acc: 89.674%\n",
      "[=================================================>] 157/157 | Loss: 1.614 | Acc: 65.810%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.338 | Acc: 89.704%\n",
      "[=================================================>] 157/157 | Loss: 1.577 | Acc: 65.750%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.322 | Acc: 90.142%\n",
      "[=================================================>] 157/157 | Loss: 1.578 | Acc: 65.800%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.317 | Acc: 90.186%\n",
      "[=================================================>] 157/157 | Loss: 1.565 | Acc: 66.150%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.310 | Acc: 90.548%\n",
      "[=================================================>] 157/157 | Loss: 1.576 | Acc: 66.110%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.304 | Acc: 90.812%\n",
      "[=================================================>] 157/157 | Loss: 1.591 | Acc: 65.820%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.288 | Acc: 91.088%\n",
      "[=================================================>] 157/157 | Loss: 1.574 | Acc: 66.370%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.286 | Acc: 91.244%\n",
      "[=================================================>] 157/157 | Loss: 1.609 | Acc: 66.360%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.273 | Acc: 91.748%\n",
      "[=================================================>] 157/157 | Loss: 1.607 | Acc: 65.720%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.268 | Acc: 91.822%\n",
      "[=================================================>] 157/157 | Loss: 1.591 | Acc: 66.990%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.257 | Acc: 92.188%\n",
      "[=================================================>] 157/157 | Loss: 1.589 | Acc: 66.820%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.244 | Acc: 92.496%\n",
      "[=================================================>] 157/157 | Loss: 1.596 | Acc: 67.360%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.237 | Acc: 92.708%\n",
      "[=================================================>] 157/157 | Loss: 1.661 | Acc: 65.670%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.229 | Acc: 92.902%\n",
      "[=================================================>] 157/157 | Loss: 1.623 | Acc: 66.790%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.223 | Acc: 93.128%\n",
      "[=================================================>] 157/157 | Loss: 1.628 | Acc: 66.630%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.215 | Acc: 93.456%\n",
      "[=================================================>] 157/157 | Loss: 1.634 | Acc: 66.520%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.210 | Acc: 93.660%\n",
      "[=================================================>] 157/157 | Loss: 1.638 | Acc: 66.980%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.203 | Acc: 93.788%\n",
      "[=================================================>] 157/157 | Loss: 1.667 | Acc: 66.780%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.192 | Acc: 94.140%\n",
      "[=================================================>] 157/157 | Loss: 1.624 | Acc: 66.960%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.186 | Acc: 94.256%\n",
      "[=================================================>] 157/157 | Loss: 1.606 | Acc: 67.700%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.179 | Acc: 94.484%\n",
      "[=================================================>] 157/157 | Loss: 1.567 | Acc: 68.370%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.169 | Acc: 94.900%\n",
      "[=================================================>] 157/157 | Loss: 1.630 | Acc: 67.440%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.165 | Acc: 94.984%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 67.210%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.156 | Acc: 95.130%\n",
      "[=================================================>] 157/157 | Loss: 1.705 | Acc: 67.070%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.152 | Acc: 95.362%\n",
      "[=================================================>] 157/157 | Loss: 1.640 | Acc: 67.650%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.138 | Acc: 95.800%\n",
      "[=================================================>] 157/157 | Loss: 1.669 | Acc: 67.960%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.138 | Acc: 95.804%\n",
      "[=================================================>] 157/157 | Loss: 1.672 | Acc: 67.320%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.133 | Acc: 95.888%\n",
      "[=================================================>] 157/157 | Loss: 1.657 | Acc: 67.490%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.120 | Acc: 96.406%\n",
      "[=================================================>] 157/157 | Loss: 1.707 | Acc: 67.390%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.122 | Acc: 96.266%\n",
      "[=================================================>] 157/157 | Loss: 1.675 | Acc: 68.090%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.109 | Acc: 96.706%\n",
      "[=================================================>] 157/157 | Loss: 1.674 | Acc: 68.440%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.106 | Acc: 96.814%\n",
      "[=================================================>] 157/157 | Loss: 1.702 | Acc: 68.330%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.097 | Acc: 97.044%\n",
      "[=================================================>] 157/157 | Loss: 1.675 | Acc: 68.390%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.094 | Acc: 97.170%\n",
      "[=================================================>] 157/157 | Loss: 1.677 | Acc: 68.560%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.087 | Acc: 97.368%\n",
      "[=================================================>] 157/157 | Loss: 1.682 | Acc: 68.950%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.080 | Acc: 97.608%\n",
      "[=================================================>] 157/157 | Loss: 1.698 | Acc: 68.880%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.076 | Acc: 97.690%\n",
      "[=================================================>] 157/157 | Loss: 1.686 | Acc: 69.500%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.074 | Acc: 97.724%\n",
      "[=================================================>] 157/157 | Loss: 1.688 | Acc: 68.620%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.065 | Acc: 98.110%\n",
      "[=================================================>] 157/157 | Loss: 1.737 | Acc: 68.500%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.062 | Acc: 98.184%\n",
      "[=================================================>] 157/157 | Loss: 1.688 | Acc: 69.220%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.054 | Acc: 98.440%\n",
      "[=================================================>] 157/157 | Loss: 1.713 | Acc: 68.910%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.051 | Acc: 98.494%\n",
      "[=================================================>] 157/157 | Loss: 1.727 | Acc: 69.400%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.051 | Acc: 98.492%\n",
      "[=================================================>] 157/157 | Loss: 1.709 | Acc: 69.610%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.043 | Acc: 98.758%\n",
      "[=================================================>] 157/157 | Loss: 1.705 | Acc: 69.380%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.042 | Acc: 98.750%\n",
      "[=================================================>] 157/157 | Loss: 1.696 | Acc: 69.590%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.039 | Acc: 98.830%\n",
      "[=================================================>] 157/157 | Loss: 1.718 | Acc: 69.270%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.034 | Acc: 99.050%\n",
      "[=================================================>] 157/157 | Loss: 1.685 | Acc: 69.850%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.031 | Acc: 99.146%\n",
      "[=================================================>] 157/157 | Loss: 1.703 | Acc: 69.700%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.030 | Acc: 99.158%\n",
      "[=================================================>] 157/157 | Loss: 1.701 | Acc: 69.970%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.027 | Acc: 99.252%\n",
      "[=================================================>] 157/157 | Loss: 1.688 | Acc: 70.020%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.024 | Acc: 99.360%\n",
      "[=================================================>] 157/157 | Loss: 1.685 | Acc: 69.990%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.023 | Acc: 99.396%\n",
      "[=================================================>] 157/157 | Loss: 1.671 | Acc: 70.080%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.021 | Acc: 99.438%\n",
      "[=================================================>] 157/157 | Loss: 1.688 | Acc: 70.070%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.508%\n",
      "[=================================================>] 157/157 | Loss: 1.696 | Acc: 70.010%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.017 | Acc: 99.542%\n",
      "[=================================================>] 157/157 | Loss: 1.685 | Acc: 70.430%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.610%\n",
      "[=================================================>] 157/157 | Loss: 1.677 | Acc: 70.430%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.015 | Acc: 99.594%\n",
      "[=================================================>] 157/157 | Loss: 1.681 | Acc: 70.550%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.015 | Acc: 99.632%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 71.030%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.015 | Acc: 99.636%\n",
      "[=================================================>] 157/157 | Loss: 1.655 | Acc: 70.810%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.012 | Acc: 99.688%\n",
      "[=================================================>] 157/157 | Loss: 1.659 | Acc: 70.770%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.680%\n",
      "[=================================================>] 157/157 | Loss: 1.640 | Acc: 70.830%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.012 | Acc: 99.692%\n",
      "[=================================================>] 157/157 | Loss: 1.643 | Acc: 71.040%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.012 | Acc: 99.728%\n",
      "[=================================================>] 157/157 | Loss: 1.653 | Acc: 70.840%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.758%\n",
      "[=================================================>] 157/157 | Loss: 1.646 | Acc: 70.830%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.790%\n",
      "[=================================================>] 157/157 | Loss: 1.642 | Acc: 70.840%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.772%\n",
      "[=================================================>] 157/157 | Loss: 1.639 | Acc: 70.770%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.786%\n",
      "[=================================================>] 157/157 | Loss: 1.641 | Acc: 71.010%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.796%\n",
      "[=================================================>] 157/157 | Loss: 1.644 | Acc: 70.890%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.746%\n",
      "[=================================================>] 157/157 | Loss: 1.638 | Acc: 71.190%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.796%\n",
      "[=================================================>] 157/157 | Loss: 1.628 | Acc: 71.050%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.800%\n",
      "[=================================================>] 157/157 | Loss: 1.639 | Acc: 71.060%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.792%\n",
      "[=================================================>] 157/157 | Loss: 1.630 | Acc: 70.930%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.792%\n",
      "[=================================================>] 157/157 | Loss: 1.628 | Acc: 71.120%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.826%\n",
      "[=================================================>] 157/157 | Loss: 1.633 | Acc: 70.910%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.838%\n",
      "[=================================================>] 157/157 | Loss: 1.633 | Acc: 70.730%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.820%\n",
      "[=================================================>] 157/157 | Loss: 1.633 | Acc: 70.860%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.820%\n",
      "[=================================================>] 157/157 | Loss: 1.628 | Acc: 71.020%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.826%\n",
      "[=================================================>] 157/157 | Loss: 1.640 | Acc: 71.020%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 21.45M (36.9% reduction)\n",
      "MACs: 0.33G => 0.14G (59.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'avg'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 4\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_avg_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e3e164",
   "metadata": {},
   "source": [
    "##### ~36% filters pruned (512*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c3e24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.790%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 12, 3: 16, 7: 24, 10: 37, 14: 88, 17: 82, 20: 73, 24: 198, 27: 172, 30: 190, 34: 188, 37: 170, 40: 286}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(52, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(48, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(104, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(91, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(168, 174, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(174, 183, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(183, 314, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(314, 340, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(340, 322, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(322, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(324, 342, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(342, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 1.639 | Acc: 57.150%\n",
      "[=================================================>] 157/157 | Loss: 2.066 | Acc: 49.060%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 1.456 | Acc: 60.566%\n",
      "[=================================================>] 157/157 | Loss: 1.907 | Acc: 51.860%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 1.354 | Acc: 62.878%\n",
      "[=================================================>] 157/157 | Loss: 1.836 | Acc: 53.750%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 1.271 | Acc: 64.792%\n",
      "[=================================================>] 157/157 | Loss: 1.809 | Acc: 54.210%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 1.216 | Acc: 66.184%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 57.200%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 1.147 | Acc: 67.656%\n",
      "[=================================================>] 157/157 | Loss: 1.621 | Acc: 58.300%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 1.091 | Acc: 69.384%\n",
      "[=================================================>] 157/157 | Loss: 1.618 | Acc: 58.650%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 1.055 | Acc: 70.090%\n",
      "[=================================================>] 157/157 | Loss: 1.606 | Acc: 58.800%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 1.030 | Acc: 70.806%\n",
      "[=================================================>] 157/157 | Loss: 1.568 | Acc: 60.120%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 0.994 | Acc: 71.612%\n",
      "[=================================================>] 157/157 | Loss: 1.470 | Acc: 61.510%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 0.955 | Acc: 72.722%\n",
      "[=================================================>] 157/157 | Loss: 1.635 | Acc: 59.810%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 0.931 | Acc: 73.366%\n",
      "[=================================================>] 157/157 | Loss: 1.579 | Acc: 60.250%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 0.904 | Acc: 74.040%\n",
      "[=================================================>] 157/157 | Loss: 1.537 | Acc: 61.060%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 0.878 | Acc: 74.810%\n",
      "[=================================================>] 157/157 | Loss: 1.584 | Acc: 60.940%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 0.858 | Acc: 75.282%\n",
      "[=================================================>] 157/157 | Loss: 1.492 | Acc: 62.000%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 0.841 | Acc: 75.872%\n",
      "[=================================================>] 157/157 | Loss: 1.544 | Acc: 61.290%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 0.819 | Acc: 76.460%\n",
      "[=================================================>] 157/157 | Loss: 1.428 | Acc: 63.760%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 0.804 | Acc: 76.634%\n",
      "[=================================================>] 157/157 | Loss: 1.582 | Acc: 60.620%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 0.790 | Acc: 77.220%\n",
      "[=================================================>] 157/157 | Loss: 1.496 | Acc: 62.880%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 0.762 | Acc: 77.748%\n",
      "[=================================================>] 157/157 | Loss: 1.550 | Acc: 61.640%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 0.752 | Acc: 78.022%\n",
      "[=================================================>] 157/157 | Loss: 1.491 | Acc: 63.430%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 0.747 | Acc: 78.320%\n",
      "[=================================================>] 157/157 | Loss: 1.513 | Acc: 63.000%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 0.722 | Acc: 78.882%\n",
      "[=================================================>] 157/157 | Loss: 1.531 | Acc: 62.570%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 0.712 | Acc: 79.130%\n",
      "[=================================================>] 157/157 | Loss: 1.452 | Acc: 64.030%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 0.703 | Acc: 79.376%\n",
      "[=================================================>] 157/157 | Loss: 1.518 | Acc: 63.000%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 0.691 | Acc: 79.852%\n",
      "[=================================================>] 157/157 | Loss: 1.563 | Acc: 62.520%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 0.669 | Acc: 80.304%\n",
      "[=================================================>] 157/157 | Loss: 1.528 | Acc: 62.130%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 0.659 | Acc: 80.566%\n",
      "[=================================================>] 157/157 | Loss: 1.540 | Acc: 63.090%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 0.654 | Acc: 80.752%\n",
      "[=================================================>] 157/157 | Loss: 1.513 | Acc: 63.420%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 0.639 | Acc: 81.222%\n",
      "[=================================================>] 157/157 | Loss: 1.538 | Acc: 63.020%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 0.637 | Acc: 81.314%\n",
      "[=================================================>] 157/157 | Loss: 1.529 | Acc: 62.880%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 0.622 | Acc: 81.464%\n",
      "[=================================================>] 157/157 | Loss: 1.622 | Acc: 62.090%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 0.612 | Acc: 82.072%\n",
      "[=================================================>] 157/157 | Loss: 1.487 | Acc: 64.410%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 0.599 | Acc: 82.280%\n",
      "[=================================================>] 157/157 | Loss: 1.512 | Acc: 63.720%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 0.591 | Acc: 82.412%\n",
      "[=================================================>] 157/157 | Loss: 1.483 | Acc: 64.680%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 0.581 | Acc: 82.600%\n",
      "[=================================================>] 157/157 | Loss: 1.513 | Acc: 63.940%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 0.572 | Acc: 83.024%\n",
      "[=================================================>] 157/157 | Loss: 1.508 | Acc: 64.340%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 0.567 | Acc: 83.146%\n",
      "[=================================================>] 157/157 | Loss: 1.486 | Acc: 64.800%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 0.551 | Acc: 83.860%\n",
      "[=================================================>] 157/157 | Loss: 1.522 | Acc: 64.530%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 0.550 | Acc: 83.484%\n",
      "[=================================================>] 157/157 | Loss: 1.515 | Acc: 64.070%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 0.537 | Acc: 83.994%\n",
      "[=================================================>] 157/157 | Loss: 1.547 | Acc: 63.920%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 0.538 | Acc: 84.038%\n",
      "[=================================================>] 157/157 | Loss: 1.551 | Acc: 63.770%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 0.521 | Acc: 84.292%\n",
      "[=================================================>] 157/157 | Loss: 1.568 | Acc: 62.760%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 0.516 | Acc: 84.572%\n",
      "[=================================================>] 157/157 | Loss: 1.509 | Acc: 64.370%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 0.496 | Acc: 85.108%\n",
      "[=================================================>] 157/157 | Loss: 1.535 | Acc: 64.320%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 0.494 | Acc: 85.076%\n",
      "[=================================================>] 157/157 | Loss: 1.536 | Acc: 64.740%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 0.483 | Acc: 85.478%\n",
      "[=================================================>] 157/157 | Loss: 1.514 | Acc: 65.090%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 0.480 | Acc: 85.638%\n",
      "[=================================================>] 157/157 | Loss: 1.515 | Acc: 65.220%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 0.468 | Acc: 85.850%\n",
      "[=================================================>] 157/157 | Loss: 1.641 | Acc: 63.460%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 0.468 | Acc: 86.000%\n",
      "[=================================================>] 157/157 | Loss: 1.504 | Acc: 64.760%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 0.456 | Acc: 86.344%\n",
      "[=================================================>] 157/157 | Loss: 1.559 | Acc: 64.510%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 0.447 | Acc: 86.618%\n",
      "[=================================================>] 157/157 | Loss: 1.591 | Acc: 64.320%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 0.437 | Acc: 86.886%\n",
      "[=================================================>] 157/157 | Loss: 1.586 | Acc: 64.940%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 0.424 | Acc: 87.218%\n",
      "[=================================================>] 157/157 | Loss: 1.559 | Acc: 64.920%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 0.418 | Acc: 87.438%\n",
      "[=================================================>] 157/157 | Loss: 1.578 | Acc: 64.560%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 0.400 | Acc: 87.816%\n",
      "[=================================================>] 157/157 | Loss: 1.552 | Acc: 64.990%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 0.402 | Acc: 87.790%\n",
      "[=================================================>] 157/157 | Loss: 1.508 | Acc: 65.260%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 0.391 | Acc: 88.246%\n",
      "[=================================================>] 157/157 | Loss: 1.543 | Acc: 65.240%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 0.390 | Acc: 88.214%\n",
      "[=================================================>] 157/157 | Loss: 1.550 | Acc: 65.180%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 0.373 | Acc: 88.682%\n",
      "[=================================================>] 157/157 | Loss: 1.585 | Acc: 65.220%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 0.367 | Acc: 88.790%\n",
      "[=================================================>] 157/157 | Loss: 1.536 | Acc: 65.150%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 0.363 | Acc: 88.950%\n",
      "[=================================================>] 157/157 | Loss: 1.564 | Acc: 65.440%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 0.358 | Acc: 89.208%\n",
      "[=================================================>] 157/157 | Loss: 1.563 | Acc: 65.330%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 0.347 | Acc: 89.514%\n",
      "[=================================================>] 157/157 | Loss: 1.558 | Acc: 66.520%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 0.337 | Acc: 89.806%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 66.200%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 0.328 | Acc: 90.060%\n",
      "[=================================================>] 157/157 | Loss: 1.567 | Acc: 65.770%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 0.324 | Acc: 90.228%\n",
      "[=================================================>] 157/157 | Loss: 1.529 | Acc: 66.600%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 0.318 | Acc: 90.378%\n",
      "[=================================================>] 157/157 | Loss: 1.635 | Acc: 65.280%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 0.305 | Acc: 90.720%\n",
      "[=================================================>] 157/157 | Loss: 1.568 | Acc: 65.740%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 0.297 | Acc: 90.942%\n",
      "[=================================================>] 157/157 | Loss: 1.566 | Acc: 66.370%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.291 | Acc: 91.264%\n",
      "[=================================================>] 157/157 | Loss: 1.652 | Acc: 65.310%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.279 | Acc: 91.450%\n",
      "[=================================================>] 157/157 | Loss: 1.566 | Acc: 66.550%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.279 | Acc: 91.690%\n",
      "[=================================================>] 157/157 | Loss: 1.527 | Acc: 67.470%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.269 | Acc: 91.854%\n",
      "[=================================================>] 157/157 | Loss: 1.583 | Acc: 66.790%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.260 | Acc: 92.092%\n",
      "[=================================================>] 157/157 | Loss: 1.589 | Acc: 66.390%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.255 | Acc: 92.286%\n",
      "[=================================================>] 157/157 | Loss: 1.538 | Acc: 67.230%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.250 | Acc: 92.258%\n",
      "[=================================================>] 157/157 | Loss: 1.565 | Acc: 66.310%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.235 | Acc: 92.950%\n",
      "[=================================================>] 157/157 | Loss: 1.572 | Acc: 67.380%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.229 | Acc: 92.950%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 67.800%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.226 | Acc: 93.126%\n",
      "[=================================================>] 157/157 | Loss: 1.581 | Acc: 66.850%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.219 | Acc: 93.336%\n",
      "[=================================================>] 157/157 | Loss: 1.590 | Acc: 66.930%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.208 | Acc: 93.666%\n",
      "[=================================================>] 157/157 | Loss: 1.574 | Acc: 67.730%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.200 | Acc: 93.868%\n",
      "[=================================================>] 157/157 | Loss: 1.618 | Acc: 67.380%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.192 | Acc: 94.242%\n",
      "[=================================================>] 157/157 | Loss: 1.572 | Acc: 67.920%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.190 | Acc: 94.244%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 67.820%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.183 | Acc: 94.414%\n",
      "[=================================================>] 157/157 | Loss: 1.638 | Acc: 66.760%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.174 | Acc: 94.640%\n",
      "[=================================================>] 157/157 | Loss: 1.579 | Acc: 67.760%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.170 | Acc: 94.852%\n",
      "[=================================================>] 157/157 | Loss: 1.649 | Acc: 67.710%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.162 | Acc: 95.126%\n",
      "[=================================================>] 157/157 | Loss: 1.577 | Acc: 68.310%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.155 | Acc: 95.356%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 67.060%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.150 | Acc: 95.464%\n",
      "[=================================================>] 157/157 | Loss: 1.595 | Acc: 67.800%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.142 | Acc: 95.678%\n",
      "[=================================================>] 157/157 | Loss: 1.570 | Acc: 68.460%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.139 | Acc: 95.724%\n",
      "[=================================================>] 157/157 | Loss: 1.611 | Acc: 68.540%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.133 | Acc: 95.940%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 67.890%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.124 | Acc: 96.190%\n",
      "[=================================================>] 157/157 | Loss: 1.641 | Acc: 68.050%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.119 | Acc: 96.408%\n",
      "[=================================================>] 157/157 | Loss: 1.636 | Acc: 68.610%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.113 | Acc: 96.528%\n",
      "[=================================================>] 157/157 | Loss: 1.644 | Acc: 68.190%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.106 | Acc: 96.762%\n",
      "[=================================================>] 157/157 | Loss: 1.645 | Acc: 68.110%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.104 | Acc: 96.900%\n",
      "[=================================================>] 157/157 | Loss: 1.650 | Acc: 68.530%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.095 | Acc: 97.098%\n",
      "[=================================================>] 157/157 | Loss: 1.649 | Acc: 68.160%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.092 | Acc: 97.284%\n",
      "[=================================================>] 157/157 | Loss: 1.637 | Acc: 68.830%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.084 | Acc: 97.480%\n",
      "[=================================================>] 157/157 | Loss: 1.630 | Acc: 68.980%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.077 | Acc: 97.648%\n",
      "[=================================================>] 157/157 | Loss: 1.626 | Acc: 68.490%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.077 | Acc: 97.670%\n",
      "[=================================================>] 157/157 | Loss: 1.650 | Acc: 68.700%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.069 | Acc: 97.982%\n",
      "[=================================================>] 157/157 | Loss: 1.615 | Acc: 69.340%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.068 | Acc: 97.890%\n",
      "[=================================================>] 157/157 | Loss: 1.594 | Acc: 69.310%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.060 | Acc: 98.216%\n",
      "[=================================================>] 157/157 | Loss: 1.632 | Acc: 69.230%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.056 | Acc: 98.288%\n",
      "[=================================================>] 157/157 | Loss: 1.667 | Acc: 69.310%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.054 | Acc: 98.436%\n",
      "[=================================================>] 157/157 | Loss: 1.635 | Acc: 69.520%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.051 | Acc: 98.494%\n",
      "[=================================================>] 157/157 | Loss: 1.643 | Acc: 69.790%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.041 | Acc: 98.832%\n",
      "[=================================================>] 157/157 | Loss: 1.634 | Acc: 69.960%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.040 | Acc: 98.832%\n",
      "[=================================================>] 157/157 | Loss: 1.633 | Acc: 69.880%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.036 | Acc: 99.000%\n",
      "[=================================================>] 157/157 | Loss: 1.617 | Acc: 70.120%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.034 | Acc: 99.012%\n",
      "[=================================================>] 157/157 | Loss: 1.646 | Acc: 69.370%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.032 | Acc: 99.106%\n",
      "[=================================================>] 157/157 | Loss: 1.636 | Acc: 69.900%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.029 | Acc: 99.184%\n",
      "[=================================================>] 157/157 | Loss: 1.609 | Acc: 70.070%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.026 | Acc: 99.270%\n",
      "[=================================================>] 157/157 | Loss: 1.600 | Acc: 69.880%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.024 | Acc: 99.344%\n",
      "[=================================================>] 157/157 | Loss: 1.599 | Acc: 70.990%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.019 | Acc: 99.506%\n",
      "[=================================================>] 157/157 | Loss: 1.591 | Acc: 71.000%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.019 | Acc: 99.506%\n",
      "[=================================================>] 157/157 | Loss: 1.581 | Acc: 70.810%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.546%\n",
      "[=================================================>] 157/157 | Loss: 1.587 | Acc: 71.050%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.594%\n",
      "[=================================================>] 157/157 | Loss: 1.571 | Acc: 71.180%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.014 | Acc: 99.648%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 71.310%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.686%\n",
      "[=================================================>] 157/157 | Loss: 1.545 | Acc: 71.160%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.736%\n",
      "[=================================================>] 157/157 | Loss: 1.565 | Acc: 71.040%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.750%\n",
      "[=================================================>] 157/157 | Loss: 1.575 | Acc: 71.260%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.752%\n",
      "[=================================================>] 157/157 | Loss: 1.560 | Acc: 71.350%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.752%\n",
      "[=================================================>] 157/157 | Loss: 1.528 | Acc: 71.420%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.766%\n",
      "[=================================================>] 157/157 | Loss: 1.545 | Acc: 71.870%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.824%\n",
      "[=================================================>] 157/157 | Loss: 1.524 | Acc: 71.550%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.796%\n",
      "[=================================================>] 157/157 | Loss: 1.524 | Acc: 71.810%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.842%\n",
      "[=================================================>] 157/157 | Loss: 1.519 | Acc: 71.900%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.844%\n",
      "[=================================================>] 157/157 | Loss: 1.522 | Acc: 71.720%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.842%\n",
      "[=================================================>] 157/157 | Loss: 1.506 | Acc: 71.620%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.862%\n",
      "[=================================================>] 157/157 | Loss: 1.505 | Acc: 71.740%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.880%\n",
      "[=================================================>] 157/157 | Loss: 1.505 | Acc: 71.850%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.862%\n",
      "[=================================================>] 157/157 | Loss: 1.507 | Acc: 71.830%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.874%\n",
      "[=================================================>] 157/157 | Loss: 1.503 | Acc: 71.770%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.894%\n",
      "[=================================================>] 157/157 | Loss: 1.510 | Acc: 71.990%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.898%\n",
      "[=================================================>] 157/157 | Loss: 1.494 | Acc: 71.950%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.870%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 71.680%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.862%\n",
      "[=================================================>] 157/157 | Loss: 1.504 | Acc: 71.950%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.888%\n",
      "[=================================================>] 157/157 | Loss: 1.500 | Acc: 71.970%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.892%\n",
      "[=================================================>] 157/157 | Loss: 1.500 | Acc: 71.830%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.884%\n",
      "[=================================================>] 157/157 | Loss: 1.496 | Acc: 71.680%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.910%\n",
      "[=================================================>] 157/157 | Loss: 1.503 | Acc: 72.070%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.896%\n",
      "[=================================================>] 157/157 | Loss: 1.498 | Acc: 71.930%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.888%\n",
      "[=================================================>] 157/157 | Loss: 1.493 | Acc: 71.720%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.894%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 71.850%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.892%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 72.050%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 24.07M (29.3% reduction)\n",
      "MACs: 0.33G => 0.17G (48.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'avg'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 3\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_avg_30_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f793a",
   "metadata": {},
   "source": [
    "##### ~72% filters pruned (512*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e45b3b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.790%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 45, 3: 29, 7: 70, 10: 60, 14: 173, 17: 178, 20: 180, 24: 372, 27: 371, 30: 391, 34: 365, 37: 366, 40: 472}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(19, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(35, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(58, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(68, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(83, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(78, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(76, 140, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(140, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(141, 121, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(121, 147, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(147, 146, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(146, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 4.126 | Acc: 5.248%\n",
      "[=================================================>] 157/157 | Loss: 3.666 | Acc: 9.170%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 3.271 | Acc: 15.686%\n",
      "[=================================================>] 157/157 | Loss: 3.554 | Acc: 15.430%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 2.907 | Acc: 22.988%\n",
      "[=================================================>] 157/157 | Loss: 3.274 | Acc: 22.430%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 2.665 | Acc: 28.656%\n",
      "[=================================================>] 157/157 | Loss: 2.698 | Acc: 28.830%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 2.498 | Acc: 32.650%\n",
      "[=================================================>] 157/157 | Loss: 2.697 | Acc: 30.140%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 2.361 | Acc: 35.802%\n",
      "[=================================================>] 157/157 | Loss: 2.289 | Acc: 37.390%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 2.263 | Acc: 37.984%\n",
      "[=================================================>] 157/157 | Loss: 2.221 | Acc: 40.160%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 2.164 | Acc: 40.720%\n",
      "[=================================================>] 157/157 | Loss: 2.125 | Acc: 41.420%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 2.095 | Acc: 42.410%\n",
      "[=================================================>] 157/157 | Loss: 2.135 | Acc: 41.950%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 2.016 | Acc: 44.458%\n",
      "[=================================================>] 157/157 | Loss: 1.947 | Acc: 46.450%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 1.965 | Acc: 45.540%\n",
      "[=================================================>] 157/157 | Loss: 1.998 | Acc: 45.780%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 1.916 | Acc: 47.142%\n",
      "[=================================================>] 157/157 | Loss: 2.153 | Acc: 43.430%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 1.876 | Acc: 48.156%\n",
      "[=================================================>] 157/157 | Loss: 1.907 | Acc: 48.220%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 1.837 | Acc: 49.148%\n",
      "[=================================================>] 157/157 | Loss: 1.977 | Acc: 47.090%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 1.796 | Acc: 50.428%\n",
      "[=================================================>] 157/157 | Loss: 1.991 | Acc: 47.110%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 1.768 | Acc: 50.934%\n",
      "[=================================================>] 157/157 | Loss: 1.881 | Acc: 49.140%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 1.738 | Acc: 51.838%\n",
      "[=================================================>] 157/157 | Loss: 1.800 | Acc: 51.210%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 1.704 | Acc: 52.714%\n",
      "[=================================================>] 157/157 | Loss: 1.782 | Acc: 51.760%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 1.677 | Acc: 53.260%\n",
      "[=================================================>] 157/157 | Loss: 1.857 | Acc: 49.820%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 1.650 | Acc: 54.254%\n",
      "[=================================================>] 157/157 | Loss: 1.902 | Acc: 50.350%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 1.631 | Acc: 54.712%\n",
      "[=================================================>] 157/157 | Loss: 1.732 | Acc: 53.370%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 1.594 | Acc: 55.688%\n",
      "[=================================================>] 157/157 | Loss: 1.778 | Acc: 52.310%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 1.589 | Acc: 55.676%\n",
      "[=================================================>] 157/157 | Loss: 1.761 | Acc: 52.790%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 1.567 | Acc: 56.548%\n",
      "[=================================================>] 157/157 | Loss: 1.742 | Acc: 53.260%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 1.542 | Acc: 56.794%\n",
      "[=================================================>] 157/157 | Loss: 1.816 | Acc: 52.210%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 1.524 | Acc: 57.348%\n",
      "[=================================================>] 157/157 | Loss: 1.668 | Acc: 55.060%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 1.518 | Acc: 57.562%\n",
      "[=================================================>] 157/157 | Loss: 1.824 | Acc: 52.290%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 1.496 | Acc: 58.284%\n",
      "[=================================================>] 157/157 | Loss: 1.674 | Acc: 54.580%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 1.476 | Acc: 58.812%\n",
      "[=================================================>] 157/157 | Loss: 1.663 | Acc: 55.240%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 1.456 | Acc: 59.218%\n",
      "[=================================================>] 157/157 | Loss: 1.644 | Acc: 55.840%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 1.450 | Acc: 59.550%\n",
      "[=================================================>] 157/157 | Loss: 1.684 | Acc: 54.460%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 1.430 | Acc: 59.844%\n",
      "[=================================================>] 157/157 | Loss: 1.754 | Acc: 53.930%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 1.414 | Acc: 60.304%\n",
      "[=================================================>] 157/157 | Loss: 1.711 | Acc: 54.420%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 1.407 | Acc: 60.406%\n",
      "[=================================================>] 157/157 | Loss: 1.602 | Acc: 56.890%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 1.391 | Acc: 61.062%\n",
      "[=================================================>] 157/157 | Loss: 1.732 | Acc: 55.050%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 1.383 | Acc: 61.132%\n",
      "[=================================================>] 157/157 | Loss: 1.627 | Acc: 56.740%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 1.360 | Acc: 61.712%\n",
      "[=================================================>] 157/157 | Loss: 1.648 | Acc: 55.540%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 1.360 | Acc: 61.812%\n",
      "[=================================================>] 157/157 | Loss: 1.646 | Acc: 55.680%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 1.342 | Acc: 62.346%\n",
      "[=================================================>] 157/157 | Loss: 1.608 | Acc: 57.110%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 1.320 | Acc: 62.858%\n",
      "[=================================================>] 157/157 | Loss: 1.743 | Acc: 55.000%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 1.321 | Acc: 62.878%\n",
      "[=================================================>] 157/157 | Loss: 1.664 | Acc: 56.350%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 1.300 | Acc: 63.334%\n",
      "[=================================================>] 157/157 | Loss: 1.666 | Acc: 56.330%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 1.295 | Acc: 63.630%\n",
      "[=================================================>] 157/157 | Loss: 1.575 | Acc: 57.840%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 1.281 | Acc: 63.908%\n",
      "[=================================================>] 157/157 | Loss: 1.617 | Acc: 57.580%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 1.280 | Acc: 63.914%\n",
      "[=================================================>] 157/157 | Loss: 1.591 | Acc: 57.560%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 1.260 | Acc: 64.456%\n",
      "[=================================================>] 157/157 | Loss: 1.619 | Acc: 56.880%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 1.251 | Acc: 64.620%\n",
      "[=================================================>] 157/157 | Loss: 1.581 | Acc: 58.740%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 1.239 | Acc: 64.902%\n",
      "[=================================================>] 157/157 | Loss: 1.507 | Acc: 59.280%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 1.229 | Acc: 65.300%\n",
      "[=================================================>] 157/157 | Loss: 1.618 | Acc: 57.760%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 1.213 | Acc: 65.602%\n",
      "[=================================================>] 157/157 | Loss: 1.575 | Acc: 58.780%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 1.201 | Acc: 65.878%\n",
      "[=================================================>] 157/157 | Loss: 1.565 | Acc: 58.120%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 1.193 | Acc: 65.922%\n",
      "[=================================================>] 157/157 | Loss: 1.620 | Acc: 57.160%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 1.181 | Acc: 66.648%\n",
      "[=================================================>] 157/157 | Loss: 1.611 | Acc: 58.410%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 1.175 | Acc: 66.686%\n",
      "[=================================================>] 157/157 | Loss: 1.553 | Acc: 59.230%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 1.160 | Acc: 66.920%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 59.270%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 1.159 | Acc: 67.032%\n",
      "[=================================================>] 157/157 | Loss: 1.536 | Acc: 59.740%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 1.138 | Acc: 67.704%\n",
      "[=================================================>] 157/157 | Loss: 1.558 | Acc: 59.240%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 1.131 | Acc: 67.720%\n",
      "[=================================================>] 157/157 | Loss: 1.532 | Acc: 60.010%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 1.126 | Acc: 67.818%\n",
      "[=================================================>] 157/157 | Loss: 1.585 | Acc: 59.210%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 1.110 | Acc: 68.108%\n",
      "[=================================================>] 157/157 | Loss: 1.516 | Acc: 60.440%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 1.096 | Acc: 68.730%\n",
      "[=================================================>] 157/157 | Loss: 1.569 | Acc: 59.620%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 1.086 | Acc: 68.746%\n",
      "[=================================================>] 157/157 | Loss: 1.584 | Acc: 59.400%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 1.082 | Acc: 68.838%\n",
      "[=================================================>] 157/157 | Loss: 1.619 | Acc: 58.690%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 1.070 | Acc: 69.162%\n",
      "[=================================================>] 157/157 | Loss: 1.523 | Acc: 60.300%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 1.058 | Acc: 69.734%\n",
      "[=================================================>] 157/157 | Loss: 1.545 | Acc: 60.340%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 1.041 | Acc: 70.074%\n",
      "[=================================================>] 157/157 | Loss: 1.604 | Acc: 58.460%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 1.035 | Acc: 70.346%\n",
      "[=================================================>] 157/157 | Loss: 1.506 | Acc: 60.510%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 1.028 | Acc: 70.428%\n",
      "[=================================================>] 157/157 | Loss: 1.569 | Acc: 59.270%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 1.018 | Acc: 70.846%\n",
      "[=================================================>] 157/157 | Loss: 1.566 | Acc: 60.070%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 1.008 | Acc: 70.980%\n",
      "[=================================================>] 157/157 | Loss: 1.523 | Acc: 60.260%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.994 | Acc: 71.192%\n",
      "[=================================================>] 157/157 | Loss: 1.538 | Acc: 60.090%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.971 | Acc: 71.888%\n",
      "[=================================================>] 157/157 | Loss: 1.543 | Acc: 61.020%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.976 | Acc: 71.646%\n",
      "[=================================================>] 157/157 | Loss: 1.484 | Acc: 61.170%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.962 | Acc: 72.012%\n",
      "[=================================================>] 157/157 | Loss: 1.478 | Acc: 61.890%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.953 | Acc: 72.124%\n",
      "[=================================================>] 157/157 | Loss: 1.518 | Acc: 60.900%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.939 | Acc: 72.870%\n",
      "[=================================================>] 157/157 | Loss: 1.485 | Acc: 61.420%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.927 | Acc: 72.770%\n",
      "[=================================================>] 157/157 | Loss: 1.567 | Acc: 59.960%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.924 | Acc: 73.124%\n",
      "[=================================================>] 157/157 | Loss: 1.463 | Acc: 62.110%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.903 | Acc: 73.610%\n",
      "[=================================================>] 157/157 | Loss: 1.449 | Acc: 62.920%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.894 | Acc: 73.896%\n",
      "[=================================================>] 157/157 | Loss: 1.540 | Acc: 60.610%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.887 | Acc: 74.094%\n",
      "[=================================================>] 157/157 | Loss: 1.466 | Acc: 62.460%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.871 | Acc: 74.372%\n",
      "[=================================================>] 157/157 | Loss: 1.505 | Acc: 61.810%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.858 | Acc: 74.782%\n",
      "[=================================================>] 157/157 | Loss: 1.487 | Acc: 61.920%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.850 | Acc: 75.072%\n",
      "[=================================================>] 157/157 | Loss: 1.453 | Acc: 62.990%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.831 | Acc: 75.406%\n",
      "[=================================================>] 157/157 | Loss: 1.491 | Acc: 61.770%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.830 | Acc: 75.566%\n",
      "[=================================================>] 157/157 | Loss: 1.453 | Acc: 63.210%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.808 | Acc: 75.976%\n",
      "[=================================================>] 157/157 | Loss: 1.476 | Acc: 62.520%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.801 | Acc: 76.490%\n",
      "[=================================================>] 157/157 | Loss: 1.458 | Acc: 63.110%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.789 | Acc: 76.682%\n",
      "[=================================================>] 157/157 | Loss: 1.480 | Acc: 62.840%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.775 | Acc: 77.034%\n",
      "[=================================================>] 157/157 | Loss: 1.475 | Acc: 63.050%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.768 | Acc: 77.348%\n",
      "[=================================================>] 157/157 | Loss: 1.510 | Acc: 62.360%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.754 | Acc: 77.502%\n",
      "[=================================================>] 157/157 | Loss: 1.478 | Acc: 63.070%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.739 | Acc: 78.146%\n",
      "[=================================================>] 157/157 | Loss: 1.472 | Acc: 63.340%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.729 | Acc: 78.280%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 63.090%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.715 | Acc: 78.786%\n",
      "[=================================================>] 157/157 | Loss: 1.466 | Acc: 63.830%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.697 | Acc: 79.210%\n",
      "[=================================================>] 157/157 | Loss: 1.459 | Acc: 63.870%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.699 | Acc: 79.230%\n",
      "[=================================================>] 157/157 | Loss: 1.486 | Acc: 63.440%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.681 | Acc: 79.520%\n",
      "[=================================================>] 157/157 | Loss: 1.477 | Acc: 63.550%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.662 | Acc: 80.188%\n",
      "[=================================================>] 157/157 | Loss: 1.476 | Acc: 63.180%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.658 | Acc: 80.280%\n",
      "[=================================================>] 157/157 | Loss: 1.474 | Acc: 63.860%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.642 | Acc: 80.642%\n",
      "[=================================================>] 157/157 | Loss: 1.464 | Acc: 64.060%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.630 | Acc: 81.148%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 64.260%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.623 | Acc: 81.166%\n",
      "[=================================================>] 157/157 | Loss: 1.471 | Acc: 64.350%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.606 | Acc: 81.772%\n",
      "[=================================================>] 157/157 | Loss: 1.492 | Acc: 63.790%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.592 | Acc: 82.120%\n",
      "[=================================================>] 157/157 | Loss: 1.465 | Acc: 64.530%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.584 | Acc: 82.382%\n",
      "[=================================================>] 157/157 | Loss: 1.478 | Acc: 64.940%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.567 | Acc: 82.676%\n",
      "[=================================================>] 157/157 | Loss: 1.494 | Acc: 64.240%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.553 | Acc: 83.102%\n",
      "[=================================================>] 157/157 | Loss: 1.489 | Acc: 64.540%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.543 | Acc: 83.468%\n",
      "[=================================================>] 157/157 | Loss: 1.514 | Acc: 64.350%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.531 | Acc: 83.590%\n",
      "[=================================================>] 157/157 | Loss: 1.495 | Acc: 64.770%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.520 | Acc: 83.988%\n",
      "[=================================================>] 157/157 | Loss: 1.482 | Acc: 65.060%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.506 | Acc: 84.398%\n",
      "[=================================================>] 157/157 | Loss: 1.521 | Acc: 64.440%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.499 | Acc: 84.824%\n",
      "[=================================================>] 157/157 | Loss: 1.515 | Acc: 64.960%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.487 | Acc: 85.062%\n",
      "[=================================================>] 157/157 | Loss: 1.523 | Acc: 64.700%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.475 | Acc: 85.370%\n",
      "[=================================================>] 157/157 | Loss: 1.523 | Acc: 65.050%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.463 | Acc: 85.718%\n",
      "[=================================================>] 157/157 | Loss: 1.530 | Acc: 64.520%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.452 | Acc: 86.088%\n",
      "[=================================================>] 157/157 | Loss: 1.529 | Acc: 64.910%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.439 | Acc: 86.470%\n",
      "[=================================================>] 157/157 | Loss: 1.552 | Acc: 64.630%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.440 | Acc: 86.472%\n",
      "[=================================================>] 157/157 | Loss: 1.543 | Acc: 64.800%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.434 | Acc: 86.620%\n",
      "[=================================================>] 157/157 | Loss: 1.533 | Acc: 65.510%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.412 | Acc: 87.386%\n",
      "[=================================================>] 157/157 | Loss: 1.532 | Acc: 65.430%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.405 | Acc: 87.654%\n",
      "[=================================================>] 157/157 | Loss: 1.540 | Acc: 65.250%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.393 | Acc: 87.822%\n",
      "[=================================================>] 157/157 | Loss: 1.549 | Acc: 65.160%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.387 | Acc: 87.890%\n",
      "[=================================================>] 157/157 | Loss: 1.553 | Acc: 65.580%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.376 | Acc: 88.230%\n",
      "[=================================================>] 157/157 | Loss: 1.570 | Acc: 65.280%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.369 | Acc: 88.492%\n",
      "[=================================================>] 157/157 | Loss: 1.564 | Acc: 65.270%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.365 | Acc: 88.678%\n",
      "[=================================================>] 157/157 | Loss: 1.568 | Acc: 65.370%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.350 | Acc: 89.080%\n",
      "[=================================================>] 157/157 | Loss: 1.582 | Acc: 65.610%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.343 | Acc: 89.362%\n",
      "[=================================================>] 157/157 | Loss: 1.577 | Acc: 65.800%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.334 | Acc: 89.638%\n",
      "[=================================================>] 157/157 | Loss: 1.582 | Acc: 65.730%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.335 | Acc: 89.568%\n",
      "[=================================================>] 157/157 | Loss: 1.575 | Acc: 65.930%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.327 | Acc: 89.938%\n",
      "[=================================================>] 157/157 | Loss: 1.576 | Acc: 65.650%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.322 | Acc: 90.076%\n",
      "[=================================================>] 157/157 | Loss: 1.580 | Acc: 65.820%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.312 | Acc: 90.460%\n",
      "[=================================================>] 157/157 | Loss: 1.591 | Acc: 65.830%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.306 | Acc: 90.382%\n",
      "[=================================================>] 157/157 | Loss: 1.602 | Acc: 65.630%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.310 | Acc: 90.382%\n",
      "[=================================================>] 157/157 | Loss: 1.610 | Acc: 65.450%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.294 | Acc: 90.844%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 65.700%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.297 | Acc: 90.804%\n",
      "[=================================================>] 157/157 | Loss: 1.608 | Acc: 65.750%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.292 | Acc: 90.894%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 65.950%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.289 | Acc: 90.970%\n",
      "[=================================================>] 157/157 | Loss: 1.604 | Acc: 66.010%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.287 | Acc: 91.068%\n",
      "[=================================================>] 157/157 | Loss: 1.617 | Acc: 65.880%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.284 | Acc: 91.120%\n",
      "[=================================================>] 157/157 | Loss: 1.607 | Acc: 65.840%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.286 | Acc: 91.056%\n",
      "[=================================================>] 157/157 | Loss: 1.606 | Acc: 66.080%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.284 | Acc: 91.162%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 65.790%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.283 | Acc: 91.284%\n",
      "[=================================================>] 157/157 | Loss: 1.604 | Acc: 66.140%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.275 | Acc: 91.448%\n",
      "[=================================================>] 157/157 | Loss: 1.608 | Acc: 66.160%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.277 | Acc: 91.458%\n",
      "[=================================================>] 157/157 | Loss: 1.605 | Acc: 66.090%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.278 | Acc: 91.408%\n",
      "[=================================================>] 157/157 | Loss: 1.618 | Acc: 65.990%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.276 | Acc: 91.410%\n",
      "[=================================================>] 157/157 | Loss: 1.608 | Acc: 66.170%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.276 | Acc: 91.380%\n",
      "[=================================================>] 157/157 | Loss: 1.603 | Acc: 66.080%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 18.42M (45.9% reduction)\n",
      "MACs: 0.33G => 0.06G (82.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'avg'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 6\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_avg_70_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b7f04",
   "metadata": {},
   "source": [
    "### Global pruning (Archor) - max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef036c6",
   "metadata": {},
   "source": [
    "#### CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a09eb",
   "metadata": {},
   "source": [
    "##### ~50% filters pruned (512*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8cafce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 36, 3: 44, 7: 82, 10: 75, 14: 145, 17: 145, 20: 143, 24: 173, 27: 180, 30: 208, 34: 205, 37: 191, 40: 421}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(28, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(20, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(46, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(53, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(111, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(111, 113, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(113, 339, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(339, 332, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(332, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(304, 307, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(307, 321, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(321, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 391/391 | Loss: 0.936 | Acc: 69.218%\n",
      "[=================================================>] 79/79 | Loss: 0.966 | Acc: 68.820%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 391/391 | Loss: 0.631 | Acc: 78.926%\n",
      "[=================================================>] 79/79 | Loss: 0.700 | Acc: 76.690%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 391/391 | Loss: 0.542 | Acc: 81.728%\n",
      "[=================================================>] 79/79 | Loss: 0.589 | Acc: 80.700%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 391/391 | Loss: 0.479 | Acc: 84.096%\n",
      "[=================================================>] 79/79 | Loss: 0.495 | Acc: 83.250%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 391/391 | Loss: 0.441 | Acc: 85.070%\n",
      "[=================================================>] 79/79 | Loss: 0.534 | Acc: 81.650%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 391/391 | Loss: 0.408 | Acc: 86.434%\n",
      "[=================================================>] 79/79 | Loss: 0.521 | Acc: 82.820%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 391/391 | Loss: 0.380 | Acc: 87.316%\n",
      "[=================================================>] 79/79 | Loss: 0.540 | Acc: 82.730%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 391/391 | Loss: 0.357 | Acc: 88.028%\n",
      "[=================================================>] 79/79 | Loss: 0.484 | Acc: 84.000%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 391/391 | Loss: 0.345 | Acc: 88.294%\n",
      "[=================================================>] 79/79 | Loss: 0.444 | Acc: 84.850%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 391/391 | Loss: 0.318 | Acc: 89.240%\n",
      "[=================================================>] 79/79 | Loss: 0.426 | Acc: 86.130%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 391/391 | Loss: 0.310 | Acc: 89.604%\n",
      "[=================================================>] 79/79 | Loss: 0.495 | Acc: 83.620%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 391/391 | Loss: 0.296 | Acc: 89.994%\n",
      "[=================================================>] 79/79 | Loss: 0.433 | Acc: 86.120%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 391/391 | Loss: 0.289 | Acc: 90.236%\n",
      "[=================================================>] 79/79 | Loss: 0.441 | Acc: 85.340%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 391/391 | Loss: 0.276 | Acc: 90.514%\n",
      "[=================================================>] 79/79 | Loss: 0.421 | Acc: 86.160%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 391/391 | Loss: 0.261 | Acc: 91.082%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 87.000%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 391/391 | Loss: 0.256 | Acc: 91.220%\n",
      "[=================================================>] 79/79 | Loss: 0.444 | Acc: 85.760%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 391/391 | Loss: 0.250 | Acc: 91.478%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 86.400%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 391/391 | Loss: 0.239 | Acc: 91.970%\n",
      "[=================================================>] 79/79 | Loss: 0.433 | Acc: 86.370%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 391/391 | Loss: 0.233 | Acc: 92.148%\n",
      "[=================================================>] 79/79 | Loss: 0.461 | Acc: 85.400%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 391/391 | Loss: 0.227 | Acc: 92.290%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 87.480%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 391/391 | Loss: 0.218 | Acc: 92.660%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 87.680%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 391/391 | Loss: 0.211 | Acc: 92.682%\n",
      "[=================================================>] 79/79 | Loss: 0.442 | Acc: 86.350%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 391/391 | Loss: 0.209 | Acc: 92.864%\n",
      "[=================================================>] 79/79 | Loss: 0.405 | Acc: 87.170%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 391/391 | Loss: 0.207 | Acc: 93.062%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 86.940%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 391/391 | Loss: 0.198 | Acc: 93.362%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 87.750%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 391/391 | Loss: 0.196 | Acc: 93.258%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 87.590%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 391/391 | Loss: 0.190 | Acc: 93.580%\n",
      "[=================================================>] 79/79 | Loss: 0.447 | Acc: 86.300%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 391/391 | Loss: 0.189 | Acc: 93.536%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 87.550%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 391/391 | Loss: 0.182 | Acc: 93.900%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 87.770%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 391/391 | Loss: 0.179 | Acc: 93.982%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 87.390%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 391/391 | Loss: 0.172 | Acc: 94.108%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 88.420%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 391/391 | Loss: 0.171 | Acc: 94.042%\n",
      "[=================================================>] 79/79 | Loss: 0.407 | Acc: 87.500%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 391/391 | Loss: 0.164 | Acc: 94.386%\n",
      "[=================================================>] 79/79 | Loss: 0.423 | Acc: 87.420%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 391/391 | Loss: 0.159 | Acc: 94.416%\n",
      "[=================================================>] 79/79 | Loss: 0.435 | Acc: 86.820%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 391/391 | Loss: 0.156 | Acc: 94.630%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 88.290%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 391/391 | Loss: 0.156 | Acc: 94.646%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 89.060%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 391/391 | Loss: 0.152 | Acc: 94.876%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 87.580%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 391/391 | Loss: 0.151 | Acc: 94.826%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 88.460%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 391/391 | Loss: 0.148 | Acc: 94.844%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 88.110%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 391/391 | Loss: 0.144 | Acc: 95.058%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 87.750%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 391/391 | Loss: 0.140 | Acc: 95.274%\n",
      "[=================================================>] 79/79 | Loss: 0.440 | Acc: 87.310%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 391/391 | Loss: 0.138 | Acc: 95.388%\n",
      "[=================================================>] 79/79 | Loss: 0.540 | Acc: 85.030%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 391/391 | Loss: 0.136 | Acc: 95.382%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 88.110%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 391/391 | Loss: 0.135 | Acc: 95.384%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 88.090%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 391/391 | Loss: 0.128 | Acc: 95.552%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 88.550%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 391/391 | Loss: 0.127 | Acc: 95.660%\n",
      "[=================================================>] 79/79 | Loss: 0.429 | Acc: 87.620%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 391/391 | Loss: 0.126 | Acc: 95.680%\n",
      "[=================================================>] 79/79 | Loss: 0.428 | Acc: 87.610%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 391/391 | Loss: 0.120 | Acc: 95.974%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 88.520%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 391/391 | Loss: 0.121 | Acc: 95.876%\n",
      "[=================================================>] 79/79 | Loss: 0.405 | Acc: 88.160%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 391/391 | Loss: 0.114 | Acc: 96.134%\n",
      "[=================================================>] 79/79 | Loss: 0.405 | Acc: 88.310%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 391/391 | Loss: 0.120 | Acc: 95.956%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 88.200%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 391/391 | Loss: 0.112 | Acc: 96.184%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 88.650%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 391/391 | Loss: 0.112 | Acc: 96.052%\n",
      "[=================================================>] 79/79 | Loss: 0.433 | Acc: 87.740%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 391/391 | Loss: 0.106 | Acc: 96.358%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 88.980%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 391/391 | Loss: 0.103 | Acc: 96.566%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 88.700%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 391/391 | Loss: 0.102 | Acc: 96.534%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 88.860%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 391/391 | Loss: 0.104 | Acc: 96.464%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 88.410%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 391/391 | Loss: 0.105 | Acc: 96.372%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 88.140%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 391/391 | Loss: 0.095 | Acc: 96.730%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 89.200%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 391/391 | Loss: 0.090 | Acc: 96.930%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 89.170%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 391/391 | Loss: 0.092 | Acc: 96.866%\n",
      "[=================================================>] 79/79 | Loss: 0.409 | Acc: 89.000%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 391/391 | Loss: 0.092 | Acc: 96.848%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 88.840%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 391/391 | Loss: 0.091 | Acc: 96.934%\n",
      "[=================================================>] 79/79 | Loss: 0.429 | Acc: 88.490%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 391/391 | Loss: 0.087 | Acc: 97.032%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 88.800%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.190%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 88.550%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.180%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 89.260%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.210%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 89.260%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 391/391 | Loss: 0.082 | Acc: 97.280%\n",
      "[=================================================>] 79/79 | Loss: 0.407 | Acc: 88.880%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 391/391 | Loss: 0.083 | Acc: 97.232%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 89.170%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 391/391 | Loss: 0.074 | Acc: 97.464%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 89.260%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 391/391 | Loss: 0.069 | Acc: 97.702%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 89.940%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.532%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 89.570%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 391/391 | Loss: 0.068 | Acc: 97.666%\n",
      "[=================================================>] 79/79 | Loss: 0.402 | Acc: 89.630%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 391/391 | Loss: 0.068 | Acc: 97.688%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 89.690%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 391/391 | Loss: 0.067 | Acc: 97.724%\n",
      "[=================================================>] 79/79 | Loss: 0.387 | Acc: 89.620%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 391/391 | Loss: 0.060 | Acc: 97.924%\n",
      "[=================================================>] 79/79 | Loss: 0.425 | Acc: 89.090%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 391/391 | Loss: 0.064 | Acc: 97.864%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 89.370%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 391/391 | Loss: 0.059 | Acc: 97.994%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 89.730%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.094%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 89.580%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.108%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 89.620%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 391/391 | Loss: 0.056 | Acc: 98.124%\n",
      "[=================================================>] 79/79 | Loss: 0.391 | Acc: 89.560%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.288%\n",
      "[=================================================>] 79/79 | Loss: 0.452 | Acc: 89.050%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 391/391 | Loss: 0.049 | Acc: 98.352%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 90.040%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 391/391 | Loss: 0.050 | Acc: 98.282%\n",
      "[=================================================>] 79/79 | Loss: 0.420 | Acc: 89.270%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.442%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 90.090%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.354%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 90.260%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 391/391 | Loss: 0.043 | Acc: 98.562%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 90.050%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 391/391 | Loss: 0.045 | Acc: 98.466%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 89.940%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.658%\n",
      "[=================================================>] 79/79 | Loss: 0.403 | Acc: 90.170%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 391/391 | Loss: 0.038 | Acc: 98.678%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 89.900%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 391/391 | Loss: 0.037 | Acc: 98.780%\n",
      "[=================================================>] 79/79 | Loss: 0.429 | Acc: 89.990%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.818%\n",
      "[=================================================>] 79/79 | Loss: 0.425 | Acc: 90.060%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.906%\n",
      "[=================================================>] 79/79 | Loss: 0.428 | Acc: 89.850%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.842%\n",
      "[=================================================>] 79/79 | Loss: 0.409 | Acc: 90.070%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 391/391 | Loss: 0.032 | Acc: 98.950%\n",
      "[=================================================>] 79/79 | Loss: 0.434 | Acc: 90.040%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 391/391 | Loss: 0.029 | Acc: 99.038%\n",
      "[=================================================>] 79/79 | Loss: 0.418 | Acc: 90.210%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 391/391 | Loss: 0.026 | Acc: 99.138%\n",
      "[=================================================>] 79/79 | Loss: 0.424 | Acc: 90.130%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 391/391 | Loss: 0.024 | Acc: 99.164%\n",
      "[=================================================>] 79/79 | Loss: 0.432 | Acc: 90.230%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 391/391 | Loss: 0.025 | Acc: 99.170%\n",
      "[=================================================>] 79/79 | Loss: 0.427 | Acc: 90.190%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.250%\n",
      "[=================================================>] 79/79 | Loss: 0.431 | Acc: 90.220%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 391/391 | Loss: 0.023 | Acc: 99.252%\n",
      "[=================================================>] 79/79 | Loss: 0.422 | Acc: 90.440%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.288%\n",
      "[=================================================>] 79/79 | Loss: 0.423 | Acc: 90.560%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.434%\n",
      "[=================================================>] 79/79 | Loss: 0.443 | Acc: 90.110%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 391/391 | Loss: 0.019 | Acc: 99.376%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 90.680%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.396%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 90.430%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.524%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 90.650%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.518%\n",
      "[=================================================>] 79/79 | Loss: 0.426 | Acc: 90.560%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.576%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 90.870%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.600%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 90.840%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.652%\n",
      "[=================================================>] 79/79 | Loss: 0.418 | Acc: 90.900%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.550%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 90.610%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.668%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 90.700%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.732%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 90.850%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.742%\n",
      "[=================================================>] 79/79 | Loss: 0.418 | Acc: 90.850%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.718%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 90.860%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 391/391 | Loss: 0.009 | Acc: 99.750%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 91.090%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.764%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 91.130%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.824%\n",
      "[=================================================>] 79/79 | Loss: 0.418 | Acc: 90.990%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.818%\n",
      "[=================================================>] 79/79 | Loss: 0.421 | Acc: 91.110%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.834%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 91.100%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.830%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.340%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.858%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 90.990%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.844%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 91.280%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.898%\n",
      "[=================================================>] 79/79 | Loss: 0.421 | Acc: 91.060%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.924%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 91.150%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.888%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 91.310%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.914%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 91.140%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.900%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.190%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.914%\n",
      "[=================================================>] 79/79 | Loss: 0.416 | Acc: 91.180%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.882%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 91.370%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.900%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 91.300%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 91.380%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 91.340%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.926%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.260%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.910%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 91.400%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.540%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.946%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.350%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.340%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.912%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 91.370%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.936%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 91.320%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.926%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 91.410%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 91.420%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.390%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.920%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.330%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 91.400%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.940%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 91.410%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.934%\n",
      "[=================================================>] 79/79 | Loss: 0.414 | Acc: 91.320%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.934%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.390%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.930%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.370%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.936%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 91.360%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 33.65M => 21.77M (35.3% reduction)\n",
      "MACs: 0.33G => 0.09G (72.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'max'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 4\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_max_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc749e38",
   "metadata": {},
   "source": [
    "##### ~36% filters pruned (512*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e0dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 35, 3: 40, 7: 79, 10: 80, 14: 115, 17: 117, 20: 112, 24: 150, 27: 132, 30: 157, 34: 124, 37: 139, 40: 256}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(29, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(24, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(49, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(48, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(141, 139, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(139, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(144, 362, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(362, 380, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(380, 355, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(355, 388, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(388, 373, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(373, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 391/391 | Loss: 0.828 | Acc: 72.908%\n",
      "[=================================================>] 79/79 | Loss: 1.018 | Acc: 68.810%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 391/391 | Loss: 0.577 | Acc: 80.522%\n",
      "[=================================================>] 79/79 | Loss: 0.616 | Acc: 79.390%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 391/391 | Loss: 0.479 | Acc: 83.872%\n",
      "[=================================================>] 79/79 | Loss: 0.533 | Acc: 82.160%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 391/391 | Loss: 0.430 | Acc: 85.740%\n",
      "[=================================================>] 79/79 | Loss: 0.491 | Acc: 84.120%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 391/391 | Loss: 0.387 | Acc: 87.024%\n",
      "[=================================================>] 79/79 | Loss: 0.486 | Acc: 83.870%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 391/391 | Loss: 0.360 | Acc: 88.024%\n",
      "[=================================================>] 79/79 | Loss: 0.505 | Acc: 83.740%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 391/391 | Loss: 0.336 | Acc: 88.780%\n",
      "[=================================================>] 79/79 | Loss: 0.489 | Acc: 84.080%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 391/391 | Loss: 0.315 | Acc: 89.380%\n",
      "[=================================================>] 79/79 | Loss: 0.444 | Acc: 85.200%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 391/391 | Loss: 0.301 | Acc: 89.854%\n",
      "[=================================================>] 79/79 | Loss: 0.505 | Acc: 84.490%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 391/391 | Loss: 0.285 | Acc: 90.364%\n",
      "[=================================================>] 79/79 | Loss: 0.430 | Acc: 86.140%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 391/391 | Loss: 0.275 | Acc: 90.636%\n",
      "[=================================================>] 79/79 | Loss: 0.420 | Acc: 86.380%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 391/391 | Loss: 0.257 | Acc: 91.296%\n",
      "[=================================================>] 79/79 | Loss: 0.448 | Acc: 85.210%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 391/391 | Loss: 0.243 | Acc: 91.712%\n",
      "[=================================================>] 79/79 | Loss: 0.431 | Acc: 86.460%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 391/391 | Loss: 0.240 | Acc: 92.084%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 87.080%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 391/391 | Loss: 0.229 | Acc: 92.158%\n",
      "[=================================================>] 79/79 | Loss: 0.440 | Acc: 86.400%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 391/391 | Loss: 0.219 | Acc: 92.702%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 87.610%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 391/391 | Loss: 0.214 | Acc: 92.680%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 87.300%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 391/391 | Loss: 0.208 | Acc: 92.974%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 87.860%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 391/391 | Loss: 0.201 | Acc: 93.012%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 87.050%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 391/391 | Loss: 0.194 | Acc: 93.388%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 87.950%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 391/391 | Loss: 0.188 | Acc: 93.608%\n",
      "[=================================================>] 79/79 | Loss: 0.461 | Acc: 85.840%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 391/391 | Loss: 0.177 | Acc: 94.000%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 87.620%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 391/391 | Loss: 0.181 | Acc: 93.834%\n",
      "[=================================================>] 79/79 | Loss: 0.361 | Acc: 89.000%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 391/391 | Loss: 0.171 | Acc: 94.098%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 87.830%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 391/391 | Loss: 0.169 | Acc: 94.234%\n",
      "[=================================================>] 79/79 | Loss: 0.398 | Acc: 87.810%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 391/391 | Loss: 0.164 | Acc: 94.452%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 87.640%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 391/391 | Loss: 0.161 | Acc: 94.578%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 88.410%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 391/391 | Loss: 0.158 | Acc: 94.660%\n",
      "[=================================================>] 79/79 | Loss: 0.407 | Acc: 87.640%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 391/391 | Loss: 0.154 | Acc: 94.698%\n",
      "[=================================================>] 79/79 | Loss: 0.443 | Acc: 87.090%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 391/391 | Loss: 0.149 | Acc: 94.882%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 88.080%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 391/391 | Loss: 0.143 | Acc: 95.130%\n",
      "[=================================================>] 79/79 | Loss: 0.426 | Acc: 88.090%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 391/391 | Loss: 0.140 | Acc: 95.134%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 88.680%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 391/391 | Loss: 0.139 | Acc: 95.224%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 87.720%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 391/391 | Loss: 0.141 | Acc: 95.162%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 88.510%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 391/391 | Loss: 0.136 | Acc: 95.396%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 88.390%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 391/391 | Loss: 0.130 | Acc: 95.508%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 88.800%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 391/391 | Loss: 0.129 | Acc: 95.584%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 88.710%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 391/391 | Loss: 0.125 | Acc: 95.778%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 89.270%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 391/391 | Loss: 0.120 | Acc: 95.890%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 88.570%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 391/391 | Loss: 0.120 | Acc: 95.996%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 89.490%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 391/391 | Loss: 0.121 | Acc: 95.862%\n",
      "[=================================================>] 79/79 | Loss: 0.362 | Acc: 88.850%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 391/391 | Loss: 0.118 | Acc: 95.946%\n",
      "[=================================================>] 79/79 | Loss: 0.413 | Acc: 88.250%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 391/391 | Loss: 0.114 | Acc: 96.114%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 88.390%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 391/391 | Loss: 0.109 | Acc: 96.242%\n",
      "[=================================================>] 79/79 | Loss: 0.430 | Acc: 87.750%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 391/391 | Loss: 0.108 | Acc: 96.384%\n",
      "[=================================================>] 79/79 | Loss: 0.382 | Acc: 89.080%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 391/391 | Loss: 0.105 | Acc: 96.458%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 89.170%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 391/391 | Loss: 0.103 | Acc: 96.426%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 88.590%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 391/391 | Loss: 0.104 | Acc: 96.452%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 89.040%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 391/391 | Loss: 0.098 | Acc: 96.684%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 88.940%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 391/391 | Loss: 0.098 | Acc: 96.716%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 88.560%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 391/391 | Loss: 0.099 | Acc: 96.540%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 89.260%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 391/391 | Loss: 0.098 | Acc: 96.606%\n",
      "[=================================================>] 79/79 | Loss: 0.363 | Acc: 90.000%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 391/391 | Loss: 0.094 | Acc: 96.776%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 89.490%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 391/391 | Loss: 0.090 | Acc: 97.066%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 89.350%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 391/391 | Loss: 0.087 | Acc: 97.128%\n",
      "[=================================================>] 79/79 | Loss: 0.410 | Acc: 88.940%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 391/391 | Loss: 0.088 | Acc: 96.984%\n",
      "[=================================================>] 79/79 | Loss: 0.384 | Acc: 89.590%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.300%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 89.440%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 391/391 | Loss: 0.080 | Acc: 97.232%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 89.190%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 391/391 | Loss: 0.081 | Acc: 97.286%\n",
      "[=================================================>] 79/79 | Loss: 0.417 | Acc: 88.690%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 391/391 | Loss: 0.077 | Acc: 97.384%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 89.720%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 391/391 | Loss: 0.075 | Acc: 97.474%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 89.760%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 391/391 | Loss: 0.080 | Acc: 97.308%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 89.710%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 391/391 | Loss: 0.075 | Acc: 97.500%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 89.760%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 391/391 | Loss: 0.071 | Acc: 97.592%\n",
      "[=================================================>] 79/79 | Loss: 0.373 | Acc: 89.850%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 391/391 | Loss: 0.070 | Acc: 97.572%\n",
      "[=================================================>] 79/79 | Loss: 0.418 | Acc: 88.800%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 391/391 | Loss: 0.068 | Acc: 97.700%\n",
      "[=================================================>] 79/79 | Loss: 0.372 | Acc: 89.930%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 391/391 | Loss: 0.067 | Acc: 97.654%\n",
      "[=================================================>] 79/79 | Loss: 0.409 | Acc: 89.430%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 391/391 | Loss: 0.062 | Acc: 97.926%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 89.540%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 391/391 | Loss: 0.065 | Acc: 97.784%\n",
      "[=================================================>] 79/79 | Loss: 0.398 | Acc: 89.540%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 391/391 | Loss: 0.062 | Acc: 97.900%\n",
      "[=================================================>] 79/79 | Loss: 0.419 | Acc: 89.450%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 391/391 | Loss: 0.063 | Acc: 97.860%\n",
      "[=================================================>] 79/79 | Loss: 0.377 | Acc: 90.080%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 391/391 | Loss: 0.058 | Acc: 98.028%\n",
      "[=================================================>] 79/79 | Loss: 0.379 | Acc: 90.510%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 391/391 | Loss: 0.057 | Acc: 98.112%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 90.230%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 391/391 | Loss: 0.050 | Acc: 98.350%\n",
      "[=================================================>] 79/79 | Loss: 0.421 | Acc: 89.550%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 391/391 | Loss: 0.055 | Acc: 98.202%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 90.020%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 391/391 | Loss: 0.052 | Acc: 98.294%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 90.200%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 391/391 | Loss: 0.047 | Acc: 98.384%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 90.460%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 391/391 | Loss: 0.051 | Acc: 98.284%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 90.220%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.522%\n",
      "[=================================================>] 79/79 | Loss: 0.386 | Acc: 90.360%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 391/391 | Loss: 0.046 | Acc: 98.456%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 90.050%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.584%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 90.270%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 391/391 | Loss: 0.042 | Acc: 98.594%\n",
      "[=================================================>] 79/79 | Loss: 0.415 | Acc: 90.180%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.702%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 90.720%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 391/391 | Loss: 0.039 | Acc: 98.756%\n",
      "[=================================================>] 79/79 | Loss: 0.408 | Acc: 90.170%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 391/391 | Loss: 0.035 | Acc: 98.856%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 90.310%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.824%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 90.570%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 391/391 | Loss: 0.035 | Acc: 98.834%\n",
      "[=================================================>] 79/79 | Loss: 0.411 | Acc: 90.540%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 391/391 | Loss: 0.034 | Acc: 98.872%\n",
      "[=================================================>] 79/79 | Loss: 0.403 | Acc: 90.490%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 391/391 | Loss: 0.030 | Acc: 99.012%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 90.940%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 391/391 | Loss: 0.030 | Acc: 99.000%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 90.950%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.068%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 90.600%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 391/391 | Loss: 0.028 | Acc: 99.044%\n",
      "[=================================================>] 79/79 | Loss: 0.381 | Acc: 91.000%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 391/391 | Loss: 0.025 | Acc: 99.162%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 90.960%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 391/391 | Loss: 0.025 | Acc: 99.188%\n",
      "[=================================================>] 79/79 | Loss: 0.406 | Acc: 90.830%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 391/391 | Loss: 0.020 | Acc: 99.324%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 91.060%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 391/391 | Loss: 0.021 | Acc: 99.294%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 91.010%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 391/391 | Loss: 0.020 | Acc: 99.298%\n",
      "[=================================================>] 79/79 | Loss: 0.421 | Acc: 90.720%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.366%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 91.340%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 391/391 | Loss: 0.019 | Acc: 99.384%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 91.110%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 391/391 | Loss: 0.018 | Acc: 99.392%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 91.210%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 391/391 | Loss: 0.014 | Acc: 99.576%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 91.230%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 391/391 | Loss: 0.015 | Acc: 99.544%\n",
      "[=================================================>] 79/79 | Loss: 0.410 | Acc: 91.070%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.590%\n",
      "[=================================================>] 79/79 | Loss: 0.412 | Acc: 91.040%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 391/391 | Loss: 0.013 | Acc: 99.584%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 91.550%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.654%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 91.180%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.674%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 91.170%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.656%\n",
      "[=================================================>] 79/79 | Loss: 0.399 | Acc: 91.470%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.636%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 91.470%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 391/391 | Loss: 0.011 | Acc: 99.674%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 91.500%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 391/391 | Loss: 0.010 | Acc: 99.674%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 91.730%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 391/391 | Loss: 0.008 | Acc: 99.736%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 91.600%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 391/391 | Loss: 0.007 | Acc: 99.786%\n",
      "[=================================================>] 79/79 | Loss: 0.385 | Acc: 91.610%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.822%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 91.810%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 391/391 | Loss: 0.006 | Acc: 99.808%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 91.570%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.868%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 91.670%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.876%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 91.650%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.866%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 91.710%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.918%\n",
      "[=================================================>] 79/79 | Loss: 0.404 | Acc: 91.580%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 391/391 | Loss: 0.005 | Acc: 99.868%\n",
      "[=================================================>] 79/79 | Loss: 0.401 | Acc: 91.750%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.896%\n",
      "[=================================================>] 79/79 | Loss: 0.400 | Acc: 91.880%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.918%\n",
      "[=================================================>] 79/79 | Loss: 0.397 | Acc: 91.790%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.896%\n",
      "[=================================================>] 79/79 | Loss: 0.396 | Acc: 91.840%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 391/391 | Loss: 0.004 | Acc: 99.900%\n",
      "[=================================================>] 79/79 | Loss: 0.398 | Acc: 91.970%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.920%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 91.930%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.924%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 92.010%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.956%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 91.860%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.932%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 91.970%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.948%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 91.990%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 91.940%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 391/391 | Loss: 0.003 | Acc: 99.942%\n",
      "[=================================================>] 79/79 | Loss: 0.395 | Acc: 91.970%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.944%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 91.910%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.944%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 91.940%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.950%\n",
      "[=================================================>] 79/79 | Loss: 0.394 | Acc: 91.900%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 91.920%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.946%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 91.930%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 0.393 | Acc: 92.000%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 92.040%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 0.392 | Acc: 92.000%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.952%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 91.980%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.954%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 92.090%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.391 | Acc: 92.130%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.966%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 92.110%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 92.190%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 92.160%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 92.160%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.962%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 92.080%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.958%\n",
      "[=================================================>] 79/79 | Loss: 0.390 | Acc: 92.110%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.964%\n",
      "[=================================================>] 79/79 | Loss: 0.389 | Acc: 92.100%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 92.140%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 391/391 | Loss: 0.002 | Acc: 99.960%\n",
      "[=================================================>] 79/79 | Loss: 0.388 | Acc: 92.070%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 33.65M => 24.66M (26.7% reduction)\n",
      "MACs: 0.33G => 0.12G (63.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'max'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 3\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_max_30_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79726763",
   "metadata": {},
   "source": [
    "##### ~72% filters pruned (512*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bcd0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 183 with best acc = 93.6200\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 33.65M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 79/79 | Loss: 0.313 | Acc: 93.620%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 49, 3: 54, 7: 107, 10: 106, 14: 198, 17: 193, 20: 194, 24: 333, 27: 339, 30: 355, 34: 291, 37: 341, 40: 512}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(15, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(10, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(22, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(58, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(63, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(62, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(179, 173, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(173, 157, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(157, 221, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(221, 171, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(171, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 171, 3, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\u001b[39;00m\n\u001b[1;32m     35\u001b[0m pruner \u001b[38;5;241m=\u001b[39m VGG16Pruner(config, model, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpruned_max_70_ckpt.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mpruner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_filters_to_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_filters_to_prune\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_after_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_after_prune\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 894\u001b[0m, in \u001b[0;36mVGG16Pruner.prune\u001b[0;34m(self, num_filters_to_prune, epochs_after_prune)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_after_prune):\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs_after_prune\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(epoch)\n\u001b[1;32m    896\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[10], line 708\u001b[0m, in \u001b[0;36mVGG16Pruner.train_epoch\u001b[0;34m(self, optimizer, rank_filters)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 708\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, targets)\n\u001b[1;32m    710\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 491\u001b[0m, in \u001b[0;36mModifiedVGG16Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 491\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    493\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 171, 3, 3] instead"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'max'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "\n",
    "num_filters_to_prune = 512 * 6\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar10_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='pruned_max_70_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da7d93",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271ae54",
   "metadata": {},
   "source": [
    "##### ~50% filters pruned (512*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c65243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.780%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 34, 3: 44, 7: 83, 10: 71, 14: 139, 17: 124, 20: 136, 24: 207, 27: 200, 30: 201, 34: 195, 37: 204, 40: 410}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(30, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(20, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(45, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(57, 117, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(117, 132, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(132, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(120, 305, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(305, 312, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(312, 311, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(311, 317, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(317, 308, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(308, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 2.896 | Acc: 27.998%\n",
      "[=================================================>] 157/157 | Loss: 2.417 | Acc: 38.290%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 2.157 | Acc: 43.340%\n",
      "[=================================================>] 157/157 | Loss: 2.205 | Acc: 42.710%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 1.915 | Acc: 48.946%\n",
      "[=================================================>] 157/157 | Loss: 2.054 | Acc: 45.650%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 1.769 | Acc: 52.290%\n",
      "[=================================================>] 157/157 | Loss: 2.127 | Acc: 46.750%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 1.657 | Acc: 55.062%\n",
      "[=================================================>] 157/157 | Loss: 1.738 | Acc: 54.090%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 1.571 | Acc: 57.164%\n",
      "[=================================================>] 157/157 | Loss: 1.834 | Acc: 52.240%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 1.499 | Acc: 58.844%\n",
      "[=================================================>] 157/157 | Loss: 1.743 | Acc: 54.270%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 1.429 | Acc: 60.628%\n",
      "[=================================================>] 157/157 | Loss: 1.851 | Acc: 51.850%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 1.389 | Acc: 61.622%\n",
      "[=================================================>] 157/157 | Loss: 1.754 | Acc: 54.200%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 1.333 | Acc: 62.838%\n",
      "[=================================================>] 157/157 | Loss: 1.737 | Acc: 54.740%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 1.296 | Acc: 63.784%\n",
      "[=================================================>] 157/157 | Loss: 1.658 | Acc: 56.400%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 1.257 | Acc: 64.906%\n",
      "[=================================================>] 157/157 | Loss: 1.600 | Acc: 58.280%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 1.226 | Acc: 65.540%\n",
      "[=================================================>] 157/157 | Loss: 1.714 | Acc: 55.600%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 1.195 | Acc: 66.268%\n",
      "[=================================================>] 157/157 | Loss: 1.582 | Acc: 58.240%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 1.159 | Acc: 67.184%\n",
      "[=================================================>] 157/157 | Loss: 1.636 | Acc: 57.670%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 1.131 | Acc: 67.872%\n",
      "[=================================================>] 157/157 | Loss: 1.623 | Acc: 56.920%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 1.113 | Acc: 68.538%\n",
      "[=================================================>] 157/157 | Loss: 1.600 | Acc: 58.380%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 1.090 | Acc: 69.006%\n",
      "[=================================================>] 157/157 | Loss: 1.587 | Acc: 59.170%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 1.067 | Acc: 69.688%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 59.210%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 1.040 | Acc: 70.220%\n",
      "[=================================================>] 157/157 | Loss: 1.598 | Acc: 58.750%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 1.024 | Acc: 70.708%\n",
      "[=================================================>] 157/157 | Loss: 1.596 | Acc: 58.850%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 1.006 | Acc: 71.156%\n",
      "[=================================================>] 157/157 | Loss: 1.547 | Acc: 60.470%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 0.982 | Acc: 71.798%\n",
      "[=================================================>] 157/157 | Loss: 1.628 | Acc: 58.700%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 0.961 | Acc: 72.534%\n",
      "[=================================================>] 157/157 | Loss: 1.563 | Acc: 60.090%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 0.952 | Acc: 72.400%\n",
      "[=================================================>] 157/157 | Loss: 1.541 | Acc: 60.760%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 0.932 | Acc: 73.290%\n",
      "[=================================================>] 157/157 | Loss: 1.609 | Acc: 59.150%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 0.917 | Acc: 73.498%\n",
      "[=================================================>] 157/157 | Loss: 1.603 | Acc: 59.880%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 0.902 | Acc: 73.742%\n",
      "[=================================================>] 157/157 | Loss: 1.511 | Acc: 61.280%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 0.883 | Acc: 74.474%\n",
      "[=================================================>] 157/157 | Loss: 1.529 | Acc: 61.980%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 0.877 | Acc: 74.650%\n",
      "[=================================================>] 157/157 | Loss: 1.538 | Acc: 61.500%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 0.864 | Acc: 74.926%\n",
      "[=================================================>] 157/157 | Loss: 1.655 | Acc: 59.580%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 0.843 | Acc: 75.504%\n",
      "[=================================================>] 157/157 | Loss: 1.567 | Acc: 61.250%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 0.824 | Acc: 75.826%\n",
      "[=================================================>] 157/157 | Loss: 1.622 | Acc: 60.430%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 0.817 | Acc: 76.006%\n",
      "[=================================================>] 157/157 | Loss: 1.486 | Acc: 62.820%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 0.798 | Acc: 76.544%\n",
      "[=================================================>] 157/157 | Loss: 1.579 | Acc: 61.190%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 0.794 | Acc: 76.694%\n",
      "[=================================================>] 157/157 | Loss: 1.537 | Acc: 61.840%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 0.777 | Acc: 77.182%\n",
      "[=================================================>] 157/157 | Loss: 1.533 | Acc: 62.080%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 0.765 | Acc: 77.496%\n",
      "[=================================================>] 157/157 | Loss: 1.554 | Acc: 62.050%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 0.754 | Acc: 78.050%\n",
      "[=================================================>] 157/157 | Loss: 1.509 | Acc: 62.140%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 0.728 | Acc: 78.382%\n",
      "[=================================================>] 157/157 | Loss: 1.651 | Acc: 59.850%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 0.728 | Acc: 78.720%\n",
      "[=================================================>] 157/157 | Loss: 1.587 | Acc: 61.870%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 0.711 | Acc: 79.138%\n",
      "[=================================================>] 157/157 | Loss: 1.557 | Acc: 61.980%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 0.710 | Acc: 78.994%\n",
      "[=================================================>] 157/157 | Loss: 1.585 | Acc: 61.930%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 0.692 | Acc: 79.642%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 61.180%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 0.687 | Acc: 79.470%\n",
      "[=================================================>] 157/157 | Loss: 1.601 | Acc: 61.430%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 0.670 | Acc: 80.226%\n",
      "[=================================================>] 157/157 | Loss: 1.633 | Acc: 61.050%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 0.657 | Acc: 80.518%\n",
      "[=================================================>] 157/157 | Loss: 1.639 | Acc: 61.090%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 0.641 | Acc: 80.950%\n",
      "[=================================================>] 157/157 | Loss: 1.626 | Acc: 61.690%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 0.639 | Acc: 81.102%\n",
      "[=================================================>] 157/157 | Loss: 1.740 | Acc: 58.820%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 0.631 | Acc: 81.298%\n",
      "[=================================================>] 157/157 | Loss: 1.564 | Acc: 62.850%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 0.614 | Acc: 81.618%\n",
      "[=================================================>] 157/157 | Loss: 1.546 | Acc: 63.330%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 0.597 | Acc: 82.174%\n",
      "[=================================================>] 157/157 | Loss: 1.533 | Acc: 63.100%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 0.594 | Acc: 82.148%\n",
      "[=================================================>] 157/157 | Loss: 1.572 | Acc: 62.680%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 0.574 | Acc: 82.688%\n",
      "[=================================================>] 157/157 | Loss: 1.574 | Acc: 63.040%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 0.571 | Acc: 82.966%\n",
      "[=================================================>] 157/157 | Loss: 1.622 | Acc: 62.380%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 0.554 | Acc: 83.386%\n",
      "[=================================================>] 157/157 | Loss: 1.580 | Acc: 62.530%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 0.548 | Acc: 83.392%\n",
      "[=================================================>] 157/157 | Loss: 1.635 | Acc: 62.320%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 0.529 | Acc: 84.200%\n",
      "[=================================================>] 157/157 | Loss: 1.599 | Acc: 62.720%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 0.529 | Acc: 84.100%\n",
      "[=================================================>] 157/157 | Loss: 1.684 | Acc: 61.900%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 0.509 | Acc: 84.694%\n",
      "[=================================================>] 157/157 | Loss: 1.678 | Acc: 62.210%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 0.498 | Acc: 84.850%\n",
      "[=================================================>] 157/157 | Loss: 1.764 | Acc: 61.590%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 0.492 | Acc: 85.122%\n",
      "[=================================================>] 157/157 | Loss: 1.596 | Acc: 63.440%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 0.486 | Acc: 85.326%\n",
      "[=================================================>] 157/157 | Loss: 1.714 | Acc: 62.150%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 0.470 | Acc: 85.696%\n",
      "[=================================================>] 157/157 | Loss: 1.631 | Acc: 62.470%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 0.459 | Acc: 86.048%\n",
      "[=================================================>] 157/157 | Loss: 1.688 | Acc: 61.630%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 0.453 | Acc: 86.174%\n",
      "[=================================================>] 157/157 | Loss: 1.620 | Acc: 63.540%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 0.435 | Acc: 86.752%\n",
      "[=================================================>] 157/157 | Loss: 1.601 | Acc: 63.360%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 0.426 | Acc: 87.108%\n",
      "[=================================================>] 157/157 | Loss: 1.643 | Acc: 63.250%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 0.417 | Acc: 87.314%\n",
      "[=================================================>] 157/157 | Loss: 1.636 | Acc: 63.460%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 0.409 | Acc: 87.618%\n",
      "[=================================================>] 157/157 | Loss: 1.609 | Acc: 64.180%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.396 | Acc: 87.912%\n",
      "[=================================================>] 157/157 | Loss: 1.691 | Acc: 62.430%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.389 | Acc: 87.982%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 63.340%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.376 | Acc: 88.528%\n",
      "[=================================================>] 157/157 | Loss: 1.668 | Acc: 63.670%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.364 | Acc: 88.946%\n",
      "[=================================================>] 157/157 | Loss: 1.710 | Acc: 63.720%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.357 | Acc: 88.880%\n",
      "[=================================================>] 157/157 | Loss: 1.694 | Acc: 63.370%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.353 | Acc: 89.294%\n",
      "[=================================================>] 157/157 | Loss: 1.627 | Acc: 64.460%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.338 | Acc: 89.824%\n",
      "[=================================================>] 157/157 | Loss: 1.701 | Acc: 63.330%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.334 | Acc: 89.788%\n",
      "[=================================================>] 157/157 | Loss: 1.695 | Acc: 63.360%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.321 | Acc: 90.194%\n",
      "[=================================================>] 157/157 | Loss: 1.716 | Acc: 63.540%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.313 | Acc: 90.492%\n",
      "[=================================================>] 157/157 | Loss: 1.703 | Acc: 63.490%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.297 | Acc: 90.836%\n",
      "[=================================================>] 157/157 | Loss: 1.713 | Acc: 63.890%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.290 | Acc: 91.232%\n",
      "[=================================================>] 157/157 | Loss: 1.752 | Acc: 63.380%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.282 | Acc: 91.350%\n",
      "[=================================================>] 157/157 | Loss: 1.728 | Acc: 63.660%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.267 | Acc: 91.730%\n",
      "[=================================================>] 157/157 | Loss: 1.704 | Acc: 64.650%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.265 | Acc: 91.792%\n",
      "[=================================================>] 157/157 | Loss: 1.696 | Acc: 64.800%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.254 | Acc: 92.238%\n",
      "[=================================================>] 157/157 | Loss: 1.764 | Acc: 63.770%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.241 | Acc: 92.512%\n",
      "[=================================================>] 157/157 | Loss: 1.727 | Acc: 64.850%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.241 | Acc: 92.636%\n",
      "[=================================================>] 157/157 | Loss: 1.682 | Acc: 65.170%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.228 | Acc: 92.986%\n",
      "[=================================================>] 157/157 | Loss: 1.766 | Acc: 63.940%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.216 | Acc: 93.316%\n",
      "[=================================================>] 157/157 | Loss: 1.704 | Acc: 65.600%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.211 | Acc: 93.634%\n",
      "[=================================================>] 157/157 | Loss: 1.754 | Acc: 64.920%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.198 | Acc: 93.832%\n",
      "[=================================================>] 157/157 | Loss: 1.753 | Acc: 64.510%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.193 | Acc: 93.998%\n",
      "[=================================================>] 157/157 | Loss: 1.691 | Acc: 65.240%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.186 | Acc: 94.330%\n",
      "[=================================================>] 157/157 | Loss: 1.729 | Acc: 65.290%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.175 | Acc: 94.680%\n",
      "[=================================================>] 157/157 | Loss: 1.735 | Acc: 65.540%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.169 | Acc: 94.792%\n",
      "[=================================================>] 157/157 | Loss: 1.742 | Acc: 65.670%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.160 | Acc: 95.110%\n",
      "[=================================================>] 157/157 | Loss: 1.732 | Acc: 66.030%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.153 | Acc: 95.416%\n",
      "[=================================================>] 157/157 | Loss: 1.775 | Acc: 65.440%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.147 | Acc: 95.378%\n",
      "[=================================================>] 157/157 | Loss: 1.780 | Acc: 65.480%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.139 | Acc: 95.796%\n",
      "[=================================================>] 157/157 | Loss: 1.769 | Acc: 65.610%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.134 | Acc: 95.956%\n",
      "[=================================================>] 157/157 | Loss: 1.799 | Acc: 65.420%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.127 | Acc: 96.238%\n",
      "[=================================================>] 157/157 | Loss: 1.750 | Acc: 65.990%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.119 | Acc: 96.454%\n",
      "[=================================================>] 157/157 | Loss: 1.788 | Acc: 66.230%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.115 | Acc: 96.426%\n",
      "[=================================================>] 157/157 | Loss: 1.749 | Acc: 66.540%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.106 | Acc: 96.776%\n",
      "[=================================================>] 157/157 | Loss: 1.809 | Acc: 65.720%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.100 | Acc: 97.064%\n",
      "[=================================================>] 157/157 | Loss: 1.761 | Acc: 66.700%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.097 | Acc: 97.090%\n",
      "[=================================================>] 157/157 | Loss: 1.790 | Acc: 66.360%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.091 | Acc: 97.234%\n",
      "[=================================================>] 157/157 | Loss: 1.807 | Acc: 66.050%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.086 | Acc: 97.434%\n",
      "[=================================================>] 157/157 | Loss: 1.784 | Acc: 66.470%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.078 | Acc: 97.644%\n",
      "[=================================================>] 157/157 | Loss: 1.797 | Acc: 66.570%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.073 | Acc: 97.828%\n",
      "[=================================================>] 157/157 | Loss: 1.801 | Acc: 66.350%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.070 | Acc: 97.956%\n",
      "[=================================================>] 157/157 | Loss: 1.794 | Acc: 66.330%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.066 | Acc: 98.048%\n",
      "[=================================================>] 157/157 | Loss: 1.815 | Acc: 66.860%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.058 | Acc: 98.334%\n",
      "[=================================================>] 157/157 | Loss: 1.791 | Acc: 66.760%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.053 | Acc: 98.458%\n",
      "[=================================================>] 157/157 | Loss: 1.778 | Acc: 67.220%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.049 | Acc: 98.602%\n",
      "[=================================================>] 157/157 | Loss: 1.805 | Acc: 67.210%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.045 | Acc: 98.660%\n",
      "[=================================================>] 157/157 | Loss: 1.811 | Acc: 67.230%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.045 | Acc: 98.680%\n",
      "[=================================================>] 157/157 | Loss: 1.785 | Acc: 67.320%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.038 | Acc: 98.966%\n",
      "[=================================================>] 157/157 | Loss: 1.796 | Acc: 67.350%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.038 | Acc: 98.940%\n",
      "[=================================================>] 157/157 | Loss: 1.787 | Acc: 67.720%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.033 | Acc: 99.032%\n",
      "[=================================================>] 157/157 | Loss: 1.779 | Acc: 67.770%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.032 | Acc: 99.098%\n",
      "[=================================================>] 157/157 | Loss: 1.791 | Acc: 67.900%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.029 | Acc: 99.186%\n",
      "[=================================================>] 157/157 | Loss: 1.788 | Acc: 67.950%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.026 | Acc: 99.278%\n",
      "[=================================================>] 157/157 | Loss: 1.791 | Acc: 67.880%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.025 | Acc: 99.300%\n",
      "[=================================================>] 157/157 | Loss: 1.819 | Acc: 67.670%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.023 | Acc: 99.412%\n",
      "[=================================================>] 157/157 | Loss: 1.793 | Acc: 68.000%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.020 | Acc: 99.456%\n",
      "[=================================================>] 157/157 | Loss: 1.768 | Acc: 68.210%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.020 | Acc: 99.488%\n",
      "[=================================================>] 157/157 | Loss: 1.766 | Acc: 68.220%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.558%\n",
      "[=================================================>] 157/157 | Loss: 1.780 | Acc: 67.980%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.017 | Acc: 99.588%\n",
      "[=================================================>] 157/157 | Loss: 1.763 | Acc: 68.420%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.576%\n",
      "[=================================================>] 157/157 | Loss: 1.748 | Acc: 68.220%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.600%\n",
      "[=================================================>] 157/157 | Loss: 1.754 | Acc: 68.260%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.582%\n",
      "[=================================================>] 157/157 | Loss: 1.751 | Acc: 68.340%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.690%\n",
      "[=================================================>] 157/157 | Loss: 1.745 | Acc: 68.170%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.688%\n",
      "[=================================================>] 157/157 | Loss: 1.752 | Acc: 68.380%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.012 | Acc: 99.744%\n",
      "[=================================================>] 157/157 | Loss: 1.736 | Acc: 68.330%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.688%\n",
      "[=================================================>] 157/157 | Loss: 1.753 | Acc: 68.430%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.740%\n",
      "[=================================================>] 157/157 | Loss: 1.745 | Acc: 68.200%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.744%\n",
      "[=================================================>] 157/157 | Loss: 1.752 | Acc: 68.420%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.758%\n",
      "[=================================================>] 157/157 | Loss: 1.751 | Acc: 68.520%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.800%\n",
      "[=================================================>] 157/157 | Loss: 1.737 | Acc: 68.530%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.766%\n",
      "[=================================================>] 157/157 | Loss: 1.749 | Acc: 68.300%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.766%\n",
      "[=================================================>] 157/157 | Loss: 1.751 | Acc: 68.470%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.750%\n",
      "[=================================================>] 157/157 | Loss: 1.749 | Acc: 68.570%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.774%\n",
      "[=================================================>] 157/157 | Loss: 1.744 | Acc: 68.490%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.756%\n",
      "[=================================================>] 157/157 | Loss: 1.730 | Acc: 68.570%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.778%\n",
      "[=================================================>] 157/157 | Loss: 1.740 | Acc: 68.460%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.766%\n",
      "[=================================================>] 157/157 | Loss: 1.739 | Acc: 68.430%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.788%\n",
      "[=================================================>] 157/157 | Loss: 1.740 | Acc: 68.530%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.794%\n",
      "[=================================================>] 157/157 | Loss: 1.735 | Acc: 68.560%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 22.11M (35.0% reduction)\n",
      "MACs: 0.33G => 0.10G (71.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'max'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 4\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_max_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f647b",
   "metadata": {},
   "source": [
    "##### ~36% filters pruned (512*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a5c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.780%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 23, 3: 40, 7: 69, 10: 77, 14: 127, 17: 119, 20: 124, 24: 143, 27: 142, 30: 151, 34: 155, 37: 133, 40: 233}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(41, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(24, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(59, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(51, 129, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(129, 137, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(137, 132, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(132, 369, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(369, 370, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(370, 361, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(361, 357, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(357, 379, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(379, 279, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 2.469 | Acc: 38.386%\n",
      "[=================================================>] 157/157 | Loss: 2.314 | Acc: 42.090%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 1.892 | Acc: 50.108%\n",
      "[=================================================>] 157/157 | Loss: 2.116 | Acc: 46.130%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 1.703 | Acc: 54.330%\n",
      "[=================================================>] 157/157 | Loss: 1.904 | Acc: 51.040%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 1.576 | Acc: 57.216%\n",
      "[=================================================>] 157/157 | Loss: 1.837 | Acc: 52.240%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 1.487 | Acc: 59.408%\n",
      "[=================================================>] 157/157 | Loss: 1.751 | Acc: 54.190%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 1.408 | Acc: 61.166%\n",
      "[=================================================>] 157/157 | Loss: 1.756 | Acc: 54.830%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 1.341 | Acc: 62.720%\n",
      "[=================================================>] 157/157 | Loss: 1.767 | Acc: 55.380%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 1.286 | Acc: 64.222%\n",
      "[=================================================>] 157/157 | Loss: 1.665 | Acc: 57.030%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 1.236 | Acc: 65.352%\n",
      "[=================================================>] 157/157 | Loss: 1.861 | Acc: 53.620%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 1.199 | Acc: 66.468%\n",
      "[=================================================>] 157/157 | Loss: 1.738 | Acc: 56.520%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 1.161 | Acc: 67.354%\n",
      "[=================================================>] 157/157 | Loss: 1.706 | Acc: 55.760%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 1.123 | Acc: 68.296%\n",
      "[=================================================>] 157/157 | Loss: 1.547 | Acc: 59.590%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 1.086 | Acc: 69.106%\n",
      "[=================================================>] 157/157 | Loss: 1.577 | Acc: 59.440%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 1.064 | Acc: 69.858%\n",
      "[=================================================>] 157/157 | Loss: 1.588 | Acc: 59.100%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 1.034 | Acc: 70.418%\n",
      "[=================================================>] 157/157 | Loss: 1.660 | Acc: 57.820%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 0.997 | Acc: 71.520%\n",
      "[=================================================>] 157/157 | Loss: 1.592 | Acc: 59.610%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 0.984 | Acc: 71.958%\n",
      "[=================================================>] 157/157 | Loss: 1.596 | Acc: 59.860%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 0.955 | Acc: 72.520%\n",
      "[=================================================>] 157/157 | Loss: 1.574 | Acc: 60.170%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 0.930 | Acc: 73.364%\n",
      "[=================================================>] 157/157 | Loss: 1.600 | Acc: 59.810%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 0.912 | Acc: 73.816%\n",
      "[=================================================>] 157/157 | Loss: 1.594 | Acc: 60.230%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 0.889 | Acc: 74.388%\n",
      "[=================================================>] 157/157 | Loss: 1.589 | Acc: 60.550%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 0.863 | Acc: 75.062%\n",
      "[=================================================>] 157/157 | Loss: 1.526 | Acc: 61.690%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 0.864 | Acc: 74.974%\n",
      "[=================================================>] 157/157 | Loss: 1.531 | Acc: 61.970%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 0.835 | Acc: 75.738%\n",
      "[=================================================>] 157/157 | Loss: 1.632 | Acc: 59.410%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 0.825 | Acc: 75.900%\n",
      "[=================================================>] 157/157 | Loss: 1.536 | Acc: 62.090%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 0.805 | Acc: 76.486%\n",
      "[=================================================>] 157/157 | Loss: 1.703 | Acc: 59.860%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 0.803 | Acc: 76.490%\n",
      "[=================================================>] 157/157 | Loss: 1.596 | Acc: 60.700%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 0.782 | Acc: 77.026%\n",
      "[=================================================>] 157/157 | Loss: 1.641 | Acc: 60.530%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 0.762 | Acc: 77.742%\n",
      "[=================================================>] 157/157 | Loss: 1.602 | Acc: 60.830%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 0.745 | Acc: 78.172%\n",
      "[=================================================>] 157/157 | Loss: 1.547 | Acc: 61.660%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 0.742 | Acc: 78.236%\n",
      "[=================================================>] 157/157 | Loss: 1.593 | Acc: 61.590%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 0.718 | Acc: 78.766%\n",
      "[=================================================>] 157/157 | Loss: 1.608 | Acc: 61.060%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 0.705 | Acc: 79.190%\n",
      "[=================================================>] 157/157 | Loss: 1.556 | Acc: 61.950%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 0.696 | Acc: 79.424%\n",
      "[=================================================>] 157/157 | Loss: 1.589 | Acc: 61.920%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 0.685 | Acc: 79.686%\n",
      "[=================================================>] 157/157 | Loss: 1.575 | Acc: 62.250%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 0.675 | Acc: 80.040%\n",
      "[=================================================>] 157/157 | Loss: 1.733 | Acc: 59.880%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 0.662 | Acc: 80.412%\n",
      "[=================================================>] 157/157 | Loss: 1.593 | Acc: 61.840%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 0.649 | Acc: 80.740%\n",
      "[=================================================>] 157/157 | Loss: 1.603 | Acc: 62.280%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 0.632 | Acc: 81.154%\n",
      "[=================================================>] 157/157 | Loss: 1.602 | Acc: 61.740%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 0.621 | Acc: 81.400%\n",
      "[=================================================>] 157/157 | Loss: 1.580 | Acc: 62.290%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 0.616 | Acc: 81.864%\n",
      "[=================================================>] 157/157 | Loss: 1.632 | Acc: 61.750%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 0.605 | Acc: 81.822%\n",
      "[=================================================>] 157/157 | Loss: 1.679 | Acc: 61.270%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 0.587 | Acc: 82.338%\n",
      "[=================================================>] 157/157 | Loss: 1.531 | Acc: 63.730%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 0.581 | Acc: 82.696%\n",
      "[=================================================>] 157/157 | Loss: 1.586 | Acc: 62.750%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 0.576 | Acc: 82.766%\n",
      "[=================================================>] 157/157 | Loss: 1.570 | Acc: 63.330%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 0.562 | Acc: 83.164%\n",
      "[=================================================>] 157/157 | Loss: 1.597 | Acc: 62.040%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 0.548 | Acc: 83.656%\n",
      "[=================================================>] 157/157 | Loss: 1.580 | Acc: 63.300%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 0.539 | Acc: 83.714%\n",
      "[=================================================>] 157/157 | Loss: 1.632 | Acc: 61.990%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 0.525 | Acc: 84.214%\n",
      "[=================================================>] 157/157 | Loss: 1.645 | Acc: 61.920%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 0.523 | Acc: 84.324%\n",
      "[=================================================>] 157/157 | Loss: 1.615 | Acc: 63.000%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 0.510 | Acc: 84.684%\n",
      "[=================================================>] 157/157 | Loss: 1.653 | Acc: 62.350%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 0.504 | Acc: 84.914%\n",
      "[=================================================>] 157/157 | Loss: 1.610 | Acc: 63.580%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 0.483 | Acc: 85.500%\n",
      "[=================================================>] 157/157 | Loss: 1.699 | Acc: 61.780%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 0.477 | Acc: 85.708%\n",
      "[=================================================>] 157/157 | Loss: 1.647 | Acc: 62.930%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 0.470 | Acc: 85.824%\n",
      "[=================================================>] 157/157 | Loss: 1.761 | Acc: 61.390%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 0.453 | Acc: 86.302%\n",
      "[=================================================>] 157/157 | Loss: 1.711 | Acc: 61.660%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 0.447 | Acc: 86.528%\n",
      "[=================================================>] 157/157 | Loss: 1.658 | Acc: 63.260%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 0.434 | Acc: 86.938%\n",
      "[=================================================>] 157/157 | Loss: 1.636 | Acc: 63.560%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 0.430 | Acc: 86.994%\n",
      "[=================================================>] 157/157 | Loss: 1.645 | Acc: 63.700%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 0.419 | Acc: 87.248%\n",
      "[=================================================>] 157/157 | Loss: 1.712 | Acc: 62.770%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 0.407 | Acc: 87.616%\n",
      "[=================================================>] 157/157 | Loss: 1.713 | Acc: 62.600%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 0.394 | Acc: 88.050%\n",
      "[=================================================>] 157/157 | Loss: 1.672 | Acc: 63.460%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 0.391 | Acc: 88.126%\n",
      "[=================================================>] 157/157 | Loss: 1.779 | Acc: 62.180%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 0.378 | Acc: 88.392%\n",
      "[=================================================>] 157/157 | Loss: 1.695 | Acc: 63.120%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 0.374 | Acc: 88.408%\n",
      "[=================================================>] 157/157 | Loss: 1.713 | Acc: 63.390%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 0.368 | Acc: 88.862%\n",
      "[=================================================>] 157/157 | Loss: 1.652 | Acc: 63.930%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 0.355 | Acc: 89.216%\n",
      "[=================================================>] 157/157 | Loss: 1.672 | Acc: 63.570%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 0.349 | Acc: 89.434%\n",
      "[=================================================>] 157/157 | Loss: 1.647 | Acc: 64.310%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 0.337 | Acc: 89.726%\n",
      "[=================================================>] 157/157 | Loss: 1.755 | Acc: 62.780%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 0.323 | Acc: 90.120%\n",
      "[=================================================>] 157/157 | Loss: 1.721 | Acc: 63.850%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 0.316 | Acc: 90.348%\n",
      "[=================================================>] 157/157 | Loss: 1.746 | Acc: 63.150%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 0.314 | Acc: 90.410%\n",
      "[=================================================>] 157/157 | Loss: 1.684 | Acc: 64.090%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 0.297 | Acc: 91.042%\n",
      "[=================================================>] 157/157 | Loss: 1.738 | Acc: 63.430%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 0.292 | Acc: 91.266%\n",
      "[=================================================>] 157/157 | Loss: 1.724 | Acc: 63.590%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 0.290 | Acc: 91.140%\n",
      "[=================================================>] 157/157 | Loss: 1.725 | Acc: 64.260%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 0.276 | Acc: 91.482%\n",
      "[=================================================>] 157/157 | Loss: 1.761 | Acc: 63.940%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 0.268 | Acc: 91.758%\n",
      "[=================================================>] 157/157 | Loss: 1.738 | Acc: 64.150%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 0.267 | Acc: 91.906%\n",
      "[=================================================>] 157/157 | Loss: 1.732 | Acc: 63.910%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 0.258 | Acc: 92.096%\n",
      "[=================================================>] 157/157 | Loss: 1.683 | Acc: 65.120%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 0.244 | Acc: 92.586%\n",
      "[=================================================>] 157/157 | Loss: 1.720 | Acc: 64.680%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 0.232 | Acc: 92.844%\n",
      "[=================================================>] 157/157 | Loss: 1.730 | Acc: 64.560%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 0.226 | Acc: 93.086%\n",
      "[=================================================>] 157/157 | Loss: 1.726 | Acc: 65.320%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 0.225 | Acc: 93.248%\n",
      "[=================================================>] 157/157 | Loss: 1.746 | Acc: 65.010%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 0.213 | Acc: 93.594%\n",
      "[=================================================>] 157/157 | Loss: 1.769 | Acc: 64.370%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 0.207 | Acc: 93.782%\n",
      "[=================================================>] 157/157 | Loss: 1.759 | Acc: 64.540%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 0.198 | Acc: 93.934%\n",
      "[=================================================>] 157/157 | Loss: 1.732 | Acc: 65.000%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 0.189 | Acc: 94.212%\n",
      "[=================================================>] 157/157 | Loss: 1.744 | Acc: 64.910%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 0.181 | Acc: 94.484%\n",
      "[=================================================>] 157/157 | Loss: 1.770 | Acc: 64.650%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 0.176 | Acc: 94.526%\n",
      "[=================================================>] 157/157 | Loss: 1.748 | Acc: 65.510%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 0.166 | Acc: 94.958%\n",
      "[=================================================>] 157/157 | Loss: 1.758 | Acc: 65.530%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 0.160 | Acc: 95.078%\n",
      "[=================================================>] 157/157 | Loss: 1.771 | Acc: 65.880%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.157 | Acc: 95.256%\n",
      "[=================================================>] 157/157 | Loss: 1.807 | Acc: 64.870%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.149 | Acc: 95.466%\n",
      "[=================================================>] 157/157 | Loss: 1.776 | Acc: 65.340%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.140 | Acc: 95.664%\n",
      "[=================================================>] 157/157 | Loss: 1.808 | Acc: 65.210%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.130 | Acc: 96.044%\n",
      "[=================================================>] 157/157 | Loss: 1.786 | Acc: 65.430%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.136 | Acc: 95.910%\n",
      "[=================================================>] 157/157 | Loss: 1.788 | Acc: 65.660%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.120 | Acc: 96.522%\n",
      "[=================================================>] 157/157 | Loss: 1.828 | Acc: 65.500%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.115 | Acc: 96.582%\n",
      "[=================================================>] 157/157 | Loss: 1.796 | Acc: 65.750%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.111 | Acc: 96.720%\n",
      "[=================================================>] 157/157 | Loss: 1.814 | Acc: 65.690%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.107 | Acc: 96.852%\n",
      "[=================================================>] 157/157 | Loss: 1.810 | Acc: 65.970%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.098 | Acc: 97.052%\n",
      "[=================================================>] 157/157 | Loss: 1.825 | Acc: 66.290%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.095 | Acc: 97.078%\n",
      "[=================================================>] 157/157 | Loss: 1.781 | Acc: 66.450%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.091 | Acc: 97.346%\n",
      "[=================================================>] 157/157 | Loss: 1.785 | Acc: 66.560%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.086 | Acc: 97.460%\n",
      "[=================================================>] 157/157 | Loss: 1.724 | Acc: 67.770%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.073 | Acc: 97.792%\n",
      "[=================================================>] 157/157 | Loss: 1.769 | Acc: 67.080%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.074 | Acc: 97.854%\n",
      "[=================================================>] 157/157 | Loss: 1.753 | Acc: 67.140%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.066 | Acc: 98.030%\n",
      "[=================================================>] 157/157 | Loss: 1.780 | Acc: 66.960%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.062 | Acc: 98.130%\n",
      "[=================================================>] 157/157 | Loss: 1.784 | Acc: 66.910%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.055 | Acc: 98.386%\n",
      "[=================================================>] 157/157 | Loss: 1.800 | Acc: 66.820%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.055 | Acc: 98.390%\n",
      "[=================================================>] 157/157 | Loss: 1.793 | Acc: 67.040%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.049 | Acc: 98.602%\n",
      "[=================================================>] 157/157 | Loss: 1.782 | Acc: 67.200%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.045 | Acc: 98.650%\n",
      "[=================================================>] 157/157 | Loss: 1.791 | Acc: 66.890%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.041 | Acc: 98.856%\n",
      "[=================================================>] 157/157 | Loss: 1.765 | Acc: 67.730%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.037 | Acc: 98.912%\n",
      "[=================================================>] 157/157 | Loss: 1.767 | Acc: 67.330%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.034 | Acc: 99.026%\n",
      "[=================================================>] 157/157 | Loss: 1.795 | Acc: 67.400%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.031 | Acc: 99.116%\n",
      "[=================================================>] 157/157 | Loss: 1.778 | Acc: 67.660%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.032 | Acc: 99.140%\n",
      "[=================================================>] 157/157 | Loss: 1.763 | Acc: 67.940%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.026 | Acc: 99.256%\n",
      "[=================================================>] 157/157 | Loss: 1.773 | Acc: 67.720%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.024 | Acc: 99.332%\n",
      "[=================================================>] 157/157 | Loss: 1.749 | Acc: 67.980%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.022 | Acc: 99.416%\n",
      "[=================================================>] 157/157 | Loss: 1.751 | Acc: 68.250%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.518%\n",
      "[=================================================>] 157/157 | Loss: 1.727 | Acc: 68.430%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.018 | Acc: 99.550%\n",
      "[=================================================>] 157/157 | Loss: 1.745 | Acc: 68.300%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.016 | Acc: 99.594%\n",
      "[=================================================>] 157/157 | Loss: 1.740 | Acc: 68.440%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.015 | Acc: 99.616%\n",
      "[=================================================>] 157/157 | Loss: 1.726 | Acc: 68.470%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.654%\n",
      "[=================================================>] 157/157 | Loss: 1.727 | Acc: 68.550%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.013 | Acc: 99.662%\n",
      "[=================================================>] 157/157 | Loss: 1.711 | Acc: 68.650%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.726%\n",
      "[=================================================>] 157/157 | Loss: 1.698 | Acc: 69.040%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.011 | Acc: 99.772%\n",
      "[=================================================>] 157/157 | Loss: 1.717 | Acc: 68.710%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.758%\n",
      "[=================================================>] 157/157 | Loss: 1.700 | Acc: 68.570%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.010 | Acc: 99.756%\n",
      "[=================================================>] 157/157 | Loss: 1.691 | Acc: 68.920%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.786%\n",
      "[=================================================>] 157/157 | Loss: 1.689 | Acc: 69.280%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.780%\n",
      "[=================================================>] 157/157 | Loss: 1.688 | Acc: 69.340%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.009 | Acc: 99.808%\n",
      "[=================================================>] 157/157 | Loss: 1.674 | Acc: 69.280%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.834%\n",
      "[=================================================>] 157/157 | Loss: 1.662 | Acc: 69.250%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.830%\n",
      "[=================================================>] 157/157 | Loss: 1.678 | Acc: 69.100%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.008 | Acc: 99.860%\n",
      "[=================================================>] 157/157 | Loss: 1.676 | Acc: 69.170%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.858%\n",
      "[=================================================>] 157/157 | Loss: 1.674 | Acc: 69.250%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.876%\n",
      "[=================================================>] 157/157 | Loss: 1.677 | Acc: 69.200%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.880%\n",
      "[=================================================>] 157/157 | Loss: 1.657 | Acc: 69.280%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.872%\n",
      "[=================================================>] 157/157 | Loss: 1.655 | Acc: 69.350%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.854%\n",
      "[=================================================>] 157/157 | Loss: 1.657 | Acc: 69.430%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.862%\n",
      "[=================================================>] 157/157 | Loss: 1.661 | Acc: 69.440%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.862%\n",
      "[=================================================>] 157/157 | Loss: 1.654 | Acc: 69.410%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.864%\n",
      "[=================================================>] 157/157 | Loss: 1.656 | Acc: 69.350%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.876%\n",
      "[=================================================>] 157/157 | Loss: 1.640 | Acc: 69.520%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.007 | Acc: 99.874%\n",
      "[=================================================>] 157/157 | Loss: 1.661 | Acc: 69.330%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.886%\n",
      "[=================================================>] 157/157 | Loss: 1.651 | Acc: 69.670%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.890%\n",
      "[=================================================>] 157/157 | Loss: 1.645 | Acc: 69.430%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.862%\n",
      "[=================================================>] 157/157 | Loss: 1.663 | Acc: 69.540%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.006 | Acc: 99.886%\n",
      "[=================================================>] 157/157 | Loss: 1.657 | Acc: 69.490%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 24.98M (26.6% reduction)\n",
      "MACs: 0.33G => 0.12G (63.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'max'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 3\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_max_30_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d0b97",
   "metadata": {},
   "source": [
    "##### ~72% filters pruned (512*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8977c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from epoch 238 with best acc = 72.7800\n",
      "\n",
      "==> Starting pruning process...\n",
      "Base MACs: 0.33G | Params: 34.02M\n",
      "\n",
      "Testing before pruning:\n",
      "[=================================================>] 157/157 | Loss: 1.390 | Acc: 72.780%\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Ranking filters...\n",
      "\n",
      "Layers to prune: {0: 48, 3: 58, 7: 112, 10: 109, 14: 201, 17: 180, 20: 197, 24: 356, 27: 322, 30: 355, 34: 321, 37: 330, 40: 483}\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Pruning filters...\n",
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(16, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(19, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(55, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(76, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(59, 156, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(156, 190, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(190, 157, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(157, 191, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(191, 182, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(182, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Fine-tuning after pruning...\n",
      "\n",
      "Epoch 1/150\n",
      "[=================================================>] 782/782 | Loss: 4.300 | Acc: 3.530%\n",
      "[=================================================>] 157/157 | Loss: 3.951 | Acc: 6.840%\n",
      "\n",
      "Epoch 2/150\n",
      "[=================================================>] 782/782 | Loss: 3.852 | Acc: 8.062%\n",
      "[=================================================>] 157/157 | Loss: 3.658 | Acc: 10.890%\n",
      "\n",
      "Epoch 3/150\n",
      "[=================================================>] 782/782 | Loss: 3.556 | Acc: 13.014%\n",
      "[=================================================>] 157/157 | Loss: 3.605 | Acc: 14.170%\n",
      "\n",
      "Epoch 4/150\n",
      "[=================================================>] 782/782 | Loss: 3.316 | Acc: 17.274%\n",
      "[=================================================>] 157/157 | Loss: 3.443 | Acc: 16.630%\n",
      "\n",
      "Epoch 5/150\n",
      "[=================================================>] 782/782 | Loss: 3.145 | Acc: 20.428%\n",
      "[=================================================>] 157/157 | Loss: 3.104 | Acc: 22.150%\n",
      "\n",
      "Epoch 6/150\n",
      "[=================================================>] 782/782 | Loss: 3.018 | Acc: 22.828%\n",
      "[=================================================>] 157/157 | Loss: 2.995 | Acc: 23.330%\n",
      "\n",
      "Epoch 7/150\n",
      "[=================================================>] 782/782 | Loss: 2.885 | Acc: 25.818%\n",
      "[=================================================>] 157/157 | Loss: 2.790 | Acc: 28.400%\n",
      "\n",
      "Epoch 8/150\n",
      "[=================================================>] 782/782 | Loss: 2.785 | Acc: 27.868%\n",
      "[=================================================>] 157/157 | Loss: 2.719 | Acc: 29.800%\n",
      "\n",
      "Epoch 9/150\n",
      "[=================================================>] 782/782 | Loss: 2.683 | Acc: 29.938%\n",
      "[=================================================>] 157/157 | Loss: 2.818 | Acc: 29.260%\n",
      "\n",
      "Epoch 10/150\n",
      "[=================================================>] 782/782 | Loss: 2.609 | Acc: 31.780%\n",
      "[=================================================>] 157/157 | Loss: 2.578 | Acc: 33.350%\n",
      "\n",
      "Epoch 11/150\n",
      "[=================================================>] 782/782 | Loss: 2.539 | Acc: 33.284%\n",
      "[=================================================>] 157/157 | Loss: 2.446 | Acc: 34.690%\n",
      "\n",
      "Epoch 12/150\n",
      "[=================================================>] 782/782 | Loss: 2.476 | Acc: 34.912%\n",
      "[=================================================>] 157/157 | Loss: 2.564 | Acc: 33.680%\n",
      "\n",
      "Epoch 13/150\n",
      "[=================================================>] 782/782 | Loss: 2.424 | Acc: 35.870%\n",
      "[=================================================>] 157/157 | Loss: 2.460 | Acc: 35.910%\n",
      "\n",
      "Epoch 14/150\n",
      "[=================================================>] 782/782 | Loss: 2.379 | Acc: 37.006%\n",
      "[=================================================>] 157/157 | Loss: 2.521 | Acc: 35.270%\n",
      "\n",
      "Epoch 15/150\n",
      "[=================================================>] 782/782 | Loss: 2.332 | Acc: 38.472%\n",
      "[=================================================>] 157/157 | Loss: 2.361 | Acc: 38.390%\n",
      "\n",
      "Epoch 16/150\n",
      "[=================================================>] 782/782 | Loss: 2.300 | Acc: 38.904%\n",
      "[=================================================>] 157/157 | Loss: 2.393 | Acc: 37.940%\n",
      "\n",
      "Epoch 17/150\n",
      "[=================================================>] 782/782 | Loss: 2.250 | Acc: 40.242%\n",
      "[=================================================>] 157/157 | Loss: 2.354 | Acc: 38.670%\n",
      "\n",
      "Epoch 18/150\n",
      "[=================================================>] 782/782 | Loss: 2.223 | Acc: 40.902%\n",
      "[=================================================>] 157/157 | Loss: 2.323 | Acc: 39.400%\n",
      "\n",
      "Epoch 19/150\n",
      "[=================================================>] 782/782 | Loss: 2.187 | Acc: 41.656%\n",
      "[=================================================>] 157/157 | Loss: 2.376 | Acc: 39.270%\n",
      "\n",
      "Epoch 20/150\n",
      "[=================================================>] 782/782 | Loss: 2.162 | Acc: 42.298%\n",
      "[=================================================>] 157/157 | Loss: 2.477 | Acc: 37.740%\n",
      "\n",
      "Epoch 21/150\n",
      "[=================================================>] 782/782 | Loss: 2.130 | Acc: 43.048%\n",
      "[=================================================>] 157/157 | Loss: 2.217 | Acc: 41.770%\n",
      "\n",
      "Epoch 22/150\n",
      "[=================================================>] 782/782 | Loss: 2.115 | Acc: 43.624%\n",
      "[=================================================>] 157/157 | Loss: 2.227 | Acc: 42.050%\n",
      "\n",
      "Epoch 23/150\n",
      "[=================================================>] 782/782 | Loss: 2.074 | Acc: 44.686%\n",
      "[=================================================>] 157/157 | Loss: 2.315 | Acc: 40.390%\n",
      "\n",
      "Epoch 24/150\n",
      "[=================================================>] 782/782 | Loss: 2.052 | Acc: 45.102%\n",
      "[=================================================>] 157/157 | Loss: 2.092 | Acc: 45.070%\n",
      "\n",
      "Epoch 25/150\n",
      "[=================================================>] 782/782 | Loss: 2.026 | Acc: 45.722%\n",
      "[=================================================>] 157/157 | Loss: 2.164 | Acc: 44.090%\n",
      "\n",
      "Epoch 26/150\n",
      "[=================================================>] 782/782 | Loss: 2.008 | Acc: 46.304%\n",
      "[=================================================>] 157/157 | Loss: 2.131 | Acc: 44.180%\n",
      "\n",
      "Epoch 27/150\n",
      "[=================================================>] 782/782 | Loss: 1.990 | Acc: 46.428%\n",
      "[=================================================>] 157/157 | Loss: 2.183 | Acc: 42.900%\n",
      "\n",
      "Epoch 28/150\n",
      "[=================================================>] 782/782 | Loss: 1.958 | Acc: 47.416%\n",
      "[=================================================>] 157/157 | Loss: 2.278 | Acc: 42.450%\n",
      "\n",
      "Epoch 29/150\n",
      "[=================================================>] 782/782 | Loss: 1.939 | Acc: 47.962%\n",
      "[=================================================>] 157/157 | Loss: 2.035 | Acc: 46.280%\n",
      "\n",
      "Epoch 30/150\n",
      "[=================================================>] 782/782 | Loss: 1.917 | Acc: 48.268%\n",
      "[=================================================>] 157/157 | Loss: 2.118 | Acc: 44.560%\n",
      "\n",
      "Epoch 31/150\n",
      "[=================================================>] 782/782 | Loss: 1.900 | Acc: 48.930%\n",
      "[=================================================>] 157/157 | Loss: 2.042 | Acc: 46.270%\n",
      "\n",
      "Epoch 32/150\n",
      "[=================================================>] 782/782 | Loss: 1.878 | Acc: 49.216%\n",
      "[=================================================>] 157/157 | Loss: 2.060 | Acc: 46.400%\n",
      "\n",
      "Epoch 33/150\n",
      "[=================================================>] 782/782 | Loss: 1.863 | Acc: 49.578%\n",
      "[=================================================>] 157/157 | Loss: 2.146 | Acc: 44.820%\n",
      "\n",
      "Epoch 34/150\n",
      "[=================================================>] 782/782 | Loss: 1.850 | Acc: 50.004%\n",
      "[=================================================>] 157/157 | Loss: 1.981 | Acc: 47.860%\n",
      "\n",
      "Epoch 35/150\n",
      "[=================================================>] 782/782 | Loss: 1.827 | Acc: 50.632%\n",
      "[=================================================>] 157/157 | Loss: 1.980 | Acc: 48.020%\n",
      "\n",
      "Epoch 36/150\n",
      "[=================================================>] 782/782 | Loss: 1.808 | Acc: 51.092%\n",
      "[=================================================>] 157/157 | Loss: 2.178 | Acc: 45.430%\n",
      "\n",
      "Epoch 37/150\n",
      "[=================================================>] 782/782 | Loss: 1.798 | Acc: 51.052%\n",
      "[=================================================>] 157/157 | Loss: 1.989 | Acc: 48.030%\n",
      "\n",
      "Epoch 38/150\n",
      "[=================================================>] 782/782 | Loss: 1.782 | Acc: 51.926%\n",
      "[=================================================>] 157/157 | Loss: 1.961 | Acc: 48.970%\n",
      "\n",
      "Epoch 39/150\n",
      "[=================================================>] 782/782 | Loss: 1.760 | Acc: 52.176%\n",
      "[=================================================>] 157/157 | Loss: 1.970 | Acc: 48.490%\n",
      "\n",
      "Epoch 40/150\n",
      "[=================================================>] 782/782 | Loss: 1.753 | Acc: 52.446%\n",
      "[=================================================>] 157/157 | Loss: 1.963 | Acc: 48.530%\n",
      "\n",
      "Epoch 41/150\n",
      "[=================================================>] 782/782 | Loss: 1.736 | Acc: 52.872%\n",
      "[=================================================>] 157/157 | Loss: 1.988 | Acc: 48.570%\n",
      "\n",
      "Epoch 42/150\n",
      "[=================================================>] 782/782 | Loss: 1.714 | Acc: 53.402%\n",
      "[=================================================>] 157/157 | Loss: 1.932 | Acc: 49.160%\n",
      "\n",
      "Epoch 43/150\n",
      "[=================================================>] 782/782 | Loss: 1.698 | Acc: 54.092%\n",
      "[=================================================>] 157/157 | Loss: 1.917 | Acc: 50.160%\n",
      "\n",
      "Epoch 44/150\n",
      "[=================================================>] 782/782 | Loss: 1.682 | Acc: 53.786%\n",
      "[=================================================>] 157/157 | Loss: 2.005 | Acc: 48.930%\n",
      "\n",
      "Epoch 45/150\n",
      "[=================================================>] 782/782 | Loss: 1.675 | Acc: 54.582%\n",
      "[=================================================>] 157/157 | Loss: 1.942 | Acc: 49.820%\n",
      "\n",
      "Epoch 46/150\n",
      "[=================================================>] 782/782 | Loss: 1.649 | Acc: 54.970%\n",
      "[=================================================>] 157/157 | Loss: 2.005 | Acc: 48.470%\n",
      "\n",
      "Epoch 47/150\n",
      "[=================================================>] 782/782 | Loss: 1.634 | Acc: 55.320%\n",
      "[=================================================>] 157/157 | Loss: 1.873 | Acc: 50.730%\n",
      "\n",
      "Epoch 48/150\n",
      "[=================================================>] 782/782 | Loss: 1.631 | Acc: 55.470%\n",
      "[=================================================>] 157/157 | Loss: 1.897 | Acc: 50.660%\n",
      "\n",
      "Epoch 49/150\n",
      "[=================================================>] 782/782 | Loss: 1.621 | Acc: 55.484%\n",
      "[=================================================>] 157/157 | Loss: 2.057 | Acc: 47.640%\n",
      "\n",
      "Epoch 50/150\n",
      "[=================================================>] 782/782 | Loss: 1.593 | Acc: 56.526%\n",
      "[=================================================>] 157/157 | Loss: 1.883 | Acc: 50.980%\n",
      "\n",
      "Epoch 51/150\n",
      "[=================================================>] 782/782 | Loss: 1.589 | Acc: 56.520%\n",
      "[=================================================>] 157/157 | Loss: 1.860 | Acc: 51.470%\n",
      "\n",
      "Epoch 52/150\n",
      "[=================================================>] 782/782 | Loss: 1.574 | Acc: 56.828%\n",
      "[=================================================>] 157/157 | Loss: 1.868 | Acc: 50.750%\n",
      "\n",
      "Epoch 53/150\n",
      "[=================================================>] 782/782 | Loss: 1.567 | Acc: 56.908%\n",
      "[=================================================>] 157/157 | Loss: 1.867 | Acc: 50.830%\n",
      "\n",
      "Epoch 54/150\n",
      "[=================================================>] 782/782 | Loss: 1.546 | Acc: 57.340%\n",
      "[=================================================>] 157/157 | Loss: 1.923 | Acc: 50.930%\n",
      "\n",
      "Epoch 55/150\n",
      "[=================================================>] 782/782 | Loss: 1.536 | Acc: 57.656%\n",
      "[=================================================>] 157/157 | Loss: 1.860 | Acc: 51.640%\n",
      "\n",
      "Epoch 56/150\n",
      "[=================================================>] 782/782 | Loss: 1.519 | Acc: 58.240%\n",
      "[=================================================>] 157/157 | Loss: 1.846 | Acc: 52.180%\n",
      "\n",
      "Epoch 57/150\n",
      "[=================================================>] 782/782 | Loss: 1.501 | Acc: 58.710%\n",
      "[=================================================>] 157/157 | Loss: 1.859 | Acc: 52.450%\n",
      "\n",
      "Epoch 58/150\n",
      "[=================================================>] 782/782 | Loss: 1.497 | Acc: 58.840%\n",
      "[=================================================>] 157/157 | Loss: 1.828 | Acc: 52.640%\n",
      "\n",
      "Epoch 59/150\n",
      "[=================================================>] 782/782 | Loss: 1.478 | Acc: 59.096%\n",
      "[=================================================>] 157/157 | Loss: 1.884 | Acc: 51.820%\n",
      "\n",
      "Epoch 60/150\n",
      "[=================================================>] 782/782 | Loss: 1.471 | Acc: 59.444%\n",
      "[=================================================>] 157/157 | Loss: 1.970 | Acc: 50.190%\n",
      "\n",
      "Epoch 61/150\n",
      "[=================================================>] 782/782 | Loss: 1.451 | Acc: 59.730%\n",
      "[=================================================>] 157/157 | Loss: 1.828 | Acc: 52.680%\n",
      "\n",
      "Epoch 62/150\n",
      "[=================================================>] 782/782 | Loss: 1.435 | Acc: 60.084%\n",
      "[=================================================>] 157/157 | Loss: 1.781 | Acc: 53.780%\n",
      "\n",
      "Epoch 63/150\n",
      "[=================================================>] 782/782 | Loss: 1.425 | Acc: 60.304%\n",
      "[=================================================>] 157/157 | Loss: 1.834 | Acc: 52.300%\n",
      "\n",
      "Epoch 64/150\n",
      "[=================================================>] 782/782 | Loss: 1.403 | Acc: 60.816%\n",
      "[=================================================>] 157/157 | Loss: 1.794 | Acc: 53.390%\n",
      "\n",
      "Epoch 65/150\n",
      "[=================================================>] 782/782 | Loss: 1.398 | Acc: 60.998%\n",
      "[=================================================>] 157/157 | Loss: 1.913 | Acc: 50.980%\n",
      "\n",
      "Epoch 66/150\n",
      "[=================================================>] 782/782 | Loss: 1.394 | Acc: 61.332%\n",
      "[=================================================>] 157/157 | Loss: 1.847 | Acc: 52.500%\n",
      "\n",
      "Epoch 67/150\n",
      "[=================================================>] 782/782 | Loss: 1.374 | Acc: 61.526%\n",
      "[=================================================>] 157/157 | Loss: 1.936 | Acc: 51.340%\n",
      "\n",
      "Epoch 68/150\n",
      "[=================================================>] 782/782 | Loss: 1.354 | Acc: 62.114%\n",
      "[=================================================>] 157/157 | Loss: 1.816 | Acc: 53.090%\n",
      "\n",
      "Epoch 69/150\n",
      "[=================================================>] 782/782 | Loss: 1.344 | Acc: 62.280%\n",
      "[=================================================>] 157/157 | Loss: 1.738 | Acc: 54.560%\n",
      "\n",
      "Epoch 70/150\n",
      "[=================================================>] 782/782 | Loss: 1.326 | Acc: 62.808%\n",
      "[=================================================>] 157/157 | Loss: 1.865 | Acc: 52.290%\n",
      "\n",
      "Epoch 71/150\n",
      "[=================================================>] 782/782 | Loss: 1.313 | Acc: 63.012%\n",
      "[=================================================>] 157/157 | Loss: 1.753 | Acc: 54.380%\n",
      "\n",
      "Epoch 72/150\n",
      "[=================================================>] 782/782 | Loss: 1.301 | Acc: 63.570%\n",
      "[=================================================>] 157/157 | Loss: 1.856 | Acc: 52.670%\n",
      "\n",
      "Epoch 73/150\n",
      "[=================================================>] 782/782 | Loss: 1.289 | Acc: 63.752%\n",
      "[=================================================>] 157/157 | Loss: 1.773 | Acc: 54.680%\n",
      "\n",
      "Epoch 74/150\n",
      "[=================================================>] 782/782 | Loss: 1.270 | Acc: 64.206%\n",
      "[=================================================>] 157/157 | Loss: 1.779 | Acc: 54.560%\n",
      "\n",
      "Epoch 75/150\n",
      "[=================================================>] 782/782 | Loss: 1.253 | Acc: 64.554%\n",
      "[=================================================>] 157/157 | Loss: 1.833 | Acc: 53.590%\n",
      "\n",
      "Epoch 76/150\n",
      "[=================================================>] 782/782 | Loss: 1.236 | Acc: 65.202%\n",
      "[=================================================>] 157/157 | Loss: 1.783 | Acc: 54.380%\n",
      "\n",
      "Epoch 77/150\n",
      "[=================================================>] 782/782 | Loss: 1.231 | Acc: 65.174%\n",
      "[=================================================>] 157/157 | Loss: 1.834 | Acc: 53.410%\n",
      "\n",
      "Epoch 78/150\n",
      "[=================================================>] 782/782 | Loss: 1.219 | Acc: 65.500%\n",
      "[=================================================>] 157/157 | Loss: 1.756 | Acc: 55.910%\n",
      "\n",
      "Epoch 79/150\n",
      "[=================================================>] 782/782 | Loss: 1.198 | Acc: 65.996%\n",
      "[=================================================>] 157/157 | Loss: 1.737 | Acc: 55.190%\n",
      "\n",
      "Epoch 80/150\n",
      "[=================================================>] 782/782 | Loss: 1.194 | Acc: 66.330%\n",
      "[=================================================>] 157/157 | Loss: 1.792 | Acc: 54.300%\n",
      "\n",
      "Epoch 81/150\n",
      "[=================================================>] 782/782 | Loss: 1.168 | Acc: 66.842%\n",
      "[=================================================>] 157/157 | Loss: 1.760 | Acc: 55.410%\n",
      "\n",
      "Epoch 82/150\n",
      "[=================================================>] 782/782 | Loss: 1.160 | Acc: 67.066%\n",
      "[=================================================>] 157/157 | Loss: 1.796 | Acc: 54.280%\n",
      "\n",
      "Epoch 83/150\n",
      "[=================================================>] 782/782 | Loss: 1.150 | Acc: 67.358%\n",
      "[=================================================>] 157/157 | Loss: 1.818 | Acc: 54.530%\n",
      "\n",
      "Epoch 84/150\n",
      "[=================================================>] 782/782 | Loss: 1.119 | Acc: 67.942%\n",
      "[=================================================>] 157/157 | Loss: 1.776 | Acc: 55.020%\n",
      "\n",
      "Epoch 85/150\n",
      "[=================================================>] 782/782 | Loss: 1.107 | Acc: 68.218%\n",
      "[=================================================>] 157/157 | Loss: 1.794 | Acc: 55.510%\n",
      "\n",
      "Epoch 86/150\n",
      "[=================================================>] 782/782 | Loss: 1.090 | Acc: 68.930%\n",
      "[=================================================>] 157/157 | Loss: 1.776 | Acc: 55.790%\n",
      "\n",
      "Epoch 87/150\n",
      "[=================================================>] 782/782 | Loss: 1.066 | Acc: 69.764%\n",
      "[=================================================>] 157/157 | Loss: 1.791 | Acc: 55.780%\n",
      "\n",
      "Epoch 88/150\n",
      "[=================================================>] 782/782 | Loss: 1.062 | Acc: 69.530%\n",
      "[=================================================>] 157/157 | Loss: 1.758 | Acc: 55.650%\n",
      "\n",
      "Epoch 89/150\n",
      "[=================================================>] 782/782 | Loss: 1.043 | Acc: 70.032%\n",
      "[=================================================>] 157/157 | Loss: 1.800 | Acc: 55.270%\n",
      "\n",
      "Epoch 90/150\n",
      "[=================================================>] 782/782 | Loss: 1.032 | Acc: 70.476%\n",
      "[=================================================>] 157/157 | Loss: 1.777 | Acc: 56.140%\n",
      "\n",
      "Epoch 91/150\n",
      "[=================================================>] 782/782 | Loss: 1.015 | Acc: 70.452%\n",
      "[=================================================>] 157/157 | Loss: 1.749 | Acc: 56.540%\n",
      "\n",
      "Epoch 92/150\n",
      "[=================================================>] 782/782 | Loss: 0.998 | Acc: 71.376%\n",
      "[=================================================>] 157/157 | Loss: 1.795 | Acc: 56.400%\n",
      "\n",
      "Epoch 93/150\n",
      "[=================================================>] 782/782 | Loss: 0.982 | Acc: 71.780%\n",
      "[=================================================>] 157/157 | Loss: 1.786 | Acc: 56.070%\n",
      "\n",
      "Epoch 94/150\n",
      "[=================================================>] 782/782 | Loss: 0.969 | Acc: 71.996%\n",
      "[=================================================>] 157/157 | Loss: 1.757 | Acc: 56.710%\n",
      "\n",
      "Epoch 95/150\n",
      "[=================================================>] 782/782 | Loss: 0.955 | Acc: 72.146%\n",
      "[=================================================>] 157/157 | Loss: 1.802 | Acc: 55.960%\n",
      "\n",
      "Epoch 96/150\n",
      "[=================================================>] 782/782 | Loss: 0.939 | Acc: 72.664%\n",
      "[=================================================>] 157/157 | Loss: 1.796 | Acc: 55.800%\n",
      "\n",
      "Epoch 97/150\n",
      "[=================================================>] 782/782 | Loss: 0.917 | Acc: 73.174%\n",
      "[=================================================>] 157/157 | Loss: 1.790 | Acc: 56.770%\n",
      "\n",
      "Epoch 98/150\n",
      "[=================================================>] 782/782 | Loss: 0.909 | Acc: 73.554%\n",
      "[=================================================>] 157/157 | Loss: 1.771 | Acc: 56.320%\n",
      "\n",
      "Epoch 99/150\n",
      "[=================================================>] 782/782 | Loss: 0.889 | Acc: 74.006%\n",
      "[=================================================>] 157/157 | Loss: 1.797 | Acc: 56.310%\n",
      "\n",
      "Epoch 100/150\n",
      "[=================================================>] 782/782 | Loss: 0.861 | Acc: 74.662%\n",
      "[=================================================>] 157/157 | Loss: 1.811 | Acc: 56.440%\n",
      "\n",
      "Epoch 101/150\n",
      "[=================================================>] 782/782 | Loss: 0.851 | Acc: 75.120%\n",
      "[=================================================>] 157/157 | Loss: 1.785 | Acc: 57.000%\n",
      "\n",
      "Epoch 102/150\n",
      "[=================================================>] 782/782 | Loss: 0.839 | Acc: 75.468%\n",
      "[=================================================>] 157/157 | Loss: 1.810 | Acc: 56.700%\n",
      "\n",
      "Epoch 103/150\n",
      "[=================================================>] 782/782 | Loss: 0.822 | Acc: 75.874%\n",
      "[=================================================>] 157/157 | Loss: 1.801 | Acc: 57.350%\n",
      "\n",
      "Epoch 104/150\n",
      "[=================================================>] 782/782 | Loss: 0.802 | Acc: 76.458%\n",
      "[=================================================>] 157/157 | Loss: 1.805 | Acc: 56.910%\n",
      "\n",
      "Epoch 105/150\n",
      "[=================================================>] 782/782 | Loss: 0.784 | Acc: 76.964%\n",
      "[=================================================>] 157/157 | Loss: 1.798 | Acc: 57.400%\n",
      "\n",
      "Epoch 106/150\n",
      "[=================================================>] 782/782 | Loss: 0.760 | Acc: 77.502%\n",
      "[=================================================>] 157/157 | Loss: 1.819 | Acc: 57.320%\n",
      "\n",
      "Epoch 107/150\n",
      "[=================================================>] 782/782 | Loss: 0.754 | Acc: 77.594%\n",
      "[=================================================>] 157/157 | Loss: 1.847 | Acc: 56.700%\n",
      "\n",
      "Epoch 108/150\n",
      "[=================================================>] 782/782 | Loss: 0.734 | Acc: 78.318%\n",
      "[=================================================>] 157/157 | Loss: 1.838 | Acc: 57.110%\n",
      "\n",
      "Epoch 109/150\n",
      "[=================================================>] 782/782 | Loss: 0.719 | Acc: 78.582%\n",
      "[=================================================>] 157/157 | Loss: 1.861 | Acc: 56.780%\n",
      "\n",
      "Epoch 110/150\n",
      "[=================================================>] 782/782 | Loss: 0.705 | Acc: 79.134%\n",
      "[=================================================>] 157/157 | Loss: 1.859 | Acc: 57.230%\n",
      "\n",
      "Epoch 111/150\n",
      "[=================================================>] 782/782 | Loss: 0.689 | Acc: 79.350%\n",
      "[=================================================>] 157/157 | Loss: 1.860 | Acc: 57.440%\n",
      "\n",
      "Epoch 112/150\n",
      "[=================================================>] 782/782 | Loss: 0.665 | Acc: 80.218%\n",
      "[=================================================>] 157/157 | Loss: 1.863 | Acc: 57.710%\n",
      "\n",
      "Epoch 113/150\n",
      "[=================================================>] 782/782 | Loss: 0.659 | Acc: 80.530%\n",
      "[=================================================>] 157/157 | Loss: 1.864 | Acc: 57.240%\n",
      "\n",
      "Epoch 114/150\n",
      "[=================================================>] 782/782 | Loss: 0.644 | Acc: 80.900%\n",
      "[=================================================>] 157/157 | Loss: 1.916 | Acc: 57.290%\n",
      "\n",
      "Epoch 115/150\n",
      "[=================================================>] 782/782 | Loss: 0.622 | Acc: 81.396%\n",
      "[=================================================>] 157/157 | Loss: 1.959 | Acc: 56.860%\n",
      "\n",
      "Epoch 116/150\n",
      "[=================================================>] 782/782 | Loss: 0.609 | Acc: 81.684%\n",
      "[=================================================>] 157/157 | Loss: 1.925 | Acc: 57.060%\n",
      "\n",
      "Epoch 117/150\n",
      "[=================================================>] 782/782 | Loss: 0.598 | Acc: 82.206%\n",
      "[=================================================>] 157/157 | Loss: 1.892 | Acc: 57.620%\n",
      "\n",
      "Epoch 118/150\n",
      "[=================================================>] 782/782 | Loss: 0.576 | Acc: 82.790%\n",
      "[=================================================>] 157/157 | Loss: 1.925 | Acc: 57.940%\n",
      "\n",
      "Epoch 119/150\n",
      "[=================================================>] 782/782 | Loss: 0.567 | Acc: 82.972%\n",
      "[=================================================>] 157/157 | Loss: 1.919 | Acc: 57.950%\n",
      "\n",
      "Epoch 120/150\n",
      "[=================================================>] 782/782 | Loss: 0.559 | Acc: 83.200%\n",
      "[=================================================>] 157/157 | Loss: 1.936 | Acc: 57.430%\n",
      "\n",
      "Epoch 121/150\n",
      "[=================================================>] 782/782 | Loss: 0.540 | Acc: 83.560%\n",
      "[=================================================>] 157/157 | Loss: 1.966 | Acc: 57.070%\n",
      "\n",
      "Epoch 122/150\n",
      "[=================================================>] 782/782 | Loss: 0.524 | Acc: 84.238%\n",
      "[=================================================>] 157/157 | Loss: 1.956 | Acc: 57.580%\n",
      "\n",
      "Epoch 123/150\n",
      "[=================================================>] 782/782 | Loss: 0.513 | Acc: 84.666%\n",
      "[=================================================>] 157/157 | Loss: 1.968 | Acc: 57.750%\n",
      "\n",
      "Epoch 124/150\n",
      "[=================================================>] 782/782 | Loss: 0.496 | Acc: 84.972%\n",
      "[=================================================>] 157/157 | Loss: 1.978 | Acc: 57.740%\n",
      "\n",
      "Epoch 125/150\n",
      "[=================================================>] 782/782 | Loss: 0.488 | Acc: 85.478%\n",
      "[=================================================>] 157/157 | Loss: 1.962 | Acc: 57.890%\n",
      "\n",
      "Epoch 126/150\n",
      "[=================================================>] 782/782 | Loss: 0.482 | Acc: 85.420%\n",
      "[=================================================>] 157/157 | Loss: 1.997 | Acc: 57.480%\n",
      "\n",
      "Epoch 127/150\n",
      "[=================================================>] 782/782 | Loss: 0.471 | Acc: 85.688%\n",
      "[=================================================>] 157/157 | Loss: 1.980 | Acc: 57.860%\n",
      "\n",
      "Epoch 128/150\n",
      "[=================================================>] 782/782 | Loss: 0.455 | Acc: 86.048%\n",
      "[=================================================>] 157/157 | Loss: 1.995 | Acc: 57.690%\n",
      "\n",
      "Epoch 129/150\n",
      "[=================================================>] 782/782 | Loss: 0.446 | Acc: 86.324%\n",
      "[=================================================>] 157/157 | Loss: 2.011 | Acc: 57.510%\n",
      "\n",
      "Epoch 130/150\n",
      "[=================================================>] 782/782 | Loss: 0.440 | Acc: 86.790%\n",
      "[=================================================>] 157/157 | Loss: 1.972 | Acc: 58.120%\n",
      "\n",
      "Epoch 131/150\n",
      "[=================================================>] 782/782 | Loss: 0.432 | Acc: 87.032%\n",
      "[=================================================>] 157/157 | Loss: 2.024 | Acc: 57.860%\n",
      "\n",
      "Epoch 132/150\n",
      "[=================================================>] 782/782 | Loss: 0.422 | Acc: 87.120%\n",
      "[=================================================>] 157/157 | Loss: 2.015 | Acc: 58.020%\n",
      "\n",
      "Epoch 133/150\n",
      "[=================================================>] 782/782 | Loss: 0.411 | Acc: 87.662%\n",
      "[=================================================>] 157/157 | Loss: 2.038 | Acc: 57.990%\n",
      "\n",
      "Epoch 134/150\n",
      "[=================================================>] 782/782 | Loss: 0.405 | Acc: 87.632%\n",
      "[=================================================>] 157/157 | Loss: 2.024 | Acc: 57.800%\n",
      "\n",
      "Epoch 135/150\n",
      "[=================================================>] 782/782 | Loss: 0.399 | Acc: 87.938%\n",
      "[=================================================>] 157/157 | Loss: 2.008 | Acc: 58.090%\n",
      "\n",
      "Epoch 136/150\n",
      "[=================================================>] 782/782 | Loss: 0.393 | Acc: 88.182%\n",
      "[=================================================>] 157/157 | Loss: 2.016 | Acc: 57.980%\n",
      "\n",
      "Epoch 137/150\n",
      "[=================================================>] 782/782 | Loss: 0.391 | Acc: 88.206%\n",
      "[=================================================>] 157/157 | Loss: 2.044 | Acc: 57.660%\n",
      "\n",
      "Epoch 138/150\n",
      "[=================================================>] 782/782 | Loss: 0.383 | Acc: 88.328%\n",
      "[=================================================>] 157/157 | Loss: 2.030 | Acc: 57.970%\n",
      "\n",
      "Epoch 139/150\n",
      "[=================================================>] 782/782 | Loss: 0.382 | Acc: 88.560%\n",
      "[=================================================>] 157/157 | Loss: 2.030 | Acc: 58.130%\n",
      "\n",
      "Epoch 140/150\n",
      "[=================================================>] 782/782 | Loss: 0.376 | Acc: 88.832%\n",
      "[=================================================>] 157/157 | Loss: 2.048 | Acc: 57.910%\n",
      "\n",
      "Epoch 141/150\n",
      "[=================================================>] 782/782 | Loss: 0.368 | Acc: 88.930%\n",
      "[=================================================>] 157/157 | Loss: 2.054 | Acc: 57.670%\n",
      "\n",
      "Epoch 142/150\n",
      "[=================================================>] 782/782 | Loss: 0.372 | Acc: 88.758%\n",
      "[=================================================>] 157/157 | Loss: 2.025 | Acc: 58.170%\n",
      "\n",
      "Epoch 143/150\n",
      "[=================================================>] 782/782 | Loss: 0.365 | Acc: 89.012%\n",
      "[=================================================>] 157/157 | Loss: 2.035 | Acc: 58.120%\n",
      "\n",
      "Epoch 144/150\n",
      "[=================================================>] 782/782 | Loss: 0.359 | Acc: 88.920%\n",
      "[=================================================>] 157/157 | Loss: 2.041 | Acc: 58.020%\n",
      "\n",
      "Epoch 145/150\n",
      "[=================================================>] 782/782 | Loss: 0.367 | Acc: 88.910%\n",
      "[=================================================>] 157/157 | Loss: 2.037 | Acc: 57.830%\n",
      "\n",
      "Epoch 146/150\n",
      "[=================================================>] 782/782 | Loss: 0.357 | Acc: 89.374%\n",
      "[=================================================>] 157/157 | Loss: 2.033 | Acc: 58.000%\n",
      "\n",
      "Epoch 147/150\n",
      "[=================================================>] 782/782 | Loss: 0.358 | Acc: 89.086%\n",
      "[=================================================>] 157/157 | Loss: 2.038 | Acc: 57.680%\n",
      "\n",
      "Epoch 148/150\n",
      "[=================================================>] 782/782 | Loss: 0.356 | Acc: 89.364%\n",
      "[=================================================>] 157/157 | Loss: 2.034 | Acc: 57.950%\n",
      "\n",
      "Epoch 149/150\n",
      "[=================================================>] 782/782 | Loss: 0.361 | Acc: 89.134%\n",
      "[=================================================>] 157/157 | Loss: 2.055 | Acc: 57.860%\n",
      "\n",
      "Epoch 150/150\n",
      "[=================================================>] 782/782 | Loss: 0.352 | Acc: 89.410%\n",
      "[=================================================>] 157/157 | Loss: 2.025 | Acc: 58.080%\n",
      "\n",
      "==> Pruning Summary:\n",
      "Params: 34.02M => 18.66M (45.1% reduction)\n",
      "MACs: 0.33G => 0.04G (88.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ====== Cáº¥u hÃ¬nh cÆ¡ báº£n ======\n",
    "# Configure pruning\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B'\n",
    "config.prune_method = 'max'  # Prune least important filters\n",
    "config.prune_local = False\n",
    "config.batch_size = 64\n",
    "\n",
    "num_filters_to_prune = 512 * 6\n",
    "epochs_after_prune = 150\n",
    "\n",
    "# Cho phÃ©p class Ä‘Æ°á»£c load (náº¿u dÃ¹ng torch.load model nguyÃªn khá»‘i)\n",
    "torch.serialization.add_safe_globals([ModifiedVGG16Model])\n",
    "\n",
    "# ====== Load checkpoint tá»‘t nháº¥t ======\n",
    "checkpoint_path = \"./checkpoint/vgg16_cifar100_baseline.pth\"\n",
    "\n",
    "# 1ï¸âƒ£ Táº¡o láº¡i model trá»‘ng cÃ¹ng cáº¥u trÃºc\n",
    "model = ModifiedVGG16Model(config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# 2ï¸âƒ£ Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "\n",
    "# 3ï¸âƒ£ GÃ¡n láº¡i trá»ng sá»‘ vÃ o model\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# 4ï¸âƒ£ GÃ¡n láº¡i thÃ´ng tin acc vÃ  epoch\n",
    "best_acc = checkpoint.get(\"acc\", 0.0)\n",
    "start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "print(f\"âœ… Loaded checkpoint from epoch {start_epoch} with best acc = {best_acc:.4f}\")\n",
    "\n",
    "# ====== Táº¡o pruner vÃ  thá»±c hiá»‡n pruning ======\n",
    "pruner = VGG16Pruner(config, model, save_name='100_pruned_max_70_ckpt.pth')\n",
    "pruner.prune(num_filters_to_prune=num_filters_to_prune, epochs_after_prune=epochs_after_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa96cc2",
   "metadata": {},
   "source": [
    "## 5. GAPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "class GAPruner:\n",
    "    def __init__(self, model, config, input_size):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # 1. Base Metrics Calculation\n",
    "        self.model.eval()\n",
    "        self.base_macs, self.base_params = tp.utils.count_ops_and_params(self.model, input_size)\n",
    "        print(f\"Base MACs: {self.base_macs/1e9:.4f}G | Base Params: {self.base_params/1e6:.4f}M\")\n",
    "        \n",
    "        # 2. Extract Layer Information\n",
    "        self._extract_layer_info()\n",
    "        \n",
    "        # 3. Storage for History\n",
    "        self.history = {\n",
    "            'generation': [], \n",
    "            'best_fitness': [], \n",
    "            'avg_fitness': [], \n",
    "            'best_macs': [], \n",
    "            'best_params': [],\n",
    "            'best_chroms': []\n",
    "        }\n",
    "        self.ranking_tables = {} \n",
    "        \n",
    "    def _extract_layer_info(self):\n",
    "        \"\"\"Extract H, W, K, Cin, Cout for fast MACs/Params estimation\"\"\"\n",
    "        self.conv_layers = [] \n",
    "        self.layer_info = []  \n",
    "        \n",
    "        dummy_input = self.input_size.to(self.device)\n",
    "        hooks = []\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            x = input[0]\n",
    "            self.layer_info.append({\n",
    "                'h': x.shape[2],\n",
    "                'w': x.shape[3],\n",
    "                'k': module.kernel_size[0],\n",
    "                'cin': module.in_channels,\n",
    "                'cout': module.out_channels,\n",
    "                'bias': module.bias is not None\n",
    "            })\n",
    "            \n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                self.conv_layers.append((name, module))\n",
    "                hooks.append(module.register_forward_hook(hook_fn))\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            self.model(dummy_input)\n",
    "            \n",
    "        for h in hooks: h.remove()\n",
    "        print(f\"Analyzed {len(self.conv_layers)} Conv layers for fast estimation.\")\n",
    "    \n",
    "    def build_ranking_tables(self, pruner_helper, train_loader=None):\n",
    "        \"\"\"\n",
    "        Build ranking tables with:\n",
    "        - Layer-wise normalized scores (for Min strategy)\n",
    "        - Global median-rank reference (for Median strategy)\n",
    "        \"\"\"\n",
    "        print(\"\\nStep 1: Building Ranking Tables (Pre-computation)...\")\n",
    "\n",
    "        # 1. Get real pruner\n",
    "        if hasattr(pruner_helper, 'pruner'):\n",
    "            real_pruner = pruner_helper.pruner\n",
    "        else:\n",
    "            real_pruner = pruner_helper\n",
    "\n",
    "        real_pruner.reset()\n",
    "\n",
    "        print(\"   Running ranking epoch (accumulating gradients)...\")\n",
    "        pruner_helper.train_epoch(rank_filters=True)\n",
    "        real_pruner.normalize_ranks_per_layer()\n",
    "\n",
    "        raw_ranks = real_pruner.filter_ranks\n",
    "\n",
    "        # Init containers\n",
    "        self.ranking_tables = {'min': {}, 'avg': {}}\n",
    "        self.layer_stats = {}\n",
    "\n",
    "        collected_data = []\n",
    "\n",
    "        print(f\"\\nðŸ“Š --- SMART MATCHING LAYERS ---\")\n",
    "        print(f\"   Raw ranks key type: {type(list(raw_ranks.keys())[0])}\")\n",
    "\n",
    "        # 2. Process each layer\n",
    "        for key, ranks_tensor in raw_ranks.items():\n",
    "            ranks = ranks_tensor.detach().cpu().numpy()\n",
    "            matched_idx = -1\n",
    "\n",
    "            # Case 1: index-based\n",
    "            if isinstance(key, int):\n",
    "                if key < len(self.conv_layers):\n",
    "                    matched_idx = key\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Case 2: module-based\n",
    "            elif isinstance(key, nn.Module):\n",
    "                for my_idx, (_, my_module) in enumerate(self.conv_layers):\n",
    "                    if my_module.weight.shape != key.weight.shape:\n",
    "                        continue\n",
    "                    if torch.allclose(\n",
    "                        my_module.weight.view(-1)[:100].cpu(),\n",
    "                        key.weight.view(-1)[:100].cpu(),\n",
    "                        atol=1e-5\n",
    "                    ):\n",
    "                        matched_idx = my_idx\n",
    "                        break\n",
    "\n",
    "            if matched_idx == -1:\n",
    "                continue\n",
    "\n",
    "            # --- NEW: Layer-wise normalization ---\n",
    "            max_score = np.max(ranks)\n",
    "            norm_scores = ranks / (max_score + 1e-8)\n",
    "\n",
    "            # Save stats for fitness\n",
    "            self.layer_stats[matched_idx] = {\n",
    "                'raw': ranks,\n",
    "                'norm': norm_scores,\n",
    "                'max': max_score,\n",
    "                'mean': np.mean(ranks)\n",
    "            }\n",
    "\n",
    "            # Min strategy = layer-wise sensitivity\n",
    "            self.ranking_tables['min'][matched_idx] = np.sort(norm_scores)\n",
    "\n",
    "            collected_data.append({\n",
    "                'layer_idx': matched_idx,\n",
    "                'num_filters': len(ranks),\n",
    "                'scores': ranks\n",
    "            })\n",
    "\n",
    "            if matched_idx < 3 or matched_idx > 10:\n",
    "                print(f\"   Layer {matched_idx}: max={max_score:.4f}, mean={np.mean(ranks):.4f}\")\n",
    "\n",
    "        # 3. Build GLOBAL median-rank reference\n",
    "        all_norm_scores = np.concatenate(\n",
    "            [v['norm'] for v in self.layer_stats.values()]\n",
    "        )\n",
    "        global_sorted = np.sort(all_norm_scores)\n",
    "\n",
    "        for idx in self.layer_stats.keys():\n",
    "            self.ranking_tables['avg'][idx] = global_sorted\n",
    "\n",
    "        # Sort output for plotting / analysis\n",
    "        collected_data.sort(key=lambda x: x['layer_idx'])\n",
    "\n",
    "        print(f\"âœ… Successfully built ranking tables for {len(collected_data)} layers.\")\n",
    "        return collected_data\n",
    "\n",
    "    def _estimate_metrics(self, channels_list):\n",
    "        \"\"\"Fast estimation of both MACs and Params\"\"\"\n",
    "        total_macs = 0\n",
    "        total_params = 0\n",
    "        \n",
    "        for i, info in enumerate(self.layer_info):\n",
    "            if i == 0:\n",
    "                c_in = info['cin']\n",
    "            else:\n",
    "                c_in = channels_list[i-1]\n",
    "            \n",
    "            c_out = channels_list[i]\n",
    "            \n",
    "            macs = (info['h'] * info['w'] * info['k'] * info['k'] * c_in * c_out)\n",
    "            total_macs += macs\n",
    "            \n",
    "            params = (info['k'] * info['k'] * c_in * c_out)\n",
    "            if info['bias']:\n",
    "                params += c_out\n",
    "            total_params += params\n",
    "            \n",
    "        return total_macs, total_params\n",
    "\n",
    "    def _decode_chromosome(self, chrom):\n",
    "        channels = []\n",
    "        strategies = []\n",
    "\n",
    "        n_layers = len(self.conv_layers)\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            r = chrom[i*2]\n",
    "            s = chrom[i*2 + 1]\n",
    "\n",
    "            original_filters = self.layer_info[i]['cout']\n",
    "            n_filters = int(original_filters * r)\n",
    "\n",
    "            # --- HARD constraints ---\n",
    "            min_ratio = 0.1\n",
    "            n_filters = max(int(original_filters * min_ratio), n_filters)\n",
    "            n_filters = max(8, round(n_filters / 8) * 8)\n",
    "            n_filters = min(n_filters, original_filters)\n",
    "\n",
    "            channels.append(n_filters)\n",
    "            strategies.append('min' if s < 0.5 else 'avg')\n",
    "\n",
    "        return channels, strategies\n",
    "\n",
    "    def _calc_fitness(self, chrom, target_macs_ratio):\n",
    "        channels, strategies = self._decode_chromosome(chrom)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 1. Resource estimation\n",
    "        # --------------------------------------------------\n",
    "        est_macs, est_params = self._estimate_metrics(channels)\n",
    "        macs_ratio = est_macs / self.base_macs\n",
    "        params_ratio = est_params / self.base_params\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 2. Taylor-based Proxy Fitness\n",
    "        # --------------------------------------------------\n",
    "        total_score = 0.0\n",
    "        total_max = 0.0\n",
    "\n",
    "        for i, n_keep in enumerate(channels):\n",
    "            strat = strategies[i]\n",
    "\n",
    "            if strat == 'min':\n",
    "                # Layer-wise sensitivity\n",
    "                ranks = self.ranking_tables['min'][i]\n",
    "                preserved = np.sum(ranks[-n_keep:])\n",
    "                layer_total = np.sum(ranks)\n",
    "\n",
    "            else:\n",
    "                # GLOBAL median-rank strategy\n",
    "                global_ranks = self.ranking_tables['avg'][i]\n",
    "                start = int((len(global_ranks) - n_keep) / 2)\n",
    "                preserved = np.sum(global_ranks[start:start + n_keep])\n",
    "                layer_total = np.sum(global_ranks)\n",
    "\n",
    "            total_score += preserved\n",
    "            total_max += layer_total\n",
    "\n",
    "        base_fitness = total_score / (total_max + 1e-8)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 3. Structure-aware safeguard (score-driven)\n",
    "        # --------------------------------------------------\n",
    "        structure_penalty = 0.0\n",
    "\n",
    "        global_max_score = max(v['max'] for v in self.layer_stats.values())\n",
    "\n",
    "        for i, c in enumerate(channels):\n",
    "            orig = self.layer_info[i]['cout']\n",
    "            ratio = c / orig\n",
    "\n",
    "            layer_max = self.layer_stats[i]['max']\n",
    "            attenuation = layer_max / (global_max_score + 1e-8)\n",
    "\n",
    "            # Protect layers with severe score attenuation\n",
    "            if attenuation < 0.2 and ratio < 0.4:\n",
    "                structure_penalty += 3.0 * ((0.4 - ratio) ** 2)\n",
    "\n",
    "            # HARD safeguard: forbid collapse\n",
    "            if ratio < 0.1:\n",
    "                return -1e6  # infeasible solution\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 4. MACs soft constraint\n",
    "        # --------------------------------------------------\n",
    "        macs_penalty = 0.0\n",
    "        if macs_ratio > target_macs_ratio:\n",
    "            v = macs_ratio - target_macs_ratio\n",
    "            macs_penalty = 10.0 * v + 20.0 * (v ** 2)\n",
    "        else:\n",
    "            macs_penalty = -0.05 * (target_macs_ratio - macs_ratio)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 5. Final Fitness\n",
    "        # --------------------------------------------------\n",
    "        fitness = (\n",
    "            base_fitness\n",
    "            - macs_penalty\n",
    "            - structure_penalty\n",
    "            - 0.1 * params_ratio\n",
    "        )\n",
    "\n",
    "        return fitness\n",
    "    \n",
    "    def proxy_capacity_score(self, channels, strategies):\n",
    "        \"\"\"\n",
    "        Proxy Accuracy:\n",
    "        - Preserve Taylor importance\n",
    "        - Strategy-aware\n",
    "        - Layer-balanced\n",
    "        Higher is better\n",
    "        \"\"\"\n",
    "        total_score = 0.0\n",
    "\n",
    "        global_max = max(v['max'] for v in self.layer_stats.values()) + 1e-8\n",
    "\n",
    "        for i, keep in enumerate(channels):\n",
    "            stats = self.layer_stats[i]\n",
    "            raw = stats['raw']\n",
    "\n",
    "            sorted_idx = np.argsort(raw)[::-1]  # strong â†’ weak\n",
    "            kept_idx = sorted_idx[:keep]\n",
    "\n",
    "            preserved = raw[kept_idx].sum()\n",
    "            total = raw.sum() + 1e-8\n",
    "\n",
    "            layer_score = preserved / total\n",
    "\n",
    "            # -------- strategy effect --------\n",
    "            if strategies[i] == 'avg':\n",
    "                layer_score *= 1.05   # á»•n Ä‘á»‹nh hÆ¡n\n",
    "            else:\n",
    "                layer_score *= 0.95   # min hÆ¡i risky\n",
    "\n",
    "            # -------- protect important layers --------\n",
    "            attenuation = stats['max'] / global_max\n",
    "            if attenuation < 0.25 and keep / len(raw) < 0.4:\n",
    "                layer_score *= 0.8\n",
    "\n",
    "            total_score += layer_score\n",
    "\n",
    "        return total_score / len(channels)\n",
    "\n",
    "    def evolve(self, pop_size=50, n_generations=50, target_macs_ratio=0.5):\n",
    "        print(f\"\\nðŸš€ Starting GA Evolution... Target MACs <= {target_macs_ratio*100}%\")\n",
    "        n_layers = len(self.conv_layers)\n",
    "        \n",
    "        # [FIX] Reset history má»—i láº§n cháº¡y evolve Ä‘á»ƒ khÃ´ng bá»‹ chá»“ng láº¥n dá»¯ liá»‡u cÅ©\n",
    "        self.history = {\n",
    "            'generation': [], 'best_fitness': [], 'avg_fitness': [], \n",
    "            'best_macs': [], 'best_params': [], 'best_chroms': []\n",
    "        }\n",
    "\n",
    "        # Init Population\n",
    "        population = np.random.rand(pop_size, n_layers * 2)\n",
    "        \n",
    "        # [FIX] Logic khá»Ÿi táº¡o vÃ¹ng tháº¥p náº¿u target gáº¯t (< 20%)\n",
    "        if target_macs_ratio < 0.2:\n",
    "            print(\"âš ï¸ Strict target detected! Initializing population in low-ratio region.\")\n",
    "            population[:, ::2] = np.random.uniform(0.05, 0.4, (pop_size, n_layers))\n",
    "        else:\n",
    "            population[:, ::2] = np.random.uniform(0.3, 1.0, (pop_size, n_layers))\n",
    "        \n",
    "        best_chrom = None\n",
    "        best_fitness_ever = -100\n",
    "        \n",
    "        for gen in range(n_generations):\n",
    "            fitnesses = []\n",
    "            for chrom in population:\n",
    "                f = self._calc_fitness(chrom, target_macs_ratio)\n",
    "                fitnesses.append(f)\n",
    "            \n",
    "            fitnesses = np.array(fitnesses)\n",
    "            indices = np.argsort(fitnesses)[::-1]\n",
    "            population = population[indices]\n",
    "            fitnesses = fitnesses[indices]\n",
    "            \n",
    "            # [QUAN TRá»ŒNG] LÆ°u láº¡i Best Chromosome cá»§a Gen nÃ y Ä‘á»ƒ phá»¥c vá»¥ tÃ­nh toÃ¡n náº¿u cáº§n.\n",
    "            self.history['best_chroms'].append(copy.deepcopy(population[0]))\n",
    "\n",
    "            current_best_fit = fitnesses[0]\n",
    "            if current_best_fit > best_fitness_ever:\n",
    "                best_fitness_ever = current_best_fit\n",
    "                best_chrom = copy.deepcopy(population[0])\n",
    "                \n",
    "            # Logging\n",
    "            best_channels, _ = self._decode_chromosome(population[0])\n",
    "            best_macs, best_params = self._estimate_metrics(best_channels)\n",
    "            \n",
    "            self.history['generation'].append(gen)\n",
    "            self.history['best_fitness'].append(current_best_fit)\n",
    "            self.history['avg_fitness'].append(np.mean(fitnesses[fitnesses > -1]))\n",
    "            self.history['best_macs'].append(best_macs / 1e9)\n",
    "            self.history['best_params'].append(best_params / 1e6)\n",
    "            \n",
    "            print(f\"Gen {gen+1}/{n_generations} | Fit: {current_best_fit:.4f} | MACs: {best_macs/1e9:.2f}G | Params: {best_params/1e6:.2f}M\")\n",
    "            \n",
    "            # [FIX] Gaussian Mutation Ä‘á»ƒ tinh chá»‰nh tá»‘t hÆ¡n\n",
    "            n_elite = int(pop_size * 0.2)\n",
    "            new_pop = [population[i] for i in range(n_elite)]\n",
    "            \n",
    "            while len(new_pop) < pop_size:\n",
    "                # Tournament Selection\n",
    "                idx1, idx2 = np.random.randint(0, pop_size // 2, 2)\n",
    "                p1 = population[idx1]\n",
    "                idx3, idx4 = np.random.randint(0, pop_size // 2, 2)\n",
    "                p2 = population[idx3]\n",
    "                \n",
    "                # Crossover\n",
    "                mask = np.random.rand(len(p1)) > 0.5\n",
    "                child = np.where(mask, p1, p2)\n",
    "                \n",
    "                # Mutation (Gaussian cho Ratio, Flip cho Strategy)\n",
    "                if np.random.rand() < 0.1:\n",
    "                    idx = np.random.randint(0, len(child))\n",
    "                    if idx % 2 == 0: # Gen Ratio\n",
    "                        noise = np.random.normal(0, 0.1)\n",
    "                        child[idx] = np.clip(child[idx] + noise, 0.05, 1.0)\n",
    "                    else: # Gen Strategy\n",
    "                        child[idx] = 1.0 - child[idx] # Äáº£o ngÆ°á»£c strategy\n",
    "                        \n",
    "                new_pop.append(child)\n",
    "                \n",
    "            population = np.array(new_pop)\n",
    "            \n",
    "        print(\"\\nðŸ† Evolution Finished.\")\n",
    "        return best_chrom\n",
    "\n",
    "    def plot_results(self, best_chrom):\n",
    "        \"\"\"Generate plots\"\"\"\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.set_xlabel('Generation')\n",
    "        ax1.set_ylabel('Fitness', color='tab:blue')\n",
    "        ax1.plot(self.history['generation'], self.history['best_fitness'], color='tab:blue', label='Fitness')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('MACs (G)', color='tab:red')\n",
    "        ax2.plot(self.history['generation'], self.history['best_macs'], color='tab:red', linestyle='--', label='MACs')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "        \n",
    "        plt.title('GA Evolution History')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Strategy Dist\n",
    "        channels, strategies = self._decode_chromosome(best_chrom)\n",
    "        min_c = strategies.count('min')\n",
    "        avg_c = strategies.count('avg')\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.pie([min_c, avg_c], labels=['Min', 'Avg'], autopct='%1.1f%%', colors=['#ff9999','#66b3ff'])\n",
    "        plt.title('Strategy Distribution')\n",
    "        plt.show()\n",
    "\n",
    "    def apply_best_pruning(self, best_chrom):\n",
    "        print(\"\\nâœ‚ï¸ Pruning Plan:\")\n",
    "        channels, strategies = self._decode_chromosome(best_chrom)\n",
    "        plan = []\n",
    "        for i, (name, module) in enumerate(self.conv_layers):\n",
    "            plan.append({'layer_idx': i, 'n_keep': channels[i], 'strategy': strategies[i]})\n",
    "            print(f\"Layer {i}: Keep {channels[i]} ({strategies[i]})\")\n",
    "        return plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7d46f",
   "metadata": {},
   "source": [
    "### 5.1 Building Ranking Tables (Pre-computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48231f83",
   "metadata": {},
   "source": [
    "### 5.1.1. CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3314eacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model from epoch 238 with Acc: 72.78%\n",
      "Base MACs: 0.3337G | Base Params: 34.0154M\n",
      "Analyzed 13 Conv layers for fast estimation.\n",
      "\n",
      "Step 1: Building Ranking Tables (Pre-computation)...\n",
      "   Running ranking epoch (accumulating gradients)...\n",
      "\n",
      "ðŸ“Š --- SMART MATCHING LAYERS ---\n",
      "   Raw ranks key type: <class 'int'>\n",
      "   Layer 12: max=0.0789, mean=0.0430\n",
      "   Layer 11: max=0.1586, mean=0.0349\n",
      "   Layer 2: max=0.2966, mean=0.0670\n",
      "   Layer 1: max=0.5181, mean=0.0892\n",
      "   Layer 0: max=0.4035, mean=0.0778\n",
      "âœ… Successfully built ranking tables for 13 layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'layer_idx': 0,\n",
       "  'num_filters': 64,\n",
       "  'scores': array([0.00118699, 0.04366034, 0.00477327, 0.29226753, 0.40346605,\n",
       "         0.03922206, 0.17713046, 0.03280242, 0.0540837 , 0.02109884,\n",
       "         0.08339912, 0.13890722, 0.01832195, 0.02418005, 0.01817449,\n",
       "         0.01856743, 0.04939338, 0.01060051, 0.04585752, 0.12386014,\n",
       "         0.16573389, 0.02990702, 0.01369759, 0.10129495, 0.2217365 ,\n",
       "         0.00529693, 0.02388143, 0.03008626, 0.05379191, 0.00859108,\n",
       "         0.07120577, 0.34752074, 0.03951694, 0.03981584, 0.0661987 ,\n",
       "         0.03974599, 0.04968381, 0.01851748, 0.37179703, 0.06131101,\n",
       "         0.09670813, 0.09460011, 0.00240925, 0.3534191 , 0.01420081,\n",
       "         0.04121916, 0.02861835, 0.25131106, 0.01715607, 0.01547737,\n",
       "         0.00363448, 0.01826376, 0.0018551 , 0.00214145, 0.00998633,\n",
       "         0.03256943, 0.00650114, 0.06187624, 0.11613344, 0.00677078,\n",
       "         0.15737325, 0.07542453, 0.02847013, 0.18069986], dtype=float32)},\n",
       " {'layer_idx': 1,\n",
       "  'num_filters': 64,\n",
       "  'scores': array([0.0921232 , 0.2564546 , 0.01190277, 0.01194087, 0.01728134,\n",
       "         0.05711416, 0.08155842, 0.06792367, 0.06521033, 0.11080775,\n",
       "         0.07765346, 0.09362501, 0.306116  , 0.03493277, 0.18057495,\n",
       "         0.01439624, 0.11544502, 0.00738811, 0.02997709, 0.04959539,\n",
       "         0.05175734, 0.06678794, 0.00912844, 0.0114221 , 0.1531924 ,\n",
       "         0.24681395, 0.03922143, 0.04903503, 0.51814526, 0.08516871,\n",
       "         0.22639497, 0.04105336, 0.26297316, 0.15460625, 0.17240888,\n",
       "         0.04218528, 0.03698127, 0.01937091, 0.2275977 , 0.02777345,\n",
       "         0.05702447, 0.08092412, 0.07956588, 0.0207077 , 0.01612844,\n",
       "         0.08589626, 0.06091436, 0.0213997 , 0.04872916, 0.05719465,\n",
       "         0.08175693, 0.04720495, 0.13400643, 0.08684462, 0.137116  ,\n",
       "         0.07657856, 0.01718795, 0.08427259, 0.0692015 , 0.09300359,\n",
       "         0.03032611, 0.11574544, 0.03741809, 0.04422789], dtype=float32)},\n",
       " {'layer_idx': 2,\n",
       "  'num_filters': 128,\n",
       "  'scores': array([1.95551310e-02, 4.99242470e-02, 2.17642002e-02, 5.33920005e-02,\n",
       "         4.11630981e-02, 5.91688678e-02, 1.32430121e-01, 2.67809927e-02,\n",
       "         2.73317154e-02, 4.79064807e-02, 9.44886804e-02, 4.00303341e-02,\n",
       "         8.04779828e-02, 1.77949090e-02, 4.52676713e-02, 3.80914584e-02,\n",
       "         1.04338273e-01, 2.11910263e-01, 3.53845917e-02, 6.34621531e-02,\n",
       "         5.10768592e-02, 8.45931843e-03, 7.22689405e-02, 4.59781289e-03,\n",
       "         1.17392763e-02, 4.18166965e-02, 7.13605946e-03, 5.58158234e-02,\n",
       "         1.19499803e-01, 5.42367948e-03, 1.15706176e-01, 5.63510172e-02,\n",
       "         7.32829124e-02, 3.31325666e-03, 2.42537379e-01, 2.42427923e-02,\n",
       "         1.63137037e-02, 4.12059873e-02, 1.25115573e-01, 2.51821633e-02,\n",
       "         9.65790674e-02, 1.92358289e-02, 4.22327965e-02, 2.04583898e-01,\n",
       "         4.10411023e-02, 7.78427050e-02, 2.77589578e-02, 5.24733104e-02,\n",
       "         1.43979937e-02, 1.24987185e-01, 3.12227141e-02, 1.15578426e-02,\n",
       "         8.07948411e-02, 8.17574263e-02, 8.62363055e-02, 7.10010231e-02,\n",
       "         1.45327941e-01, 5.94050586e-02, 7.97977746e-02, 1.96296558e-01,\n",
       "         1.08451717e-01, 1.25562683e-01, 3.11002582e-02, 6.94641396e-02,\n",
       "         6.44035935e-02, 1.71396539e-01, 6.18176013e-02, 1.22361481e-01,\n",
       "         7.36246035e-02, 1.81498155e-01, 9.46701244e-02, 1.73787698e-02,\n",
       "         7.57820010e-02, 1.81866363e-02, 3.88125516e-02, 1.01788156e-01,\n",
       "         1.52918231e-02, 1.30416183e-02, 6.97009787e-02, 2.21847440e-04,\n",
       "         7.09848851e-02, 7.22856596e-02, 5.60519211e-02, 1.49155393e-01,\n",
       "         3.59769422e-03, 6.54911324e-02, 8.08603875e-03, 3.76807386e-03,\n",
       "         6.71476871e-02, 1.80162992e-02, 1.05509609e-02, 2.96580106e-01,\n",
       "         8.95370394e-02, 1.13860399e-01, 1.00951977e-01, 4.23166864e-02,\n",
       "         4.78672534e-02, 4.69565168e-02, 1.15726247e-01, 4.53151297e-03,\n",
       "         5.85090294e-02, 1.06317438e-01, 5.34911565e-02, 1.19401505e-02,\n",
       "         5.24102002e-02, 8.02089646e-02, 2.30971932e-01, 2.05047801e-03,\n",
       "         3.96764986e-02, 3.55013274e-02, 1.12876773e-01, 1.04844123e-02,\n",
       "         4.21837065e-03, 6.59845173e-02, 2.58831494e-02, 1.35211766e-01,\n",
       "         2.55110133e-02, 9.33334604e-03, 7.56707639e-02, 8.75721797e-02,\n",
       "         4.08858687e-05, 8.11612904e-02, 1.35927796e-01, 8.60772133e-02,\n",
       "         4.68443111e-02, 1.61314104e-02, 2.69498855e-01, 2.22249534e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 3,\n",
       "  'num_filters': 128,\n",
       "  'scores': array([0.03562708, 0.04618753, 0.1841735 , 0.1555981 , 0.12286861,\n",
       "         0.17263097, 0.05669951, 0.0609052 , 0.11559705, 0.0631045 ,\n",
       "         0.10849253, 0.03178129, 0.10909569, 0.16110452, 0.00720902,\n",
       "         0.06805029, 0.01822935, 0.04882546, 0.09637084, 0.07656915,\n",
       "         0.02362991, 0.04260417, 0.03258459, 0.0488406 , 0.05771669,\n",
       "         0.03588651, 0.05803788, 0.16981925, 0.19048618, 0.04793712,\n",
       "         0.15116264, 0.07029062, 0.1336439 , 0.03301923, 0.01668657,\n",
       "         0.00785577, 0.11122914, 0.01576769, 0.0486679 , 0.05511273,\n",
       "         0.06686165, 0.14007828, 0.00461878, 0.04260882, 0.04574256,\n",
       "         0.0035227 , 0.01782266, 0.04598958, 0.08375791, 0.15905479,\n",
       "         0.08971925, 0.06381682, 0.06033228, 0.11451862, 0.00116666,\n",
       "         0.02950904, 0.0746707 , 0.03220589, 0.02188712, 0.13413043,\n",
       "         0.05136398, 0.12844792, 0.14249231, 0.09664626, 0.00369511,\n",
       "         0.04025787, 0.10277715, 0.01875493, 0.10987425, 0.16347215,\n",
       "         0.03196196, 0.06545915, 0.09046252, 0.02945993, 0.05180413,\n",
       "         0.04831952, 0.04067315, 0.08118534, 0.14473678, 0.04672834,\n",
       "         0.09653115, 0.1967198 , 0.09819523, 0.03333998, 0.10345152,\n",
       "         0.06328995, 0.04163109, 0.05914752, 0.01140327, 0.04158919,\n",
       "         0.17319624, 0.02938439, 0.12014515, 0.06624836, 0.02707152,\n",
       "         0.0606663 , 0.00854619, 0.00959658, 0.06630416, 0.05848065,\n",
       "         0.03235571, 0.01384491, 0.055234  , 0.03611038, 0.18170115,\n",
       "         0.02037337, 0.18851855, 0.06003444, 0.11138912, 0.03344852,\n",
       "         0.02586119, 0.02803918, 0.19890171, 0.15729867, 0.12586813,\n",
       "         0.094674  , 0.07554356, 0.03614644, 0.11104555, 0.0803608 ,\n",
       "         0.10228419, 0.02462669, 0.03042459, 0.03787654, 0.06496176,\n",
       "         0.11029421, 0.01123475, 0.02514249], dtype=float32)},\n",
       " {'layer_idx': 4,\n",
       "  'num_filters': 256,\n",
       "  'scores': array([5.21483384e-02, 1.76667720e-01, 4.40535657e-02, 1.12011604e-01,\n",
       "         7.11729154e-02, 4.84993123e-03, 3.10383867e-02, 7.67528489e-02,\n",
       "         3.56813036e-02, 1.44061759e-01, 7.44605958e-02, 3.08621880e-02,\n",
       "         2.03128997e-02, 4.36440157e-03, 7.10298074e-03, 3.91995162e-02,\n",
       "         5.51773123e-02, 4.62552533e-03, 8.40285271e-02, 1.43637452e-02,\n",
       "         7.30515048e-02, 1.39724165e-02, 2.68914029e-02, 1.05001284e-02,\n",
       "         5.37574030e-02, 1.28641119e-02, 1.01065546e-01, 1.25983246e-02,\n",
       "         1.24700353e-01, 5.44856861e-02, 1.17005683e-01, 4.27105688e-02,\n",
       "         8.38804338e-03, 2.28403091e-01, 6.01179786e-02, 1.04269028e-01,\n",
       "         6.36632517e-02, 3.84121686e-02, 3.94475758e-02, 2.61011776e-02,\n",
       "         2.00210772e-02, 2.07451731e-02, 2.21710608e-04, 9.63655934e-02,\n",
       "         7.28098396e-03, 1.30995931e-02, 1.11482315e-01, 9.43161175e-02,\n",
       "         1.13756947e-01, 2.12942762e-03, 2.08003521e-02, 3.57449315e-02,\n",
       "         3.03668343e-02, 7.41365701e-02, 3.91231244e-03, 5.42078763e-02,\n",
       "         4.01218422e-02, 1.58259775e-02, 4.61939350e-02, 5.86775951e-02,\n",
       "         3.70820425e-02, 1.07423857e-01, 9.60991085e-02, 1.68349016e-02,\n",
       "         6.37024790e-02, 7.21319988e-02, 1.49721680e-02, 5.45981787e-02,\n",
       "         9.58713877e-04, 7.81738833e-02, 1.27638923e-02, 4.19641137e-02,\n",
       "         1.43126532e-01, 1.41615802e-02, 6.91024214e-02, 9.70018134e-02,\n",
       "         4.61172722e-02, 5.71100367e-03, 1.57659557e-02, 8.46540481e-02,\n",
       "         9.40913558e-02, 2.41565090e-02, 9.11568850e-03, 4.10542218e-03,\n",
       "         4.93390374e-02, 5.80279268e-02, 3.30248731e-03, 1.72170773e-02,\n",
       "         1.08179294e-01, 2.09765434e-02, 3.18782101e-03, 5.44640273e-02,\n",
       "         2.13492531e-02, 3.63869732e-03, 1.01606108e-01, 1.19480424e-01,\n",
       "         5.91298193e-03, 1.38527192e-02, 4.58997786e-02, 7.94593245e-03,\n",
       "         2.33390983e-02, 3.81012680e-03, 7.26250932e-02, 6.43381253e-02,\n",
       "         3.37511636e-02, 4.94847558e-02, 6.59270138e-02, 9.42101981e-03,\n",
       "         2.83311289e-02, 3.22716637e-03, 1.13850031e-02, 9.64020118e-02,\n",
       "         4.19461774e-03, 4.27138582e-02, 1.58992922e-03, 6.23144722e-03,\n",
       "         2.61270944e-02, 6.09120429e-02, 2.66304798e-02, 4.37516980e-02,\n",
       "         7.74385557e-02, 2.53093261e-02, 6.11836417e-03, 4.18991374e-04,\n",
       "         4.13500033e-02, 9.23834741e-03, 1.79340839e-02, 1.46614239e-02,\n",
       "         1.92723162e-02, 5.62573671e-02, 2.39863563e-02, 1.48286931e-02,\n",
       "         2.39462163e-02, 4.19457704e-02, 7.21294135e-02, 7.82540664e-02,\n",
       "         1.41028725e-02, 4.11437266e-02, 3.05059589e-02, 9.97252762e-02,\n",
       "         6.32921830e-02, 1.70533042e-02, 1.57213607e-03, 8.77781361e-02,\n",
       "         1.16505446e-02, 9.61507186e-02, 1.15887977e-01, 6.67133629e-02,\n",
       "         2.34697796e-02, 3.43809277e-02, 2.02002659e-01, 5.11382855e-02,\n",
       "         2.88098864e-02, 1.27729438e-02, 2.92373281e-02, 5.52060902e-02,\n",
       "         2.71680057e-02, 7.07880333e-02, 3.17851491e-02, 2.06815377e-02,\n",
       "         7.13516995e-02, 5.52793778e-02, 2.33792746e-03, 7.39948377e-02,\n",
       "         2.52660662e-02, 1.08222999e-01, 1.75902948e-01, 9.08773914e-02,\n",
       "         3.65859047e-02, 2.89456956e-02, 7.82565102e-02, 1.01032443e-01,\n",
       "         7.31963962e-02, 5.28265424e-02, 4.36655022e-02, 5.56746013e-02,\n",
       "         4.64931652e-02, 1.32369213e-02, 1.46298967e-02, 2.96033956e-02,\n",
       "         1.14822708e-01, 5.90026937e-02, 3.44910286e-02, 1.82557479e-02,\n",
       "         4.92209867e-02, 6.14974760e-02, 1.61622576e-02, 6.84513361e-04,\n",
       "         3.98555472e-02, 7.93289617e-02, 9.61107239e-02, 9.28480700e-02,\n",
       "         5.56882508e-02, 1.18567564e-01, 6.45316318e-02, 3.12061589e-02,\n",
       "         7.09938928e-02, 9.09338053e-03, 2.74455212e-02, 2.62879161e-03,\n",
       "         5.73063679e-02, 5.05803935e-02, 9.43655614e-03, 7.25699142e-02,\n",
       "         3.26844747e-03, 3.65381800e-02, 8.20753630e-03, 3.50937508e-02,\n",
       "         5.58771677e-02, 4.14120071e-02, 6.92593902e-02, 1.58293739e-01,\n",
       "         1.07707776e-01, 1.09601870e-01, 2.75777746e-02, 1.51901394e-02,\n",
       "         4.82909866e-02, 5.92261739e-02, 5.00987694e-02, 3.21330614e-02,\n",
       "         1.08672947e-01, 1.61700577e-01, 2.99341567e-02, 6.54779449e-02,\n",
       "         2.42315680e-02, 2.94615477e-02, 1.85244251e-02, 1.57885291e-02,\n",
       "         8.61910805e-02, 8.15693289e-02, 6.42010421e-02, 7.43318768e-03,\n",
       "         3.09129637e-02, 7.09459633e-02, 2.52895220e-03, 1.97653528e-02,\n",
       "         7.78655289e-03, 4.29549180e-02, 7.84322247e-03, 2.67260652e-02,\n",
       "         2.91738976e-02, 3.31166806e-03, 6.97865635e-02, 3.95578742e-02,\n",
       "         1.05839660e-02, 5.30690961e-02, 7.09607005e-02, 1.14214778e-01,\n",
       "         6.17812984e-02, 1.32020200e-02, 4.29046489e-02, 3.47856358e-02,\n",
       "         5.77209629e-02, 4.88383509e-02, 9.71583650e-02, 6.26305193e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 5,\n",
       "  'num_filters': 256,\n",
       "  'scores': array([0.01187424, 0.02113912, 0.03764461, 0.0267687 , 0.09526014,\n",
       "         0.00451451, 0.06332143, 0.03151422, 0.05584646, 0.02317336,\n",
       "         0.0860629 , 0.01810031, 0.05163254, 0.04320494, 0.02166092,\n",
       "         0.11797652, 0.04241235, 0.10596895, 0.00735628, 0.02766633,\n",
       "         0.03994631, 0.01869324, 0.06407794, 0.04954992, 0.10415417,\n",
       "         0.0760624 , 0.1210101 , 0.06311244, 0.02561214, 0.08958861,\n",
       "         0.017252  , 0.05413004, 0.1281733 , 0.05362359, 0.04609575,\n",
       "         0.03632694, 0.05305957, 0.00563073, 0.0097367 , 0.02461407,\n",
       "         0.01597891, 0.00982485, 0.00439788, 0.04670281, 0.07602331,\n",
       "         0.0102942 , 0.04570851, 0.08737128, 0.025333  , 0.09716313,\n",
       "         0.10829835, 0.06965785, 0.01715687, 0.06831141, 0.02638096,\n",
       "         0.14936836, 0.08584331, 0.01244096, 0.00071254, 0.03085396,\n",
       "         0.07711208, 0.06428005, 0.02833387, 0.03947186, 0.02965778,\n",
       "         0.03252701, 0.02307835, 0.02829958, 0.15923724, 0.06464241,\n",
       "         0.02157029, 0.07681219, 0.03047094, 0.00503214, 0.01223269,\n",
       "         0.00251896, 0.0388465 , 0.00087687, 0.07841834, 0.16367005,\n",
       "         0.04250645, 0.06293081, 0.01519336, 0.04116175, 0.04120816,\n",
       "         0.06607036, 0.00392097, 0.03555245, 0.07858641, 0.01293863,\n",
       "         0.08130213, 0.13776965, 0.00424943, 0.00595089, 0.01416149,\n",
       "         0.07375756, 0.04355177, 0.14409846, 0.0101545 , 0.03662549,\n",
       "         0.04406973, 0.04135106, 0.01066751, 0.10290641, 0.05319454,\n",
       "         0.12533122, 0.02359702, 0.02592465, 0.06457806, 0.07839622,\n",
       "         0.00274396, 0.06386228, 0.08923122, 0.04066512, 0.17281124,\n",
       "         0.0246964 , 0.02547593, 0.0190519 , 0.05362323, 0.01774828,\n",
       "         0.04127814, 0.08184177, 0.05099131, 0.03031923, 0.02605982,\n",
       "         0.02129241, 0.08581746, 0.1128749 , 0.04613346, 0.05920399,\n",
       "         0.08812807, 0.01896184, 0.00983823, 0.09943723, 0.06072687,\n",
       "         0.09154462, 0.11240581, 0.06636348, 0.04622624, 0.00739652,\n",
       "         0.01350211, 0.03466214, 0.02928221, 0.04272121, 0.0381778 ,\n",
       "         0.02032314, 0.08263984, 0.0829124 , 0.0867397 , 0.07156619,\n",
       "         0.05470479, 0.07055469, 0.10371097, 0.09550086, 0.02416138,\n",
       "         0.11568386, 0.0107779 , 0.02202103, 0.08385315, 0.01346447,\n",
       "         0.00866281, 0.04656596, 0.03468854, 0.09471932, 0.0194039 ,\n",
       "         0.06187603, 0.0166208 , 0.0026142 , 0.02974519, 0.01828644,\n",
       "         0.08634476, 0.01608325, 0.05906201, 0.05960855, 0.06190246,\n",
       "         0.15781158, 0.0019054 , 0.00873995, 0.03440707, 0.07008215,\n",
       "         0.00771214, 0.06456933, 0.00161639, 0.02717484, 0.08517318,\n",
       "         0.03434667, 0.05037529, 0.00845917, 0.00993276, 0.05745465,\n",
       "         0.03726317, 0.0435922 , 0.0848107 , 0.00821215, 0.05332795,\n",
       "         0.1960222 , 0.0853033 , 0.03145901, 0.03445368, 0.02261637,\n",
       "         0.09879458, 0.03162843, 0.05347836, 0.02658315, 0.07123475,\n",
       "         0.01787642, 0.01974613, 0.02271421, 0.03786625, 0.08929914,\n",
       "         0.00037548, 0.0006313 , 0.10719316, 0.02760269, 0.04867343,\n",
       "         0.00974749, 0.0409236 , 0.05397719, 0.06005895, 0.02462498,\n",
       "         0.07364161, 0.10445279, 0.03529228, 0.11207136, 0.0997602 ,\n",
       "         0.01892735, 0.0016636 , 0.04107125, 0.16990416, 0.04313073,\n",
       "         0.00959043, 0.02961836, 0.05775009, 0.07530218, 0.02426987,\n",
       "         0.01651333, 0.00066768, 0.01725838, 0.03647956, 0.03841245,\n",
       "         0.01723388, 0.00739525, 0.01198346, 0.11844692, 0.03308991,\n",
       "         0.07857576, 0.04319896, 0.10081755, 0.03692525, 0.04614719,\n",
       "         0.01034832, 0.02973303, 0.10244205, 0.07755893, 0.03243124,\n",
       "         0.06077611], dtype=float32)},\n",
       " {'layer_idx': 6,\n",
       "  'num_filters': 256,\n",
       "  'scores': array([0.05715434, 0.07690065, 0.00535476, 0.01661372, 0.05657842,\n",
       "         0.06643131, 0.02017586, 0.04095392, 0.06180216, 0.03098827,\n",
       "         0.02283007, 0.00227807, 0.01424098, 0.05643884, 0.12781233,\n",
       "         0.04780694, 0.13645425, 0.05686089, 0.01207077, 0.01435662,\n",
       "         0.07255662, 0.01901739, 0.08767518, 0.06472782, 0.11265095,\n",
       "         0.00158114, 0.00052245, 0.01909428, 0.06114139, 0.00930907,\n",
       "         0.06547614, 0.02150166, 0.03533907, 0.06528023, 0.10064507,\n",
       "         0.05579626, 0.03593865, 0.08427689, 0.0840378 , 0.05090184,\n",
       "         0.06685437, 0.02249912, 0.01920863, 0.03380187, 0.0757765 ,\n",
       "         0.03918351, 0.04878179, 0.07162078, 0.00536536, 0.02055898,\n",
       "         0.09626888, 0.07292423, 0.00797736, 0.07158939, 0.03584208,\n",
       "         0.06405663, 0.05503033, 0.00436831, 0.00710224, 0.06020459,\n",
       "         0.03303018, 0.00414625, 0.01643139, 0.00861119, 0.01615629,\n",
       "         0.1200793 , 0.0127108 , 0.01711418, 0.04770084, 0.07488249,\n",
       "         0.02733655, 0.11061069, 0.08699387, 0.07790709, 0.02711073,\n",
       "         0.07300962, 0.03946152, 0.02342404, 0.0955999 , 0.05868278,\n",
       "         0.09450065, 0.01121782, 0.06741793, 0.08387472, 0.00518325,\n",
       "         0.03017317, 0.01107719, 0.03088461, 0.03365012, 0.10467724,\n",
       "         0.00819911, 0.04450179, 0.11075991, 0.061343  , 0.04701601,\n",
       "         0.00804572, 0.07780804, 0.04036787, 0.06826777, 0.0476893 ,\n",
       "         0.0028767 , 0.05067514, 0.0502961 , 0.00133131, 0.02651245,\n",
       "         0.02346289, 0.1185525 , 0.03437591, 0.00076656, 0.00546027,\n",
       "         0.14763652, 0.06460188, 0.0428059 , 0.1435289 , 0.03114248,\n",
       "         0.00279552, 0.07352491, 0.04337686, 0.09343462, 0.04648195,\n",
       "         0.0080894 , 0.0060988 , 0.06655353, 0.05595389, 0.0358541 ,\n",
       "         0.06439099, 0.04320431, 0.01423093, 0.12404217, 0.06908476,\n",
       "         0.05401053, 0.04813499, 0.00382464, 0.00939599, 0.00469038,\n",
       "         0.01288182, 0.04088498, 0.04764692, 0.05547941, 0.01793336,\n",
       "         0.12953565, 0.03043287, 0.02268884, 0.04340867, 0.09513266,\n",
       "         0.06155908, 0.07773074, 0.05614721, 0.01104392, 0.01800044,\n",
       "         0.09641791, 0.01521392, 0.04059439, 0.06779077, 0.01758783,\n",
       "         0.02901807, 0.0621676 , 0.08731019, 0.00425716, 0.06050865,\n",
       "         0.07006287, 0.0594341 , 0.06360312, 0.05306714, 0.01065265,\n",
       "         0.02411053, 0.01995702, 0.05041333, 0.03862462, 0.0628254 ,\n",
       "         0.00769042, 0.08752856, 0.00294019, 0.02083881, 0.12060724,\n",
       "         0.11054794, 0.09579534, 0.04420321, 0.00630189, 0.16622464,\n",
       "         0.05343865, 0.05399098, 0.02752348, 0.06235738, 0.00163973,\n",
       "         0.03531454, 0.04626482, 0.04113293, 0.08605952, 0.05763897,\n",
       "         0.09092909, 0.06480666, 0.01107778, 0.05109861, 0.12164738,\n",
       "         0.05444257, 0.09431189, 0.05963444, 0.00826369, 0.00492416,\n",
       "         0.02634581, 0.0316766 , 0.06698484, 0.01536258, 0.0499365 ,\n",
       "         0.03444698, 0.03224711, 0.01010469, 0.09006231, 0.03355935,\n",
       "         0.02959265, 0.02662117, 0.0003755 , 0.07947632, 0.0136446 ,\n",
       "         0.13477981, 0.01467384, 0.03763512, 0.03598806, 0.04796232,\n",
       "         0.00065591, 0.17739707, 0.07596374, 0.03191359, 0.04323679,\n",
       "         0.02692527, 0.08085874, 0.12294361, 0.00827212, 0.04839291,\n",
       "         0.00068722, 0.08947778, 0.03554917, 0.05437719, 0.01835987,\n",
       "         0.15280637, 0.02424053, 0.17565341, 0.09255715, 0.00625388,\n",
       "         0.0049947 , 0.06033563, 0.12268583, 0.09360075, 0.00516089,\n",
       "         0.04447813, 0.18016905, 0.02157167, 0.00647938, 0.02872431,\n",
       "         0.11643151, 0.04674373, 0.01978241, 0.07680339, 0.01232843,\n",
       "         0.00180582], dtype=float32)},\n",
       " {'layer_idx': 7,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([1.26900211e-01, 3.01345587e-02, 9.39485729e-02, 7.10648578e-03,\n",
       "         2.86344644e-02, 7.92121515e-03, 1.26569746e-02, 1.69914905e-02,\n",
       "         5.40002286e-02, 1.03973057e-02, 1.75254811e-02, 7.40370387e-03,\n",
       "         4.84514795e-02, 1.90244988e-02, 4.52918075e-02, 8.00064057e-02,\n",
       "         1.11218709e-02, 6.85360655e-02, 1.81805191e-03, 3.07106078e-02,\n",
       "         1.82519909e-02, 1.52451079e-02, 1.88856777e-02, 1.13712139e-01,\n",
       "         8.48541874e-03, 5.90398209e-03, 1.43667858e-03, 1.92472860e-02,\n",
       "         2.39043403e-02, 9.01960954e-03, 6.39862716e-02, 4.01716307e-03,\n",
       "         2.06085164e-02, 1.98274106e-02, 7.80918300e-02, 1.78476498e-02,\n",
       "         3.67619768e-02, 1.97795928e-02, 4.07347195e-02, 1.53972441e-02,\n",
       "         4.14403342e-02, 5.39055765e-02, 5.10240458e-02, 2.72540618e-02,\n",
       "         3.44690643e-02, 1.69548951e-02, 6.22333474e-02, 6.86356500e-02,\n",
       "         3.09993383e-02, 1.93795916e-02, 6.92333374e-03, 6.33618608e-02,\n",
       "         2.85438076e-03, 2.11230088e-02, 2.51805689e-02, 1.21452035e-02,\n",
       "         9.20830108e-03, 1.71925838e-03, 2.41311863e-02, 1.84162669e-02,\n",
       "         7.25686923e-02, 2.85151210e-02, 5.35768308e-02, 6.38391897e-02,\n",
       "         3.72587256e-02, 2.05672383e-02, 6.02041706e-02, 9.00491048e-03,\n",
       "         3.59587185e-02, 3.31063531e-02, 3.50178815e-02, 3.39152962e-02,\n",
       "         6.14664592e-02, 8.26184005e-02, 2.97161256e-04, 1.88864376e-02,\n",
       "         3.72581892e-02, 7.61720985e-02, 3.02168764e-02, 3.19638848e-02,\n",
       "         9.00037959e-03, 5.16635217e-02, 3.01434938e-02, 7.49558769e-03,\n",
       "         5.49253561e-02, 6.47792220e-02, 4.17712517e-03, 5.74503690e-02,\n",
       "         5.82251027e-02, 1.76605470e-02, 4.80176695e-02, 2.77598146e-02,\n",
       "         3.45799699e-02, 1.70058217e-02, 3.44081819e-02, 1.07089542e-02,\n",
       "         3.67353596e-02, 6.90750927e-02, 6.37404695e-02, 8.51439014e-02,\n",
       "         1.38755757e-02, 3.45660150e-02, 3.63734961e-02, 1.99171645e-03,\n",
       "         4.35308851e-02, 1.03007071e-01, 1.63634482e-03, 6.61181053e-03,\n",
       "         8.26033112e-03, 1.62481423e-02, 1.69193931e-02, 8.16063210e-02,\n",
       "         1.71136875e-02, 2.29902118e-02, 8.56175125e-02, 6.48404732e-02,\n",
       "         1.39383599e-02, 4.69105393e-02, 3.80502548e-03, 4.52343263e-02,\n",
       "         4.43468355e-02, 3.34775001e-02, 5.25353216e-02, 1.09755285e-01,\n",
       "         4.54767607e-02, 5.06298207e-02, 6.93566799e-02, 3.27126682e-02,\n",
       "         1.11933639e-02, 6.90068956e-03, 8.66007712e-03, 2.02507954e-02,\n",
       "         4.33597676e-02, 6.97045997e-02, 2.79211570e-02, 2.14616931e-03,\n",
       "         1.13024134e-02, 3.55066769e-02, 4.62248623e-02, 1.16909966e-02,\n",
       "         1.62924314e-03, 2.92768944e-02, 8.07629302e-02, 7.34090980e-04,\n",
       "         6.24409951e-02, 7.13202357e-02, 6.76163211e-02, 2.12575236e-04,\n",
       "         1.38227474e-02, 5.03952764e-02, 4.47009206e-02, 1.03320470e-02,\n",
       "         6.60207681e-03, 1.00193508e-02, 4.84197959e-02, 8.29594210e-03,\n",
       "         1.39127858e-02, 3.14222686e-02, 3.29259112e-02, 2.82410346e-02,\n",
       "         6.02863580e-02, 6.15138151e-02, 1.85014773e-02, 7.25112185e-02,\n",
       "         4.51589786e-02, 3.08469925e-02, 8.80268961e-02, 3.69959809e-02,\n",
       "         6.16478212e-02, 4.78016119e-03, 2.64515579e-02, 5.71395382e-02,\n",
       "         4.02856171e-02, 2.02611256e-02, 1.00858845e-02, 5.48877381e-02,\n",
       "         1.20142577e-02, 4.83040959e-02, 4.22857776e-02, 2.63058227e-02,\n",
       "         3.32213826e-02, 3.67292017e-02, 5.86581454e-02, 3.55060361e-02,\n",
       "         4.82817441e-02, 2.57093906e-02, 3.09751760e-02, 7.28031099e-02,\n",
       "         5.49163148e-02, 4.47162874e-02, 2.82072350e-02, 8.02267119e-02,\n",
       "         5.05192615e-02, 1.09807756e-02, 7.62895271e-02, 5.29533178e-02,\n",
       "         8.32460746e-02, 1.20028891e-02, 3.68063711e-02, 2.38575544e-02,\n",
       "         8.73156562e-02, 6.99278247e-03, 6.99040964e-02, 2.95946766e-02,\n",
       "         3.80583256e-02, 2.03545243e-02, 3.54290791e-02, 1.30789506e-03,\n",
       "         2.11291946e-02, 2.33880114e-02, 5.55419363e-02, 4.43413369e-02,\n",
       "         2.66630612e-02, 7.17787370e-02, 1.69385448e-02, 1.74994376e-02,\n",
       "         6.78247884e-02, 1.03342086e-02, 2.62102019e-02, 6.84764609e-02,\n",
       "         3.97219285e-02, 3.95480134e-02, 7.96533599e-02, 2.73495559e-02,\n",
       "         4.51607034e-02, 8.65989272e-03, 7.87367076e-02, 1.14611067e-01,\n",
       "         1.11691058e-02, 5.01510836e-02, 9.42909569e-02, 1.42389489e-02,\n",
       "         1.92902293e-02, 7.22552941e-04, 5.30163497e-02, 2.46015005e-02,\n",
       "         1.51435668e-02, 3.56403552e-02, 3.67165357e-02, 2.29544342e-02,\n",
       "         9.49694738e-02, 5.68383709e-02, 6.61412477e-02, 4.93006669e-02,\n",
       "         5.12611913e-03, 5.10986056e-03, 1.10975150e-02, 2.05766596e-02,\n",
       "         4.39922437e-02, 3.83374505e-02, 4.86569814e-02, 3.61018181e-02,\n",
       "         1.57256592e-02, 5.64395040e-02, 3.20594311e-02, 8.13259464e-03,\n",
       "         9.17203650e-02, 2.75235865e-02, 4.52977233e-02, 8.85445718e-03,\n",
       "         2.31417418e-02, 5.81747340e-03, 1.13616781e-02, 3.89796123e-02,\n",
       "         4.27468587e-03, 1.46952504e-03, 4.51583304e-02, 8.72781314e-03,\n",
       "         3.16448212e-02, 8.94233733e-02, 3.26892883e-02, 6.51420429e-02,\n",
       "         6.44817054e-02, 4.43597464e-03, 4.45415778e-03, 7.53065646e-02,\n",
       "         1.65370051e-02, 2.54989346e-03, 9.88684874e-03, 1.49267893e-02,\n",
       "         1.07933674e-02, 2.69747688e-03, 3.37061137e-02, 1.45305945e-02,\n",
       "         3.24011408e-02, 2.42647287e-02, 2.13029325e-01, 1.72601901e-02,\n",
       "         6.30634800e-02, 1.88172553e-02, 3.79561773e-03, 2.69540437e-02,\n",
       "         9.15961713e-03, 2.80827638e-02, 3.53171714e-02, 6.06815703e-02,\n",
       "         9.05769598e-03, 5.36362603e-02, 5.17944284e-02, 1.99432466e-02,\n",
       "         5.23463152e-02, 1.34577220e-02, 8.65187347e-02, 2.10680235e-02,\n",
       "         9.48060900e-02, 3.31270918e-02, 5.34142135e-03, 1.92652326e-02,\n",
       "         3.42972465e-02, 1.20989718e-02, 5.74024394e-02, 8.65372568e-02,\n",
       "         4.54765996e-06, 7.99155235e-02, 7.01620653e-02, 2.23761313e-02,\n",
       "         2.01845486e-02, 1.04714399e-02, 3.04661468e-02, 3.22063379e-02,\n",
       "         2.99870200e-03, 4.16189199e-03, 2.73540113e-02, 1.70042031e-02,\n",
       "         4.22213152e-02, 8.83625913e-03, 6.32247049e-03, 4.21804301e-02,\n",
       "         2.00778842e-02, 1.51335727e-02, 7.99682960e-02, 5.68058640e-02,\n",
       "         2.26943679e-02, 1.79488678e-02, 1.35058258e-03, 7.66931176e-02,\n",
       "         5.95908016e-02, 2.25251503e-02, 2.57689860e-02, 5.35177672e-03,\n",
       "         7.73169240e-03, 2.55475268e-02, 1.89008415e-01, 1.40882775e-01,\n",
       "         1.30757987e-02, 2.19924003e-02, 2.68042963e-02, 3.85430194e-02,\n",
       "         2.11536027e-02, 1.65667254e-02, 1.85683519e-02, 1.71968166e-03,\n",
       "         1.92310717e-02, 8.29899982e-02, 9.94304754e-03, 1.22518139e-02,\n",
       "         1.47659518e-02, 2.98385713e-02, 3.10527533e-02, 2.96610612e-02,\n",
       "         1.84789691e-02, 7.81352632e-03, 3.04573495e-02, 4.95091174e-03,\n",
       "         3.03656831e-02, 4.89095598e-02, 1.51802471e-03, 1.62704196e-02,\n",
       "         2.95218434e-02, 1.71252172e-02, 1.26207918e-02, 1.18733019e-01,\n",
       "         1.55267287e-02, 1.19169457e-02, 2.68989392e-02, 1.67470351e-02,\n",
       "         3.29534821e-02, 7.01755926e-04, 2.51513179e-02, 4.93938364e-02,\n",
       "         1.38314962e-02, 3.88363265e-02, 1.57793984e-02, 4.01540510e-02,\n",
       "         6.59700558e-02, 1.41310049e-02, 1.67926140e-02, 3.36303264e-02,\n",
       "         5.88524900e-02, 1.24710668e-02, 8.62498023e-03, 1.31695615e-02,\n",
       "         9.33607370e-02, 9.76526067e-02, 6.73978329e-02, 1.39459688e-03,\n",
       "         3.38493064e-02, 3.87482047e-02, 7.29785766e-03, 1.01842061e-02,\n",
       "         5.95671907e-02, 4.12688591e-02, 4.65044454e-02, 7.96108320e-02,\n",
       "         2.76222695e-02, 1.31449988e-02, 3.96820270e-02, 5.49029484e-02,\n",
       "         2.39699222e-02, 9.95781273e-03, 2.06829999e-02, 4.30290811e-02,\n",
       "         8.40600394e-03, 4.29842919e-02, 2.69899741e-02, 2.59776711e-02,\n",
       "         2.22820714e-02, 4.45979908e-02, 2.37044040e-02, 2.55849231e-02,\n",
       "         5.66621386e-02, 3.36445346e-02, 1.79148512e-03, 3.04723792e-02,\n",
       "         5.15132807e-02, 1.27160149e-02, 9.37322006e-02, 5.64925112e-02,\n",
       "         3.04910559e-02, 1.02773413e-01, 3.04112509e-02, 3.09887696e-02,\n",
       "         4.85019805e-03, 2.40479540e-02, 1.79534648e-02, 3.96751128e-02,\n",
       "         2.80853454e-02, 1.08542796e-02, 4.55317227e-03, 1.08720236e-01,\n",
       "         8.10621753e-02, 4.12164163e-03, 2.16132198e-02, 4.87592407e-02,\n",
       "         1.15172723e-02, 4.62021343e-02, 2.02415884e-03, 1.69710703e-02,\n",
       "         2.54178457e-02, 4.30359878e-02, 2.63019186e-02, 2.94150319e-02,\n",
       "         4.05908078e-02, 3.56915817e-02, 4.33271639e-02, 4.85281646e-02,\n",
       "         9.77329910e-04, 3.10534984e-02, 3.17778140e-02, 5.20837307e-02,\n",
       "         2.01988462e-02, 6.46664202e-02, 6.96789101e-02, 4.10784334e-02,\n",
       "         1.72812734e-02, 1.53285628e-02, 4.62873876e-02, 4.21181358e-02,\n",
       "         2.68609375e-02, 1.11574028e-02, 4.51603642e-04, 2.29473785e-02,\n",
       "         2.79046874e-02, 1.05329202e-02, 8.13468825e-03, 4.12094891e-02,\n",
       "         3.33789475e-02, 6.60914779e-02, 5.99138103e-02, 8.84534642e-02,\n",
       "         4.57227863e-02, 3.96474078e-02, 5.32894023e-02, 2.40969080e-02,\n",
       "         1.59318894e-02, 2.32143756e-02, 1.41572692e-02, 2.35792715e-02,\n",
       "         8.08067096e-04, 6.02648482e-02, 4.65509258e-02, 1.97878294e-02,\n",
       "         3.64142247e-02, 3.45020406e-02, 4.89302203e-02, 2.26931106e-02,\n",
       "         1.99265573e-02, 1.16167655e-02, 2.88816895e-02, 1.25710003e-03,\n",
       "         7.50841748e-04, 7.90543295e-03, 2.68591288e-02, 2.04137433e-02,\n",
       "         3.05785406e-02, 1.51884221e-02, 2.50876173e-02, 1.10455574e-02,\n",
       "         4.44705784e-03, 3.28884609e-02, 3.63746956e-02, 5.32789230e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 8,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([3.37832831e-02, 6.46229014e-02, 7.96830631e-04, 2.98157558e-02,\n",
       "         5.77041730e-02, 2.11275332e-02, 4.47260514e-02, 1.93732772e-02,\n",
       "         2.01048516e-02, 7.36242533e-02, 6.66251183e-02, 2.95717176e-02,\n",
       "         7.60529637e-02, 4.46889214e-02, 4.74637933e-02, 6.41186303e-03,\n",
       "         4.66789566e-02, 2.31760163e-02, 4.23852205e-02, 6.04365161e-03,\n",
       "         1.35761788e-02, 2.09029466e-02, 4.38461453e-03, 6.89550042e-02,\n",
       "         1.55896842e-02, 3.88319269e-02, 6.18106052e-02, 8.94255005e-03,\n",
       "         9.02854055e-02, 5.02822585e-02, 2.42154282e-02, 3.79756354e-02,\n",
       "         2.31451765e-02, 5.74123487e-02, 3.21221389e-02, 3.04155573e-02,\n",
       "         2.16095685e-03, 5.37161492e-02, 3.16393189e-03, 1.22102897e-03,\n",
       "         1.18096257e-02, 3.90714733e-03, 8.15158188e-02, 1.10275857e-02,\n",
       "         8.29560012e-02, 5.36960028e-02, 1.28104230e-02, 6.26130402e-02,\n",
       "         2.67688967e-02, 2.84953532e-03, 3.48212719e-02, 6.39323145e-03,\n",
       "         1.49666623e-03, 9.53581184e-03, 6.61495747e-03, 8.51789024e-03,\n",
       "         1.54650267e-02, 6.93705902e-02, 8.41117129e-02, 8.58609658e-03,\n",
       "         7.18087405e-02, 4.08812799e-02, 9.27956402e-03, 5.36251208e-03,\n",
       "         7.36253113e-02, 2.88860556e-02, 3.86822224e-02, 1.58882304e-03,\n",
       "         3.23554128e-03, 6.17429800e-02, 4.50593568e-02, 6.19782135e-02,\n",
       "         4.46340330e-02, 1.15466034e-02, 4.77449521e-02, 5.65495044e-02,\n",
       "         6.67105010e-03, 1.59728155e-02, 5.75911924e-02, 2.86239423e-02,\n",
       "         1.81980450e-02, 1.15192235e-01, 3.28085870e-02, 1.53023368e-02,\n",
       "         2.96778930e-03, 3.48655321e-02, 6.79756626e-02, 4.38120328e-02,\n",
       "         4.40722704e-02, 1.83115359e-02, 1.99874733e-02, 4.05781120e-02,\n",
       "         1.50549814e-01, 5.97617764e-04, 7.20553994e-02, 3.52017023e-02,\n",
       "         1.69844497e-02, 5.51351383e-02, 2.47568358e-02, 2.14623958e-02,\n",
       "         3.65561657e-02, 4.25096713e-02, 1.35300402e-02, 1.35835810e-02,\n",
       "         2.76040174e-02, 1.69564076e-02, 1.53113110e-03, 5.49859134e-03,\n",
       "         2.57419050e-02, 8.57437551e-02, 1.36609692e-02, 8.17288980e-02,\n",
       "         1.96562032e-03, 7.80288922e-03, 3.40179838e-02, 4.25760634e-02,\n",
       "         5.68921380e-02, 3.38470712e-02, 4.01616655e-02, 7.41690174e-02,\n",
       "         4.04238813e-02, 1.97836850e-02, 9.36415233e-03, 2.07147487e-02,\n",
       "         5.54322544e-03, 5.07206842e-02, 6.15878329e-02, 5.50853871e-02,\n",
       "         1.46491099e-02, 8.12123250e-03, 4.21188362e-02, 6.86969049e-03,\n",
       "         3.24272029e-02, 3.02112997e-02, 9.64689907e-03, 9.51019023e-03,\n",
       "         2.77532600e-02, 7.04954844e-03, 1.92253646e-02, 5.49161015e-03,\n",
       "         1.98137350e-02, 4.82364409e-02, 1.04020581e-01, 1.28669962e-02,\n",
       "         8.34925398e-02, 5.00585809e-02, 1.71685964e-02, 3.86251020e-03,\n",
       "         8.56687650e-02, 3.79207209e-02, 4.23303582e-02, 1.61601007e-02,\n",
       "         1.65237561e-02, 3.28463353e-02, 4.37015072e-02, 2.53934748e-02,\n",
       "         8.74270573e-02, 5.08313291e-02, 1.69787481e-02, 5.25519857e-03,\n",
       "         1.77430362e-02, 1.62344966e-02, 4.52971496e-02, 7.54399002e-02,\n",
       "         5.99221773e-02, 5.47674336e-02, 1.79554410e-02, 7.20799118e-02,\n",
       "         3.72904390e-02, 4.26929630e-02, 7.75026232e-02, 6.01591542e-03,\n",
       "         3.05360872e-02, 7.79831111e-02, 1.41546717e-02, 1.16402553e-02,\n",
       "         5.06296102e-03, 6.50522262e-02, 1.72332991e-02, 4.03029844e-02,\n",
       "         2.07403954e-02, 5.61362086e-03, 2.18743067e-02, 2.79224720e-02,\n",
       "         2.07338948e-03, 9.21632051e-02, 1.05063722e-01, 4.49879561e-03,\n",
       "         3.27374786e-02, 9.61125176e-03, 1.10211223e-02, 7.61812786e-03,\n",
       "         4.74054366e-02, 1.18781894e-01, 1.39652314e-02, 2.71527302e-02,\n",
       "         2.81152520e-02, 9.18072183e-03, 7.31070060e-03, 4.17013578e-02,\n",
       "         2.81653423e-02, 1.60953719e-02, 6.22526146e-02, 1.11084618e-01,\n",
       "         3.68582904e-02, 8.33953619e-02, 1.30753443e-02, 4.63831425e-02,\n",
       "         6.98962137e-02, 3.81636508e-02, 1.21294067e-03, 4.51139989e-04,\n",
       "         1.20376684e-02, 6.48281025e-03, 5.01511199e-03, 1.30294925e-02,\n",
       "         6.92842295e-03, 7.48777250e-03, 2.78697871e-02, 2.56993528e-02,\n",
       "         2.23165657e-02, 7.26444472e-04, 3.57516445e-02, 4.89691682e-02,\n",
       "         1.74329802e-02, 4.28028107e-02, 8.12703893e-02, 1.86453667e-03,\n",
       "         1.18633226e-01, 1.05317496e-02, 4.01796550e-02, 1.13620482e-01,\n",
       "         5.48964180e-02, 5.51100411e-02, 4.39432263e-02, 2.71740537e-02,\n",
       "         1.06744587e-01, 6.70780311e-04, 4.15319018e-02, 7.66182467e-02,\n",
       "         5.68452589e-02, 1.70827553e-01, 4.36794199e-02, 4.03081402e-02,\n",
       "         4.42903079e-02, 8.19341540e-02, 2.48402776e-03, 1.49779646e-02,\n",
       "         1.74962338e-02, 2.69708522e-02, 4.60652709e-02, 7.57883638e-02,\n",
       "         2.13103425e-02, 8.50811005e-02, 2.82606278e-02, 1.01614565e-01,\n",
       "         1.06838224e-02, 2.08497029e-02, 2.29690969e-02, 1.87214464e-03,\n",
       "         5.85891791e-02, 2.22105570e-02, 1.25100445e-02, 1.61190405e-02,\n",
       "         6.53314441e-02, 1.92556791e-02, 1.43809449e-02, 6.63485825e-02,\n",
       "         2.00296883e-02, 1.25147346e-02, 1.00806598e-02, 3.53898816e-02,\n",
       "         3.94631922e-02, 2.75672972e-02, 1.50759257e-02, 5.78044653e-02,\n",
       "         1.14495801e-02, 8.69687274e-02, 2.10498068e-02, 1.60032120e-02,\n",
       "         9.49847475e-02, 3.72453928e-02, 2.50023138e-03, 1.86560992e-02,\n",
       "         5.25905155e-02, 8.72314163e-03, 1.84279811e-02, 1.25758629e-02,\n",
       "         3.33622866e-03, 6.58586994e-02, 9.93272662e-03, 1.79757755e-02,\n",
       "         8.27868059e-02, 8.24023336e-02, 1.26660420e-02, 1.33415731e-02,\n",
       "         1.18155949e-01, 1.74883415e-03, 3.93673144e-02, 1.78778376e-02,\n",
       "         2.29337607e-02, 1.03406021e-02, 2.78287213e-02, 5.85814454e-02,\n",
       "         5.40990243e-03, 5.50749665e-03, 2.86330841e-02, 1.95541214e-02,\n",
       "         1.40803093e-02, 5.64793348e-02, 2.53059715e-02, 2.79639922e-02,\n",
       "         2.03622766e-02, 4.09654491e-02, 1.75756104e-02, 2.81458977e-03,\n",
       "         9.12949443e-02, 1.85954664e-03, 4.95577902e-02, 8.68002698e-03,\n",
       "         1.11133303e-03, 1.77205838e-02, 2.79996935e-02, 4.09866385e-02,\n",
       "         2.81168800e-02, 5.37858624e-03, 2.23487131e-02, 4.75631969e-04,\n",
       "         5.78483683e-04, 4.88746911e-02, 6.91595953e-03, 1.38027407e-02,\n",
       "         1.49762040e-04, 7.69406706e-02, 1.59681197e-02, 6.00281097e-02,\n",
       "         1.77232083e-02, 7.39606656e-03, 3.49841379e-02, 4.27008756e-02,\n",
       "         4.00020648e-03, 1.51266567e-02, 4.81559746e-02, 4.78505716e-02,\n",
       "         1.01709587e-03, 3.82450111e-02, 5.54201230e-02, 8.58800765e-03,\n",
       "         1.86323188e-03, 1.07509429e-02, 2.91394591e-02, 3.55521031e-03,\n",
       "         5.56938490e-03, 5.34969196e-02, 2.54609603e-02, 3.83665748e-02,\n",
       "         1.24937401e-03, 4.73581217e-02, 6.69677230e-03, 6.69695511e-02,\n",
       "         1.35886427e-02, 1.42480573e-02, 6.21798635e-03, 3.23935300e-02,\n",
       "         6.10956214e-02, 5.21740830e-03, 7.28851883e-03, 1.56984832e-02,\n",
       "         6.12097718e-02, 4.75267395e-02, 5.27392477e-02, 5.32679148e-02,\n",
       "         1.24169448e-02, 2.57982388e-02, 3.42906788e-02, 3.02286190e-03,\n",
       "         1.21051271e-03, 4.14233934e-03, 7.46880174e-02, 5.35349548e-02,\n",
       "         6.57824501e-02, 3.58183086e-02, 4.70682867e-02, 1.96821541e-02,\n",
       "         7.17747286e-02, 1.86396006e-03, 2.88708284e-02, 1.16339326e-02,\n",
       "         3.22276540e-02, 1.88403167e-02, 5.30065820e-02, 3.49423103e-02,\n",
       "         3.98108177e-03, 1.24160219e-02, 6.22771345e-02, 5.76000437e-02,\n",
       "         4.02294770e-02, 7.86949135e-03, 5.48055805e-02, 2.18523089e-02,\n",
       "         5.03737805e-03, 1.17117772e-02, 3.69254872e-02, 3.45794968e-02,\n",
       "         1.68026611e-02, 1.01216929e-02, 4.84292768e-02, 6.44598305e-02,\n",
       "         2.38729976e-02, 2.92595327e-02, 6.38902336e-02, 8.91718722e-04,\n",
       "         5.68156131e-02, 5.22668362e-02, 4.43598591e-02, 2.65043881e-02,\n",
       "         7.08407490e-03, 2.92292573e-02, 1.17334342e-02, 1.30012510e-02,\n",
       "         5.51893599e-02, 1.46173621e-02, 1.65035133e-03, 1.72908530e-02,\n",
       "         2.23150421e-02, 3.89639251e-02, 5.38009964e-02, 3.69004309e-02,\n",
       "         2.34906431e-02, 3.61382850e-02, 1.90425403e-02, 1.15412228e-01,\n",
       "         6.09890632e-02, 7.59077221e-02, 4.20892276e-02, 9.73799825e-03,\n",
       "         1.99776981e-02, 1.94297116e-02, 5.07242717e-02, 2.46171504e-02,\n",
       "         7.46284379e-03, 4.20455635e-02, 5.01637235e-02, 8.90723150e-03,\n",
       "         8.14355561e-04, 1.60421692e-02, 3.93569432e-02, 8.85090828e-02,\n",
       "         5.65543212e-02, 5.21339849e-02, 6.90271659e-03, 5.82711697e-02,\n",
       "         1.67892613e-02, 2.47471184e-02, 1.88327879e-02, 2.02306118e-02,\n",
       "         2.83375960e-02, 6.92128018e-03, 8.17403570e-02, 4.42511849e-02,\n",
       "         1.76323131e-02, 7.21148103e-02, 3.54527384e-02, 5.68973310e-02,\n",
       "         4.24562022e-03, 9.03685912e-02, 1.39279142e-02, 5.22801070e-04,\n",
       "         6.08615205e-02, 3.17773223e-02, 7.42762256e-03, 4.97594550e-02,\n",
       "         8.05692300e-02, 2.19126493e-02, 3.11398581e-02, 3.81020829e-02,\n",
       "         3.30124907e-02, 5.04447222e-02, 5.85741326e-02, 1.43710211e-01,\n",
       "         7.39506334e-02, 2.66343746e-02, 3.63635947e-03, 1.46707501e-02,\n",
       "         3.71257700e-02, 7.04148114e-02, 2.06200127e-02, 3.70669961e-02,\n",
       "         4.51221503e-02, 2.47499570e-02, 2.45505404e-02, 1.61013193e-02,\n",
       "         1.77971516e-02, 2.29112059e-03, 1.86616275e-02, 9.96390358e-02,\n",
       "         1.27754454e-02, 3.88597399e-02, 1.47673208e-02, 6.07575290e-02,\n",
       "         2.15920620e-02, 2.69252360e-02, 2.27092649e-03, 1.27539970e-02,\n",
       "         2.40360424e-02, 1.97114423e-02, 3.81488986e-02, 4.89429310e-02,\n",
       "         8.73946846e-02, 3.60205881e-02, 7.66071007e-02, 9.64207798e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 9,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([1.78284142e-02, 1.73724853e-02, 1.02154404e-01, 1.03387989e-01,\n",
       "         4.31527682e-02, 3.63702923e-02, 3.43750156e-02, 3.14289029e-03,\n",
       "         3.25508974e-02, 1.38576434e-04, 2.26869900e-02, 5.12750074e-02,\n",
       "         5.45519330e-02, 1.74970534e-02, 4.24007513e-03, 1.30473068e-02,\n",
       "         6.47590607e-02, 5.03806919e-02, 6.94857910e-02, 1.78791005e-02,\n",
       "         1.90768950e-02, 4.20492329e-03, 5.84056377e-02, 1.46359522e-02,\n",
       "         3.55806202e-02, 3.87793519e-02, 1.39192836e-02, 3.00313104e-02,\n",
       "         4.74510193e-02, 5.95252253e-02, 1.50276637e-02, 1.92008223e-02,\n",
       "         4.10498418e-02, 6.79564029e-02, 5.80751221e-04, 2.32026223e-02,\n",
       "         2.38673333e-02, 4.41648215e-02, 4.30126414e-02, 9.11063179e-02,\n",
       "         8.85651447e-03, 3.12459227e-02, 6.49930164e-02, 7.69024948e-03,\n",
       "         2.16086954e-02, 1.99659225e-02, 3.05595342e-02, 1.53254298e-02,\n",
       "         1.09926779e-02, 4.85912412e-02, 1.21629387e-02, 1.83512587e-02,\n",
       "         4.14898582e-02, 1.55002587e-02, 2.67925505e-02, 6.42381385e-02,\n",
       "         1.24697834e-02, 8.97245575e-03, 1.35158068e-02, 2.19422076e-02,\n",
       "         7.24195549e-03, 1.33691028e-01, 1.30378893e-02, 2.33222228e-02,\n",
       "         3.09753958e-02, 6.01309501e-02, 6.04016595e-02, 1.47751104e-02,\n",
       "         2.92750378e-03, 8.91627297e-02, 5.27590141e-02, 6.88285679e-02,\n",
       "         5.65992445e-02, 4.07689810e-02, 5.12521267e-02, 4.27485742e-02,\n",
       "         8.87087267e-03, 8.57265759e-03, 2.34796721e-02, 6.84874645e-03,\n",
       "         9.25679803e-02, 1.33597665e-02, 3.25122178e-02, 1.23748006e-02,\n",
       "         6.28248230e-02, 1.34034827e-02, 6.99414611e-02, 2.91295797e-02,\n",
       "         3.05785355e-03, 1.75604764e-02, 9.93714109e-03, 9.50114150e-03,\n",
       "         3.76977175e-02, 1.28585752e-02, 1.78573523e-02, 4.05124435e-03,\n",
       "         3.06239072e-02, 9.53931883e-02, 4.15576771e-02, 1.54262520e-02,\n",
       "         4.91672894e-03, 2.72244010e-02, 1.19522542e-01, 1.32419812e-02,\n",
       "         8.28562211e-03, 4.36367393e-02, 6.95718750e-02, 4.41142172e-02,\n",
       "         4.43849573e-03, 1.83266833e-01, 4.15089130e-02, 5.34834005e-02,\n",
       "         6.41956776e-02, 1.54580623e-02, 4.43282798e-02, 5.25291227e-02,\n",
       "         1.44125326e-02, 2.16701534e-02, 6.02715882e-04, 3.55221182e-02,\n",
       "         4.70047109e-02, 1.05622942e-02, 8.63538682e-03, 2.22732127e-02,\n",
       "         1.04184290e-02, 4.86747660e-02, 4.57465984e-02, 4.09137048e-02,\n",
       "         5.95027767e-02, 5.01431637e-02, 2.22429335e-02, 4.05728742e-02,\n",
       "         7.58352038e-03, 4.77914177e-02, 7.89359733e-02, 4.46425118e-02,\n",
       "         9.21942294e-03, 1.13699727e-01, 4.23532724e-02, 3.65061387e-02,\n",
       "         4.46075872e-02, 1.49695138e-02, 2.39675324e-02, 2.82361526e-02,\n",
       "         1.51208341e-02, 4.00491208e-02, 3.10524851e-02, 1.74739771e-02,\n",
       "         2.93440409e-02, 6.49015307e-02, 1.82840191e-02, 8.01231060e-03,\n",
       "         4.47643138e-02, 2.23968714e-03, 3.20691615e-02, 6.80699199e-03,\n",
       "         4.28758450e-02, 1.00077281e-03, 8.23647603e-02, 4.53552902e-02,\n",
       "         1.43503798e-02, 2.63391379e-02, 9.29830410e-03, 1.05179399e-02,\n",
       "         3.74097824e-02, 5.84620386e-02, 9.05716326e-03, 1.25234136e-02,\n",
       "         8.48071213e-05, 3.60133802e-03, 1.92669146e-02, 1.66321695e-02,\n",
       "         4.98227291e-02, 7.55220931e-03, 4.06743698e-02, 5.67226717e-03,\n",
       "         4.15612198e-02, 4.04982045e-02, 1.17000435e-02, 3.35327536e-02,\n",
       "         5.25833368e-02, 5.81405312e-02, 2.78783683e-02, 1.11268893e-01,\n",
       "         4.33889963e-02, 2.87431870e-02, 1.15274102e-03, 3.16898935e-02,\n",
       "         1.00512765e-02, 5.57011813e-02, 5.82421832e-02, 5.65381944e-02,\n",
       "         1.03736445e-02, 8.05690512e-02, 6.09396212e-03, 1.19944224e-02,\n",
       "         3.44086140e-02, 2.35642865e-02, 2.85931360e-02, 6.77782483e-03,\n",
       "         4.45922092e-02, 3.96764278e-03, 3.58982719e-02, 5.19202277e-02,\n",
       "         6.56852052e-02, 5.15389666e-02, 8.07667449e-02, 8.33985060e-02,\n",
       "         4.76725213e-02, 3.76399676e-03, 7.89189264e-02, 5.58209606e-02,\n",
       "         1.84999555e-02, 5.76580949e-02, 4.86623636e-03, 1.32206650e-02,\n",
       "         5.32260630e-04, 6.66749775e-02, 3.95919085e-02, 4.78647910e-02,\n",
       "         4.53988574e-02, 1.78474188e-03, 2.97027193e-02, 6.68217093e-02,\n",
       "         4.60313447e-02, 9.19774771e-02, 4.89632934e-02, 8.84681344e-02,\n",
       "         1.70932543e-02, 2.45430283e-02, 5.09519652e-02, 4.45386861e-03,\n",
       "         3.91366407e-02, 1.77016519e-02, 4.17498611e-02, 1.16346609e-02,\n",
       "         7.47845694e-02, 5.47088869e-02, 2.62394175e-02, 8.26884881e-02,\n",
       "         5.36032096e-02, 6.40717987e-03, 1.72893191e-03, 7.38248378e-02,\n",
       "         4.93316688e-02, 2.03820951e-02, 2.65959483e-02, 4.33634734e-03,\n",
       "         1.94382435e-03, 2.81471778e-02, 3.93347554e-02, 8.30814894e-03,\n",
       "         1.16752513e-01, 4.97875772e-02, 2.44121403e-02, 6.65253401e-02,\n",
       "         4.05205227e-02, 2.70419866e-02, 1.61110401e-01, 4.08679023e-02,\n",
       "         2.05119625e-02, 1.88727211e-02, 3.41624767e-02, 1.94326360e-02,\n",
       "         2.24430859e-02, 5.55694290e-02, 3.15610208e-02, 2.69789137e-02,\n",
       "         6.36727363e-02, 7.06026424e-03, 1.77407544e-02, 8.56038090e-03,\n",
       "         2.35585943e-02, 1.71210896e-02, 3.91071923e-02, 1.87531090e-03,\n",
       "         3.19251083e-02, 2.99086403e-02, 6.07761182e-02, 6.02036156e-02,\n",
       "         5.60953580e-02, 7.24676624e-02, 3.86869572e-02, 6.88698068e-02,\n",
       "         6.02508895e-03, 5.82073256e-03, 1.53503045e-02, 4.94321100e-02,\n",
       "         3.60649489e-02, 2.91221775e-02, 2.96400972e-02, 4.00477536e-02,\n",
       "         2.39207614e-02, 2.32712626e-02, 1.61897261e-02, 4.44062091e-02,\n",
       "         8.86114612e-02, 4.84473109e-02, 7.90958572e-03, 6.39263839e-02,\n",
       "         5.69239398e-03, 4.44381647e-02, 1.17412321e-02, 2.66075339e-02,\n",
       "         9.34479572e-03, 2.82925721e-02, 1.31521551e-02, 4.11231667e-02,\n",
       "         1.88646130e-02, 4.59270403e-02, 1.14802485e-02, 9.55809206e-02,\n",
       "         1.11903893e-02, 3.33169252e-02, 4.58477363e-02, 6.06400855e-02,\n",
       "         8.38089511e-02, 4.37916145e-02, 2.04958115e-02, 2.89584119e-02,\n",
       "         1.99998114e-02, 1.88766159e-02, 1.04600675e-01, 1.06599130e-01,\n",
       "         5.59141189e-02, 2.52861176e-02, 6.90820590e-02, 5.69278374e-02,\n",
       "         6.05206080e-02, 6.73355609e-02, 3.60295773e-02, 2.42850017e-02,\n",
       "         1.68218147e-02, 4.71679643e-02, 1.73317045e-02, 6.42559156e-02,\n",
       "         5.60528040e-03, 5.88624626e-02, 3.12702148e-03, 1.82295386e-02,\n",
       "         4.90773469e-02, 7.90455192e-03, 1.60296867e-03, 1.41519131e-02,\n",
       "         5.84695935e-02, 4.61995862e-02, 7.18284771e-03, 2.79499888e-02,\n",
       "         8.28424376e-03, 2.29125633e-03, 2.12194696e-02, 2.66644149e-03,\n",
       "         3.17090414e-02, 8.71910527e-03, 1.47220260e-02, 1.39181297e-02,\n",
       "         1.30984960e-02, 8.32102261e-03, 4.96835560e-02, 6.55851699e-03,\n",
       "         1.31442882e-02, 4.27580811e-02, 8.01757351e-02, 1.47860259e-01,\n",
       "         4.08036299e-02, 7.10469112e-02, 5.03006987e-02, 5.65240197e-02,\n",
       "         6.26535937e-02, 2.66424473e-02, 4.83707758e-03, 2.00868119e-02,\n",
       "         1.54736247e-02, 8.50483892e-04, 4.18059304e-02, 2.10233033e-02,\n",
       "         7.16901990e-03, 1.13586187e-02, 5.93357831e-02, 7.80902952e-02,\n",
       "         2.01205071e-02, 2.46211682e-02, 1.95892807e-02, 4.51473631e-02,\n",
       "         1.86493397e-02, 2.92340051e-02, 2.76960451e-02, 4.83797863e-03,\n",
       "         1.46871945e-02, 3.01309898e-02, 1.29757524e-02, 2.03463994e-02,\n",
       "         1.77948978e-02, 2.10476592e-02, 1.07458960e-02, 4.84592281e-03,\n",
       "         3.11296643e-03, 2.39258315e-02, 6.28951117e-02, 3.57284630e-03,\n",
       "         1.21006615e-01, 4.93251905e-02, 7.29514360e-02, 1.41367754e-02,\n",
       "         5.11342939e-03, 6.54639900e-02, 4.96263206e-02, 1.07955240e-01,\n",
       "         3.60795744e-02, 2.05258559e-02, 3.85516360e-02, 2.20287833e-02,\n",
       "         2.89313141e-02, 1.04570137e-02, 1.60972644e-02, 4.21163663e-02,\n",
       "         1.70034298e-03, 3.91790196e-02, 1.43135972e-02, 6.91861212e-02,\n",
       "         1.72316823e-02, 4.14176770e-02, 2.05145013e-02, 1.92665905e-02,\n",
       "         3.31550054e-02, 1.56829820e-03, 1.10487029e-01, 6.09917454e-02,\n",
       "         2.34778281e-02, 6.71917424e-02, 6.16197772e-02, 3.92936878e-02,\n",
       "         2.00062804e-02, 2.25684443e-03, 6.12493418e-02, 1.08058043e-02,\n",
       "         5.00134528e-02, 1.09358197e-02, 5.53983115e-02, 2.84359362e-02,\n",
       "         6.94140270e-02, 2.00188309e-02, 2.83256080e-02, 4.07495722e-02,\n",
       "         5.88891357e-02, 5.82842939e-02, 2.53360178e-02, 5.39609790e-02,\n",
       "         6.18532375e-02, 4.41157967e-02, 1.48578482e-02, 9.35548637e-03,\n",
       "         2.04132795e-02, 4.41176398e-03, 1.76035073e-02, 3.77523750e-02,\n",
       "         7.48881474e-02, 1.25433244e-02, 4.21049166e-03, 1.48105361e-02,\n",
       "         7.87747186e-03, 3.43257561e-02, 7.44170994e-02, 7.24174455e-02,\n",
       "         5.07882498e-02, 8.15810729e-03, 6.80157170e-02, 3.58066522e-02,\n",
       "         4.05878574e-02, 1.07566882e-02, 4.38657254e-02, 4.54022698e-02,\n",
       "         7.42731756e-03, 2.58386694e-02, 4.56837118e-02, 4.38667871e-02,\n",
       "         1.15414420e-02, 3.87791544e-02, 2.40341425e-02, 1.39835617e-02,\n",
       "         7.67422616e-02, 7.86667410e-03, 5.37823513e-02, 3.24440859e-02,\n",
       "         3.41144055e-02, 5.14633507e-02, 9.13414918e-03, 1.94110852e-02,\n",
       "         3.22641544e-02, 3.61254588e-02, 2.72606909e-02, 5.45663014e-02,\n",
       "         1.35977585e-02, 2.95637432e-03, 1.05984639e-02, 7.97457770e-02,\n",
       "         8.12954828e-02, 1.05320453e-03, 6.10670261e-03, 2.60337312e-02,\n",
       "         1.60728313e-03, 4.98638563e-02, 2.64561176e-02, 3.96933258e-02,\n",
       "         2.23333742e-02, 2.02039052e-02, 6.85548261e-02, 4.74681350e-04,\n",
       "         7.95565732e-03, 2.80289142e-03, 3.39869745e-02, 1.66514609e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 10,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([4.55297120e-02, 5.41417338e-02, 9.47528780e-02, 1.48893790e-02,\n",
       "         5.98582812e-02, 4.26197005e-03, 1.24508413e-02, 4.92706560e-02,\n",
       "         4.63925814e-03, 4.44310457e-02, 2.76645906e-02, 1.28210185e-03,\n",
       "         2.30677668e-02, 5.93781704e-03, 3.32527496e-02, 9.92156472e-03,\n",
       "         5.21722175e-02, 7.12219402e-02, 1.16660230e-01, 7.85525963e-02,\n",
       "         1.02150766e-02, 5.71770780e-02, 4.06068750e-02, 5.04032001e-02,\n",
       "         2.57658400e-02, 2.30651107e-02, 4.52396087e-02, 2.06785835e-02,\n",
       "         1.85177494e-02, 9.35786292e-02, 5.92725240e-02, 1.06865838e-02,\n",
       "         1.03074024e-02, 5.08781523e-02, 1.14972051e-02, 5.69066294e-02,\n",
       "         4.14285362e-02, 2.09063105e-02, 3.50033045e-02, 7.66653894e-03,\n",
       "         8.74970946e-03, 2.08851099e-02, 1.64550319e-02, 3.26464139e-02,\n",
       "         2.18323637e-02, 1.98186282e-02, 3.78297195e-02, 3.69886905e-02,\n",
       "         7.35290721e-02, 4.32818849e-03, 3.29358354e-02, 4.35957238e-02,\n",
       "         4.74567562e-02, 2.68670525e-02, 4.64958884e-03, 8.07847176e-03,\n",
       "         2.34994795e-02, 3.91886756e-02, 1.47227971e-02, 4.39828774e-03,\n",
       "         4.18466367e-02, 2.08328832e-02, 4.14310098e-02, 1.58671793e-02,\n",
       "         3.01762987e-02, 2.20038183e-03, 7.29307458e-02, 1.85335651e-02,\n",
       "         7.38039240e-02, 5.51298372e-02, 2.38113049e-02, 2.04296149e-02,\n",
       "         7.02830451e-03, 8.85690078e-02, 1.75698176e-02, 4.44015861e-02,\n",
       "         2.18454115e-02, 1.06565654e-02, 1.45325363e-02, 6.71119289e-03,\n",
       "         3.61548997e-02, 5.84446639e-02, 5.04133143e-02, 5.10675535e-02,\n",
       "         2.21930742e-02, 8.75003636e-02, 1.27243781e-02, 3.40689793e-02,\n",
       "         1.68928038e-02, 2.58667786e-02, 8.79492834e-02, 6.05936125e-02,\n",
       "         1.31357629e-02, 3.32306921e-02, 2.75113285e-02, 1.74580533e-02,\n",
       "         4.89582010e-02, 4.52675223e-02, 3.38547938e-02, 1.77436303e-02,\n",
       "         1.72938146e-02, 3.38979205e-03, 1.33219613e-02, 3.60842571e-02,\n",
       "         3.21545973e-02, 5.34887128e-02, 4.11459729e-02, 4.21189256e-02,\n",
       "         1.93820451e-03, 4.21900228e-02, 2.64941137e-02, 3.66930291e-02,\n",
       "         4.87255156e-02, 2.64334809e-02, 5.96733391e-02, 1.37031311e-02,\n",
       "         2.16046181e-02, 2.45436816e-03, 3.49406004e-02, 7.11524719e-03,\n",
       "         6.15231469e-02, 3.78775075e-02, 9.30070430e-02, 1.30975805e-03,\n",
       "         2.02995352e-02, 4.26021442e-02, 4.90025617e-02, 4.37970981e-02,\n",
       "         8.32703486e-02, 1.31096542e-02, 4.71315086e-02, 1.07982121e-02,\n",
       "         3.21913324e-02, 4.40003201e-02, 8.00315291e-02, 2.17240322e-02,\n",
       "         2.55396520e-03, 3.03531922e-02, 1.19245760e-02, 1.82136951e-03,\n",
       "         5.81684448e-02, 1.24418242e-02, 1.41161690e-02, 1.55525282e-02,\n",
       "         5.42009622e-03, 3.45446616e-02, 6.73926547e-02, 1.15516614e-02,\n",
       "         4.51862738e-02, 7.66483694e-03, 2.24521756e-02, 1.28363958e-04,\n",
       "         7.50572747e-03, 2.78926305e-02, 1.19543606e-02, 6.22235090e-02,\n",
       "         5.34559265e-02, 2.50319368e-03, 1.25944540e-02, 4.33460716e-03,\n",
       "         1.67215858e-02, 6.66961074e-02, 2.93239057e-02, 8.19664076e-03,\n",
       "         6.35189041e-02, 5.34858666e-02, 2.30197720e-02, 2.70378813e-02,\n",
       "         2.18036138e-02, 1.87058691e-02, 7.72221833e-02, 4.88095395e-02,\n",
       "         1.17425593e-02, 3.75439157e-03, 1.72777828e-02, 7.66784768e-04,\n",
       "         6.13527894e-02, 6.21846318e-02, 2.10250877e-02, 3.21295597e-02,\n",
       "         3.61175579e-03, 6.03888184e-03, 2.20980030e-02, 2.17694463e-03,\n",
       "         3.51525545e-02, 1.22840307e-03, 4.91547100e-02, 4.26795818e-02,\n",
       "         2.67514344e-02, 2.73754224e-02, 1.75649263e-02, 8.02430883e-03,\n",
       "         6.06264584e-02, 3.70500945e-02, 8.73141363e-03, 3.06688007e-02,\n",
       "         4.30940068e-04, 2.96287779e-02, 4.48585153e-02, 1.66197810e-02,\n",
       "         3.85722443e-02, 2.21739020e-02, 7.61690736e-02, 5.66232614e-02,\n",
       "         3.06098945e-02, 3.17987893e-03, 1.01558536e-01, 9.14542563e-03,\n",
       "         7.65030161e-02, 8.09724722e-03, 1.66560244e-02, 1.56139918e-02,\n",
       "         6.19281903e-02, 1.30104767e-02, 1.52479960e-02, 2.76242476e-02,\n",
       "         4.73399535e-02, 9.39818658e-03, 5.59321651e-03, 6.63093999e-02,\n",
       "         5.46705397e-03, 3.77598498e-03, 1.89903367e-04, 2.87848664e-03,\n",
       "         2.84885429e-02, 3.06473533e-03, 2.22491305e-02, 9.07467008e-02,\n",
       "         2.95238551e-02, 3.42796138e-03, 5.22201248e-02, 2.91786734e-02,\n",
       "         2.83671007e-03, 3.08400346e-03, 1.70637798e-02, 6.19819351e-02,\n",
       "         1.13173788e-02, 2.79808231e-02, 4.50975355e-03, 3.17499340e-02,\n",
       "         2.02527456e-02, 5.00882510e-03, 1.35256220e-02, 3.03522013e-02,\n",
       "         4.69127111e-02, 2.05831546e-02, 1.47263557e-02, 5.92329949e-02,\n",
       "         4.45348322e-02, 1.69695411e-02, 4.37450111e-02, 1.27684355e-01,\n",
       "         7.99703132e-03, 3.74533236e-02, 6.47069439e-02, 5.34261763e-02,\n",
       "         3.03356181e-04, 4.10551503e-02, 4.02606558e-03, 4.46273796e-02,\n",
       "         3.55081521e-02, 7.13991467e-03, 3.31636630e-02, 1.24387089e-02,\n",
       "         5.84971756e-02, 6.44483715e-02, 6.63262010e-02, 4.54661399e-02,\n",
       "         2.69248206e-02, 1.16028590e-03, 1.01873977e-02, 2.00686287e-02,\n",
       "         1.02002406e-02, 6.17080219e-02, 1.71656124e-02, 6.62634447e-02,\n",
       "         1.03577666e-01, 3.74087468e-02, 9.91717074e-03, 9.25935863e-04,\n",
       "         3.67989615e-02, 2.28409730e-02, 2.82557774e-02, 7.62839429e-03,\n",
       "         2.28177290e-03, 1.35822697e-02, 1.09535791e-01, 1.97690278e-02,\n",
       "         5.05593009e-02, 1.31198466e-02, 2.64160782e-02, 3.62534937e-03,\n",
       "         4.60862294e-02, 2.30952337e-01, 7.66304210e-02, 5.48551455e-02,\n",
       "         2.18657367e-02, 3.98409441e-02, 1.43175334e-01, 3.17997225e-02,\n",
       "         3.68142426e-02, 1.19539127e-02, 2.08155625e-02, 1.72807742e-02,\n",
       "         2.29502376e-02, 1.04346216e-01, 2.84596253e-02, 5.41946515e-02,\n",
       "         5.55339865e-02, 8.24381188e-02, 3.49338651e-02, 1.86428502e-02,\n",
       "         2.40655467e-02, 2.84177847e-02, 1.05448980e-02, 6.97263628e-02,\n",
       "         3.39089595e-02, 1.35750358e-03, 3.48841995e-02, 3.40575762e-02,\n",
       "         6.56144833e-03, 2.79294178e-02, 6.01490028e-02, 9.78605077e-03,\n",
       "         1.16609195e-02, 2.99937557e-02, 3.01729534e-02, 9.31443833e-03,\n",
       "         1.59126278e-02, 1.58087034e-02, 4.90833409e-02, 3.87430890e-04,\n",
       "         1.79794542e-02, 2.22623278e-03, 1.38525935e-02, 2.55077481e-02,\n",
       "         7.14183897e-02, 2.23511048e-02, 2.60568168e-02, 1.35653103e-02,\n",
       "         1.19655859e-02, 3.28501761e-02, 2.31882539e-02, 4.54038382e-02,\n",
       "         7.43012577e-02, 2.23271847e-02, 3.68357413e-02, 4.46888097e-02,\n",
       "         2.05684602e-02, 5.82311377e-02, 4.63237353e-02, 1.48475682e-02,\n",
       "         4.33048755e-02, 6.05600094e-03, 7.01633766e-02, 1.19847031e-02,\n",
       "         5.53279230e-03, 1.64145008e-02, 1.41203329e-02, 1.26827061e-02,\n",
       "         3.25759910e-02, 2.96771992e-02, 4.23732959e-02, 1.99251566e-02,\n",
       "         6.15919344e-02, 4.04603817e-02, 1.74137913e-02, 2.23834589e-02,\n",
       "         3.77739854e-02, 6.76360503e-02, 4.92254384e-02, 4.86482866e-02,\n",
       "         7.07566068e-02, 1.08707324e-01, 4.83832918e-02, 4.66556475e-02,\n",
       "         1.24126099e-01, 1.55388834e-02, 3.46585773e-02, 2.70151533e-02,\n",
       "         3.08178575e-03, 1.47519838e-02, 6.16566539e-02, 5.73675223e-02,\n",
       "         1.83475651e-02, 9.77973118e-02, 2.05981806e-02, 2.03473121e-02,\n",
       "         2.28916318e-03, 4.09245864e-03, 8.44070315e-02, 2.94068959e-02,\n",
       "         2.55387533e-03, 5.35207614e-02, 2.29885429e-02, 4.98621650e-02,\n",
       "         1.30421463e-02, 2.01860163e-03, 1.19399438e-02, 3.43194492e-02,\n",
       "         3.58382910e-02, 3.09872404e-02, 2.70883366e-02, 6.99606240e-02,\n",
       "         9.88198258e-03, 5.11733294e-02, 2.42860038e-02, 2.11371928e-02,\n",
       "         5.94825633e-02, 6.53258935e-02, 6.67492673e-02, 1.29883150e-02,\n",
       "         1.71875730e-02, 1.56292617e-02, 5.90871945e-02, 3.73559371e-02,\n",
       "         4.18308470e-03, 5.82839176e-02, 5.02318107e-02, 7.40595907e-03,\n",
       "         1.62067842e-02, 6.94917291e-02, 7.06464201e-02, 4.81263995e-02,\n",
       "         1.02475779e-02, 5.76598532e-02, 5.27106039e-02, 5.71587635e-03,\n",
       "         3.48625667e-02, 5.63921966e-02, 2.70036310e-02, 2.09571514e-02,\n",
       "         4.40519750e-02, 9.77832545e-03, 6.52354509e-02, 6.03247881e-02,\n",
       "         6.44070506e-02, 5.70370555e-02, 3.85070853e-02, 1.49644287e-02,\n",
       "         3.89107428e-02, 4.34969366e-02, 1.46870343e-02, 3.24719260e-03,\n",
       "         4.64305654e-02, 1.15704343e-01, 2.99398918e-02, 4.04301137e-02,\n",
       "         7.61272460e-02, 5.21983542e-02, 5.54967159e-03, 2.10890993e-02,\n",
       "         9.88784991e-03, 2.19153650e-02, 6.88248128e-02, 4.63963673e-03,\n",
       "         6.00909218e-02, 5.16850092e-02, 2.05563437e-02, 2.09896415e-02,\n",
       "         1.24875661e-02, 2.59529222e-02, 1.68245807e-02, 1.59180053e-02,\n",
       "         6.20965511e-02, 2.91737765e-02, 3.56527604e-02, 8.87307376e-02,\n",
       "         2.16968823e-02, 4.89246845e-02, 4.85081561e-02, 9.02747586e-02,\n",
       "         8.41781721e-02, 6.27317876e-02, 4.86202724e-02, 5.71969012e-03,\n",
       "         4.35438976e-02, 1.53609598e-02, 1.39254794e-01, 4.15986441e-02,\n",
       "         2.82965023e-02, 4.30277176e-03, 2.74608354e-03, 5.65632284e-02,\n",
       "         6.64311871e-02, 1.17559908e-02, 5.61958067e-02, 2.97045819e-02,\n",
       "         3.86567451e-02, 9.30251647e-03, 8.73424578e-03, 2.50874702e-02,\n",
       "         5.15462924e-03, 4.51931125e-03, 5.38566895e-02, 6.10879138e-02,\n",
       "         1.53464568e-03, 8.50975588e-02, 7.00184926e-02, 4.64704372e-02,\n",
       "         8.13119709e-02, 1.31753549e-01, 5.05732186e-02, 1.06845684e-02,\n",
       "         3.87471989e-02, 3.14763151e-02, 7.36696739e-03, 4.65231277e-02,\n",
       "         8.79523158e-02, 4.89682667e-02, 5.84886670e-02, 1.66027546e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 11,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([2.05495358e-02, 8.46681744e-03, 4.70235683e-02, 3.38394828e-02,\n",
       "         1.10474685e-02, 1.70563366e-02, 2.45318897e-02, 1.18143745e-01,\n",
       "         3.04490384e-02, 3.15080881e-02, 1.34410188e-02, 4.18712758e-03,\n",
       "         1.44355111e-02, 1.64031051e-02, 1.36539647e-02, 3.36060636e-02,\n",
       "         1.00413512e-03, 1.23447347e-02, 2.08945964e-02, 1.63380615e-02,\n",
       "         9.95993521e-03, 6.59179389e-02, 1.84793621e-02, 4.85053137e-02,\n",
       "         3.36415716e-03, 3.08264140e-02, 1.37040450e-04, 7.58454809e-03,\n",
       "         1.21581424e-02, 3.83022949e-02, 8.63744020e-02, 6.48640853e-04,\n",
       "         1.56785101e-02, 2.70539615e-02, 3.12000066e-02, 2.93371864e-02,\n",
       "         6.08143695e-02, 1.06164347e-02, 9.30103660e-02, 1.69365983e-02,\n",
       "         2.95363497e-02, 2.87249889e-02, 2.95327487e-03, 3.05499062e-02,\n",
       "         2.78517492e-02, 2.15874910e-02, 4.23282236e-02, 1.28237242e-02,\n",
       "         7.21820667e-02, 4.79675010e-02, 8.25695042e-03, 3.60356569e-02,\n",
       "         2.81603690e-02, 4.47324924e-02, 1.27295122e-01, 1.42372753e-02,\n",
       "         5.37786745e-02, 2.12436565e-03, 3.51162814e-02, 2.97661051e-02,\n",
       "         9.66725200e-02, 4.96855341e-02, 4.90539428e-03, 1.82449389e-02,\n",
       "         8.39568395e-03, 3.79665196e-02, 4.64354493e-02, 4.90146689e-02,\n",
       "         4.89675850e-02, 1.31202498e-02, 8.48030970e-02, 3.93258817e-02,\n",
       "         3.86662371e-02, 2.47922763e-02, 7.34442566e-03, 5.54061234e-02,\n",
       "         3.69545594e-02, 7.42746703e-03, 4.24363464e-02, 4.40710932e-02,\n",
       "         6.94147032e-03, 4.24923003e-02, 2.88404878e-02, 1.19151011e-01,\n",
       "         3.53944898e-02, 3.73124070e-02, 2.73277704e-02, 1.97534896e-02,\n",
       "         2.90654488e-02, 1.19434297e-02, 8.27658921e-02, 2.72315908e-02,\n",
       "         1.79583300e-02, 6.95376694e-02, 4.57220189e-02, 3.63809802e-02,\n",
       "         3.11390078e-03, 7.60250166e-02, 2.79195718e-02, 1.90515202e-02,\n",
       "         5.78186885e-02, 6.36699796e-02, 1.72377694e-02, 4.28338088e-02,\n",
       "         3.30381654e-02, 2.96145026e-02, 1.80864113e-03, 2.56757718e-02,\n",
       "         6.01553731e-02, 1.19704969e-01, 2.33855937e-02, 8.87608752e-02,\n",
       "         6.39703795e-02, 6.19635452e-03, 5.69659397e-02, 6.81732898e-04,\n",
       "         1.49860904e-02, 4.91288602e-02, 5.36205899e-03, 3.26773524e-02,\n",
       "         6.88214153e-02, 1.30510181e-02, 7.32719433e-03, 7.38558024e-02,\n",
       "         2.77173258e-02, 3.09012756e-02, 4.72292081e-02, 3.17677739e-03,\n",
       "         5.77490106e-02, 6.09502159e-02, 1.12436749e-01, 1.58555895e-01,\n",
       "         4.73035425e-02, 2.06837710e-02, 7.80981383e-04, 8.39750841e-03,\n",
       "         1.84540469e-02, 7.61717558e-02, 3.38423741e-03, 7.03304186e-02,\n",
       "         2.69960836e-02, 3.77219133e-02, 3.45086935e-03, 2.35124771e-02,\n",
       "         6.22526892e-02, 2.76652649e-02, 2.20706295e-02, 7.28897704e-03,\n",
       "         6.11979188e-03, 4.49305847e-02, 1.02482006e-01, 2.93162074e-02,\n",
       "         2.14457754e-02, 4.39484650e-03, 4.77225780e-02, 1.27578914e-01,\n",
       "         2.11033016e-03, 2.54128985e-02, 5.21490127e-02, 8.25813599e-03,\n",
       "         4.74359887e-03, 6.57759532e-02, 8.01662356e-02, 3.22532058e-02,\n",
       "         7.35959560e-02, 9.73290652e-02, 2.78370958e-02, 1.33318119e-02,\n",
       "         7.09418952e-02, 7.12581873e-02, 1.19195534e-02, 1.41121098e-03,\n",
       "         7.87540823e-02, 3.55015919e-02, 1.87886544e-02, 2.91314488e-03,\n",
       "         1.96976028e-02, 6.27433136e-02, 4.59003309e-03, 7.47064278e-02,\n",
       "         3.78432381e-03, 1.72577687e-02, 2.88515948e-02, 8.13255738e-03,\n",
       "         7.58638158e-02, 2.31157765e-02, 1.17951501e-02, 7.00792000e-02,\n",
       "         1.10530863e-02, 8.40304643e-02, 1.44097358e-02, 8.05649254e-03,\n",
       "         4.47402149e-02, 3.99506986e-02, 2.84792744e-02, 2.56764479e-02,\n",
       "         3.21676233e-03, 4.31649648e-02, 3.00343875e-02, 8.90005007e-03,\n",
       "         1.36589063e-02, 1.97187718e-03, 7.97512196e-03, 7.76024610e-02,\n",
       "         1.27398707e-02, 8.18041246e-03, 2.86998991e-02, 3.14330123e-02,\n",
       "         2.06514233e-04, 1.80647485e-02, 5.39761819e-02, 5.55575863e-02,\n",
       "         3.22911739e-02, 4.66988534e-02, 2.51553208e-03, 9.55124870e-02,\n",
       "         1.16605274e-02, 1.86490230e-02, 2.01185048e-02, 3.13889384e-02,\n",
       "         1.62776224e-02, 5.90099804e-02, 1.08038731e-01, 4.11206931e-02,\n",
       "         1.21709285e-02, 7.01694041e-02, 4.04242650e-02, 4.16016132e-02,\n",
       "         4.14320529e-02, 5.20723220e-03, 1.02538969e-02, 4.01209593e-02,\n",
       "         6.53862767e-03, 5.68571792e-04, 4.23729755e-02, 4.39586379e-02,\n",
       "         1.13994330e-02, 1.05774775e-01, 5.63753732e-02, 1.90165509e-02,\n",
       "         9.58582759e-03, 1.60572566e-02, 1.80165507e-02, 5.61492518e-02,\n",
       "         1.01197679e-02, 1.99941751e-02, 1.30259231e-01, 5.96108250e-02,\n",
       "         3.26498947e-03, 1.50453020e-02, 6.31152317e-02, 7.80574931e-03,\n",
       "         5.61387464e-03, 6.16186038e-02, 2.78962534e-02, 7.10007325e-02,\n",
       "         8.44912510e-03, 3.17743756e-02, 4.18170169e-02, 6.14288710e-02,\n",
       "         3.71998809e-02, 4.44331346e-03, 4.91562821e-02, 3.43889743e-02,\n",
       "         2.57012434e-02, 8.12923443e-03, 1.25549044e-02, 9.99981090e-02,\n",
       "         7.32800961e-02, 1.37515524e-02, 8.94139172e-04, 6.05237298e-02,\n",
       "         3.18366964e-03, 5.21358624e-02, 5.86197786e-02, 1.61376465e-02,\n",
       "         4.93290089e-02, 9.73853841e-03, 2.43272614e-02, 8.62603076e-03,\n",
       "         5.73806353e-02, 2.22196002e-02, 5.88640105e-03, 7.81208826e-07,\n",
       "         1.44457761e-02, 3.62172835e-02, 6.12795725e-02, 5.65982424e-02,\n",
       "         1.90917458e-02, 8.37333351e-02, 9.93483234e-03, 1.57022430e-03,\n",
       "         6.13192469e-02, 6.66794777e-02, 3.93440090e-02, 4.40816432e-02,\n",
       "         7.81086534e-02, 7.90084675e-02, 1.14284037e-02, 1.65117402e-02,\n",
       "         1.46796508e-03, 8.08640104e-03, 7.11630273e-04, 4.55232114e-02,\n",
       "         5.93947247e-02, 8.70626494e-02, 5.01279011e-02, 3.81589197e-02,\n",
       "         4.41013277e-02, 5.15485667e-02, 5.01577184e-02, 1.11903632e-02,\n",
       "         6.00398071e-02, 1.41498027e-02, 5.16924076e-03, 5.92371868e-03,\n",
       "         3.65222828e-03, 4.24945652e-02, 6.66093454e-03, 5.21722920e-02,\n",
       "         5.20609543e-02, 8.40632766e-02, 2.00094339e-02, 9.24167037e-03,\n",
       "         5.04452810e-02, 5.66130541e-02, 8.11228827e-02, 4.44903597e-02,\n",
       "         2.62102149e-02, 6.71225116e-02, 6.21285699e-02, 5.79060838e-02,\n",
       "         1.21172860e-04, 1.42779332e-02, 7.56686330e-02, 3.14864330e-02,\n",
       "         6.29549548e-02, 2.28746352e-03, 2.10236050e-02, 1.16898736e-04,\n",
       "         4.17775624e-02, 2.73711290e-02, 4.35311608e-02, 3.55546027e-02,\n",
       "         8.91485587e-02, 4.08318713e-02, 2.75580324e-02, 3.72292586e-02,\n",
       "         4.06371243e-02, 2.60705091e-02, 8.60422403e-02, 3.39269638e-02,\n",
       "         2.49057692e-02, 1.03896288e-02, 4.47680913e-02, 5.88642480e-03,\n",
       "         3.05796284e-02, 8.64197835e-02, 2.22190171e-02, 4.31015901e-02,\n",
       "         8.71790722e-02, 2.80996636e-02, 7.93421827e-03, 1.18976377e-03,\n",
       "         3.33180986e-02, 9.18707475e-02, 5.31279258e-02, 2.72441283e-02,\n",
       "         2.90797465e-02, 8.05155654e-03, 3.81449237e-02, 5.76851740e-02,\n",
       "         5.16229123e-02, 2.44771130e-02, 4.15483825e-02, 6.48082746e-03,\n",
       "         7.53254369e-02, 8.09864551e-02, 3.99018899e-02, 1.82667263e-02,\n",
       "         1.44646382e-02, 4.29741777e-02, 4.45872433e-02, 5.62553033e-02,\n",
       "         1.35151548e-02, 5.04701491e-03, 1.88191347e-02, 1.95881762e-02,\n",
       "         7.81979114e-02, 1.34868054e-02, 2.28998680e-02, 1.12372451e-02,\n",
       "         1.20668840e-02, 9.70994867e-03, 4.87009212e-02, 1.14062401e-02,\n",
       "         3.58712748e-02, 9.83758271e-03, 5.37413731e-03, 1.70749221e-02,\n",
       "         2.77984608e-02, 4.17224914e-02, 7.79938996e-02, 4.92997747e-03,\n",
       "         3.25790122e-02, 3.79451215e-02, 2.41153408e-02, 2.25549359e-02,\n",
       "         1.03871413e-01, 5.00077829e-02, 3.16997766e-02, 7.87329599e-02,\n",
       "         1.51880691e-02, 2.54812045e-03, 3.95499021e-02, 1.67626180e-02,\n",
       "         1.90830976e-02, 4.59362566e-03, 4.66580242e-02, 2.57356316e-02,\n",
       "         3.62664238e-02, 7.02237934e-02, 4.81652580e-02, 3.91674489e-02,\n",
       "         3.75060574e-03, 1.16133820e-02, 6.57275021e-02, 2.00268570e-02,\n",
       "         1.45706264e-02, 6.37404574e-03, 7.68731721e-03, 4.74208556e-02,\n",
       "         4.82545197e-02, 1.10290190e-02, 7.20826536e-03, 1.95613764e-02,\n",
       "         2.26348247e-02, 6.02495745e-02, 2.85134874e-02, 7.31253102e-02,\n",
       "         1.24551812e-02, 1.73957814e-02, 3.32518667e-02, 3.50122228e-02,\n",
       "         7.55733158e-03, 2.76619922e-02, 2.99893809e-03, 7.01102689e-02,\n",
       "         5.33750318e-02, 1.43138003e-02, 2.47978605e-02, 1.48661565e-02,\n",
       "         7.54832849e-02, 4.59085628e-02, 5.87069802e-02, 3.26759741e-02,\n",
       "         5.74687608e-02, 2.71493308e-02, 5.83544709e-02, 4.94269989e-02,\n",
       "         2.75887195e-02, 6.23830315e-03, 7.08976015e-02, 2.67843604e-02,\n",
       "         1.69550218e-02, 2.89461017e-02, 4.58371602e-02, 3.10091004e-02,\n",
       "         1.94742419e-02, 2.75592436e-03, 3.00301313e-02, 2.81164311e-02,\n",
       "         5.37663996e-02, 4.55314182e-02, 4.84421430e-03, 4.79741320e-02,\n",
       "         1.23397913e-02, 7.13510578e-03, 8.12161993e-03, 1.29651213e-02,\n",
       "         3.67200039e-02, 6.76238239e-02, 5.36129437e-02, 1.65234208e-02,\n",
       "         4.20222245e-02, 6.07733242e-02, 2.82013901e-02, 6.45660460e-02,\n",
       "         3.70174721e-02, 8.79463777e-02, 2.29521189e-02, 7.46715888e-02,\n",
       "         3.98587994e-03, 3.76908691e-03, 2.06608456e-02, 2.40366571e-02,\n",
       "         3.30261029e-02, 2.27896944e-02, 8.57041590e-03, 5.97725771e-02,\n",
       "         2.57935487e-02, 1.49830664e-02, 9.73305316e-04, 1.59460912e-03,\n",
       "         4.98035513e-02, 2.97219940e-02, 4.99996096e-02, 9.97266173e-03,\n",
       "         3.82788815e-02, 5.26587926e-02, 3.32995653e-02, 5.42073324e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 12,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([0.026243  , 0.04284882, 0.04928661, 0.04588095, 0.03085539,\n",
       "         0.04612511, 0.05826612, 0.04666617, 0.03372556, 0.06168013,\n",
       "         0.03768526, 0.04830078, 0.02424392, 0.04916793, 0.0432652 ,\n",
       "         0.03372121, 0.04312981, 0.04472689, 0.00960832, 0.06125252,\n",
       "         0.04852424, 0.03856962, 0.04921078, 0.0350827 , 0.05440599,\n",
       "         0.03702689, 0.05677477, 0.0408472 , 0.04464145, 0.04395816,\n",
       "         0.05578566, 0.03995546, 0.05926374, 0.03858253, 0.02918668,\n",
       "         0.04955165, 0.04668141, 0.02650979, 0.03086871, 0.04076132,\n",
       "         0.06180217, 0.04839177, 0.04999323, 0.03716978, 0.03524404,\n",
       "         0.03438932, 0.048018  , 0.0470477 , 0.04051831, 0.03645647,\n",
       "         0.04459486, 0.03986062, 0.04546331, 0.04610785, 0.0186411 ,\n",
       "         0.03996409, 0.0239576 , 0.06793991, 0.04168015, 0.06900446,\n",
       "         0.06275703, 0.03640288, 0.05162907, 0.04065073, 0.06302281,\n",
       "         0.03057385, 0.04272468, 0.04473041, 0.02739163, 0.04969957,\n",
       "         0.04292967, 0.02796737, 0.04170191, 0.03630671, 0.04300979,\n",
       "         0.05113661, 0.05868715, 0.04379226, 0.0358024 , 0.03092885,\n",
       "         0.05055478, 0.03564798, 0.03810571, 0.04565661, 0.03704072,\n",
       "         0.03858721, 0.03939857, 0.04820037, 0.05784138, 0.03268057,\n",
       "         0.05804068, 0.04755345, 0.0502442 , 0.03778768, 0.03287418,\n",
       "         0.03461354, 0.033605  , 0.04010687, 0.04097734, 0.06487742,\n",
       "         0.02383413, 0.03331861, 0.0437616 , 0.04648819, 0.0479267 ,\n",
       "         0.05149905, 0.04989682, 0.04244092, 0.04022761, 0.03836894,\n",
       "         0.03849632, 0.03650957, 0.04349972, 0.04934834, 0.05366011,\n",
       "         0.03660972, 0.04984199, 0.04185091, 0.05251364, 0.03672269,\n",
       "         0.03860136, 0.03074194, 0.05064261, 0.05702516, 0.03721491,\n",
       "         0.04209007, 0.05432444, 0.03834777, 0.04246659, 0.04213294,\n",
       "         0.04392863, 0.05258685, 0.02920754, 0.03817355, 0.03540245,\n",
       "         0.05371134, 0.04128335, 0.04916918, 0.03545219, 0.03354855,\n",
       "         0.0554155 , 0.05161078, 0.03033901, 0.04283187, 0.06589883,\n",
       "         0.04351858, 0.06668977, 0.0392829 , 0.04349183, 0.03993763,\n",
       "         0.03995817, 0.05385145, 0.04203842, 0.03780775, 0.03285365,\n",
       "         0.02299864, 0.04097607, 0.03774455, 0.04056488, 0.05179006,\n",
       "         0.04082249, 0.04736912, 0.02289915, 0.06440753, 0.04723496,\n",
       "         0.03867671, 0.04303978, 0.05016291, 0.04128764, 0.04178104,\n",
       "         0.04408243, 0.03168415, 0.03575864, 0.05749083, 0.03378524,\n",
       "         0.06292984, 0.04423741, 0.03944668, 0.0456452 , 0.01647577,\n",
       "         0.05206097, 0.0362923 , 0.04726685, 0.03734418, 0.0395274 ,\n",
       "         0.04387396, 0.05505971, 0.05629412, 0.0457958 , 0.05416406,\n",
       "         0.05076529, 0.03435931, 0.04567991, 0.0454191 , 0.04580308,\n",
       "         0.04054006, 0.06006097, 0.05771099, 0.05993884, 0.0527362 ,\n",
       "         0.05028456, 0.03601266, 0.06435328, 0.03754245, 0.03217441,\n",
       "         0.04077051, 0.04650204, 0.04099879, 0.04730638, 0.04086101,\n",
       "         0.04265233, 0.04784958, 0.03516662, 0.03699079, 0.03552938,\n",
       "         0.03748874, 0.03824246, 0.06633374, 0.06267852, 0.03890186,\n",
       "         0.05106939, 0.05259636, 0.04959249, 0.03225282, 0.03670252,\n",
       "         0.03076059, 0.03455645, 0.05387942, 0.03574886, 0.04415512,\n",
       "         0.0288595 , 0.04498067, 0.02096387, 0.0522868 , 0.04210724,\n",
       "         0.04680526, 0.06095732, 0.04859276, 0.04401574, 0.06256768,\n",
       "         0.04773294, 0.04174801, 0.0540143 , 0.07890314, 0.05292869,\n",
       "         0.03729729, 0.04804966, 0.02787172, 0.04289646, 0.0442819 ,\n",
       "         0.04242832, 0.04189866, 0.03965594, 0.04011395, 0.06002132,\n",
       "         0.05090769, 0.02317757, 0.04585813, 0.04502733, 0.04097282,\n",
       "         0.04255173, 0.04669391, 0.05595418, 0.04772878, 0.03084806,\n",
       "         0.04307903, 0.05187601, 0.0509242 , 0.03698282, 0.03420127,\n",
       "         0.03698577, 0.03545262, 0.05144119, 0.03439889, 0.03511404,\n",
       "         0.04641933, 0.0536503 , 0.01491049, 0.03461073, 0.03552453,\n",
       "         0.05486817, 0.03449179, 0.04605789, 0.04070073, 0.02598499,\n",
       "         0.03704928, 0.04001703, 0.03996814, 0.04219303, 0.04205102,\n",
       "         0.03829923, 0.04280642, 0.03601191, 0.03630463, 0.04078691,\n",
       "         0.05536794, 0.04218338, 0.00667531, 0.04621469, 0.03559776,\n",
       "         0.03379936, 0.03536002, 0.03910415, 0.04469451, 0.05750449,\n",
       "         0.03996423, 0.03022618, 0.03256951, 0.04773642, 0.03623677,\n",
       "         0.04770823, 0.03936771, 0.04499188, 0.0583833 , 0.0347877 ,\n",
       "         0.06173974, 0.05317482, 0.04571137, 0.05180198, 0.04049345,\n",
       "         0.03321859, 0.05182914, 0.04506126, 0.02760217, 0.02484453,\n",
       "         0.03688719, 0.04698002, 0.02773642, 0.03896042, 0.05557028,\n",
       "         0.03938217, 0.02772388, 0.04277316, 0.05199899, 0.03868076,\n",
       "         0.04339876, 0.05264481, 0.0463263 , 0.04283319, 0.02951183,\n",
       "         0.04447712, 0.04556243, 0.04098668, 0.04728227, 0.0492357 ,\n",
       "         0.05400281, 0.04444288, 0.06049306, 0.03587471, 0.02348616,\n",
       "         0.03201574, 0.06250988, 0.04134229, 0.03028723, 0.04903087,\n",
       "         0.03402258, 0.03324466, 0.03813969, 0.04424889, 0.04838815,\n",
       "         0.02891014, 0.03997035, 0.04828753, 0.05966883, 0.03967491,\n",
       "         0.04877242, 0.03715548, 0.05292832, 0.04319477, 0.03368707,\n",
       "         0.04007759, 0.04368219, 0.04362791, 0.05213941, 0.04856515,\n",
       "         0.0361871 , 0.03224507, 0.04199936, 0.04219515, 0.04171916,\n",
       "         0.02488555, 0.03844009, 0.06249454, 0.0457867 , 0.02976461,\n",
       "         0.04927589, 0.04878801, 0.05655928, 0.04259078, 0.02272907,\n",
       "         0.02971857, 0.0565386 , 0.04296023, 0.03242355, 0.04692643,\n",
       "         0.0447717 , 0.05075884, 0.04680483, 0.04497885, 0.05924936,\n",
       "         0.04893244, 0.04868085, 0.02291279, 0.05053875, 0.07386453,\n",
       "         0.03986483, 0.03276422, 0.0513824 , 0.05710338, 0.02603055,\n",
       "         0.03884367, 0.0354374 , 0.02828151, 0.04230828, 0.06033283,\n",
       "         0.04185233, 0.04881056, 0.04439052, 0.04977939, 0.03709836,\n",
       "         0.04785081, 0.02561613, 0.04337966, 0.05584829, 0.0290746 ,\n",
       "         0.03484993, 0.07230988, 0.04187518, 0.04637987, 0.04054836,\n",
       "         0.04997855, 0.0379217 , 0.04407116, 0.07625333, 0.03804009,\n",
       "         0.05102437, 0.03011935, 0.06316654, 0.0551733 , 0.05107372,\n",
       "         0.06011596, 0.04304807, 0.04074061, 0.04545977, 0.03210222,\n",
       "         0.04578164, 0.03988086, 0.04090253, 0.04819021, 0.04053074,\n",
       "         0.05997208, 0.03458667, 0.03954307, 0.04045144, 0.03240182,\n",
       "         0.05048453, 0.05098258, 0.02844598, 0.0380442 , 0.03018602,\n",
       "         0.03986309, 0.03615704, 0.05813035, 0.04814731, 0.0365218 ,\n",
       "         0.04145615, 0.0496699 , 0.03882176, 0.03953596, 0.02426545,\n",
       "         0.03906634, 0.02089828, 0.0420081 , 0.04803141, 0.04999459,\n",
       "         0.05442512, 0.05672741, 0.03632032, 0.03291865, 0.06480607,\n",
       "         0.04013599, 0.0415092 , 0.04464703, 0.04776442, 0.02366274,\n",
       "         0.03735033, 0.0482954 , 0.05277864, 0.04667283, 0.00913106,\n",
       "         0.04749993, 0.04553764, 0.06124659, 0.02093683, 0.03496496,\n",
       "         0.0432258 , 0.04294342, 0.04473504, 0.02394718, 0.02317208,\n",
       "         0.02884549, 0.03887029, 0.03799867, 0.03421367, 0.04779763,\n",
       "         0.0540615 , 0.03779609, 0.0455935 , 0.03716482, 0.02642578,\n",
       "         0.03655711, 0.05072619], dtype=float32)}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CIFAR 100\n",
    "# ==============================================================================\n",
    "# 1. Cáº¥u hÃ¬nh & Load Model\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B' # Hoáº·c loáº¡i báº¡n Ä‘Ã£ train\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config.batch_size = 64\n",
    "# Táº¡o model vÃ  load weight cÅ©\n",
    "model = ModifiedVGG16Model(config).to(config.device)\n",
    "checkpoint = torch.load(\"./checkpoint/vgg16_cifar100_baseline.pth\", map_location=config.device) # ÄÆ°á»ng dáº«n file ckpt cá»§a báº¡n\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "print(f\"âœ… Loaded model from epoch {checkpoint['epoch']} with Acc: {checkpoint['acc']}%\")\n",
    "\n",
    "# 2. Khá»Ÿi táº¡o Helper cÅ© (Ä‘á»ƒ láº¥y dá»¯ liá»‡u train tÃ­nh rank)\n",
    "# Helper nÃ y chá»‹u trÃ¡ch nhiá»‡m tÃ­nh Taylor Score\n",
    "old_pruner = VGG16Pruner(config, model, save_name='100_ga_optimise_ckpt.pth') \n",
    "\n",
    "# 3. Khá»Ÿi táº¡o GA Pruner (Code má»›i)\n",
    "input_size = torch.randn(1, 3, 32, 32).to(config.device)\n",
    "ga_pruner = GAPruner(model, config, input_size)\n",
    "\n",
    "ga_pruner.strategy_mode = 'layer_wise'\n",
    "\n",
    "# 4. XÃ¢y dá»±ng báº£ng tra cá»©u (BÆ°á»›c nÃ y cháº¡y ~1 epoch Ä‘á»ƒ tÃ­nh rank, máº¥t khoáº£ng 1-2 phÃºt)\n",
    "ga_pruner.build_ranking_tables(old_pruner, old_pruner.train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd454bbc",
   "metadata": {},
   "source": [
    "### 5.1.2. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40a46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model from epoch 183 with Acc: 93.62%\n",
      "Base MACs: 0.3334G | Base Params: 33.6467M\n",
      "Analyzed 13 Conv layers for fast estimation.\n",
      "\n",
      "Step 1: Building Ranking Tables (Pre-computation)...\n",
      "   Running ranking epoch (accumulating gradients)...\n",
      "\n",
      "ðŸ“Š --- SMART MATCHING LAYERS ---\n",
      "   Raw ranks key type: <class 'int'>\n",
      "   Layer 12: max=0.0697, mean=0.0437\n",
      "   Layer 11: max=0.1560, mean=0.0352\n",
      "   Layer 2: max=0.3505, mean=0.0622\n",
      "   Layer 1: max=0.7757, mean=0.0670\n",
      "   Layer 0: max=0.4778, mean=0.0833\n",
      "âœ… Successfully built ranking tables for 13 layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'layer_idx': 0,\n",
       "  'num_filters': 64,\n",
       "  'scores': array([0.07468866, 0.02349718, 0.15162334, 0.02231721, 0.07752145,\n",
       "         0.00569592, 0.16902277, 0.13057435, 0.06815179, 0.01905006,\n",
       "         0.14574745, 0.11305898, 0.01357183, 0.4159274 , 0.05675606,\n",
       "         0.0352116 , 0.47779316, 0.06128458, 0.01491291, 0.06553922,\n",
       "         0.0755    , 0.03088428, 0.09835096, 0.03474858, 0.10245911,\n",
       "         0.03778552, 0.05343438, 0.04287936, 0.00758643, 0.14615206,\n",
       "         0.03836773, 0.01202247, 0.01172155, 0.03664899, 0.07602607,\n",
       "         0.03413782, 0.07106184, 0.02750696, 0.03566423, 0.06748302,\n",
       "         0.00832607, 0.049351  , 0.08678395, 0.23590322, 0.00348327,\n",
       "         0.06135817, 0.08178821, 0.35341465, 0.01011598, 0.01818684,\n",
       "         0.03268743, 0.04278018, 0.00648699, 0.1634859 , 0.07249385,\n",
       "         0.11171229, 0.05640838, 0.0236412 , 0.17080711, 0.10034201,\n",
       "         0.29379922, 0.03625072, 0.07910337, 0.04976498], dtype=float32)},\n",
       " {'layer_idx': 1,\n",
       "  'num_filters': 64,\n",
       "  'scores': array([0.13429092, 0.14089747, 0.05436997, 0.06799351, 0.0171243 ,\n",
       "         0.07822107, 0.01732073, 0.00326346, 0.04728686, 0.02930735,\n",
       "         0.11990818, 0.0042913 , 0.21314582, 0.09314145, 0.03448477,\n",
       "         0.03547528, 0.07166401, 0.10638604, 0.02300506, 0.01444573,\n",
       "         0.08473872, 0.01206653, 0.00843362, 0.00096483, 0.02379704,\n",
       "         0.07029209, 0.77567273, 0.26425886, 0.02496821, 0.00164461,\n",
       "         0.00956291, 0.02073344, 0.07479604, 0.01749502, 0.11079831,\n",
       "         0.01420182, 0.15190746, 0.01875533, 0.06127908, 0.01037498,\n",
       "         0.02449932, 0.00104465, 0.00297521, 0.00399918, 0.02705717,\n",
       "         0.0020958 , 0.01196754, 0.03705845, 0.1996603 , 0.009434  ,\n",
       "         0.04799155, 0.09048144, 0.02776556, 0.0557072 , 0.03181595,\n",
       "         0.03523136, 0.15192793, 0.00646834, 0.12587683, 0.04204219,\n",
       "         0.06067546, 0.12410176, 0.06667988, 0.04045454], dtype=float32)},\n",
       " {'layer_idx': 2,\n",
       "  'num_filters': 128,\n",
       "  'scores': array([1.12220049e-02, 1.43605312e-02, 3.07701882e-02, 6.41524186e-03,\n",
       "         7.04504400e-02, 2.12402567e-02, 8.06005001e-02, 3.16785127e-02,\n",
       "         3.43127772e-02, 8.46093427e-03, 9.39034484e-03, 1.28316144e-02,\n",
       "         4.48417738e-02, 6.04074448e-02, 3.43151693e-03, 8.54327530e-03,\n",
       "         4.30861935e-02, 3.32095549e-02, 5.33118621e-02, 1.32864878e-01,\n",
       "         1.31859422e-01, 1.90902948e-01, 3.71322855e-02, 7.15900138e-02,\n",
       "         5.47217056e-02, 3.19124162e-02, 7.18334913e-02, 8.47027972e-02,\n",
       "         2.17092335e-02, 5.60566667e-04, 1.00715429e-01, 2.38536336e-02,\n",
       "         1.69332549e-02, 4.03438471e-02, 4.67258543e-02, 5.57158096e-03,\n",
       "         4.17877622e-02, 4.67451401e-02, 2.23405417e-02, 1.28931198e-02,\n",
       "         1.02497779e-01, 7.91551359e-03, 1.52674653e-02, 7.87272602e-02,\n",
       "         5.03368303e-02, 1.02462657e-01, 9.82419681e-03, 4.83162180e-02,\n",
       "         1.18385501e-01, 9.53915268e-02, 5.70414439e-02, 4.20097634e-02,\n",
       "         1.51065998e-02, 1.13286734e-01, 3.79714184e-02, 1.63962003e-02,\n",
       "         1.23314813e-01, 1.59709267e-02, 3.23490620e-01, 3.37916054e-03,\n",
       "         1.77730061e-02, 2.49994293e-01, 8.93172547e-02, 6.35236949e-02,\n",
       "         9.76827741e-02, 3.10113598e-02, 1.63699210e-01, 1.17331326e-01,\n",
       "         1.27596363e-01, 3.10981944e-02, 4.07408513e-02, 6.26806393e-02,\n",
       "         2.71351516e-01, 4.64981720e-02, 1.29082554e-03, 2.90012993e-02,\n",
       "         2.29692295e-01, 8.87151137e-02, 2.94679347e-02, 5.86853139e-02,\n",
       "         9.05198231e-02, 3.63023505e-02, 1.90490503e-02, 1.34587094e-01,\n",
       "         1.44531757e-01, 5.97338043e-02, 1.15904301e-01, 1.85477585e-01,\n",
       "         1.46583244e-01, 1.98394340e-02, 9.08390805e-03, 8.56390893e-02,\n",
       "         3.87594029e-02, 7.59299546e-02, 3.50543112e-01, 9.33472707e-04,\n",
       "         3.70466174e-03, 4.05202545e-02, 4.56950441e-02, 3.14764939e-02,\n",
       "         9.55445021e-02, 1.53368870e-02, 5.90149723e-02, 1.20798513e-01,\n",
       "         4.16791588e-02, 1.41414395e-02, 2.66466197e-02, 3.27890832e-03,\n",
       "         3.55375260e-02, 5.04178479e-02, 2.90465050e-05, 2.65228953e-02,\n",
       "         4.57249023e-02, 3.10885236e-02, 1.07811287e-01, 1.25034079e-01,\n",
       "         9.27362964e-02, 4.25773524e-02, 2.62425840e-03, 1.33211955e-01,\n",
       "         2.74133701e-02, 2.52341051e-02, 1.00034781e-01, 1.38389179e-02,\n",
       "         2.14099884e-02, 4.46018726e-02, 7.85922725e-03, 3.21610495e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 3,\n",
       "  'num_filters': 128,\n",
       "  'scores': array([0.11958288, 0.26538748, 0.17581041, 0.15314002, 0.07844546,\n",
       "         0.0123064 , 0.17646417, 0.10527749, 0.09587903, 0.0116688 ,\n",
       "         0.00039676, 0.00450919, 0.06709798, 0.02425919, 0.10520338,\n",
       "         0.02266824, 0.05090536, 0.0586647 , 0.06523641, 0.05788733,\n",
       "         0.10242333, 0.08978327, 0.02788629, 0.02328948, 0.02174069,\n",
       "         0.09256633, 0.06705892, 0.1504308 , 0.06006692, 0.03688829,\n",
       "         0.20890294, 0.141017  , 0.03278577, 0.0400993 , 0.03191557,\n",
       "         0.00437177, 0.01603748, 0.10816705, 0.02840436, 0.08680958,\n",
       "         0.04003675, 0.08792421, 0.05862065, 0.0584336 , 0.01344056,\n",
       "         0.03678422, 0.08401112, 0.00538921, 0.02248383, 0.10530072,\n",
       "         0.0313601 , 0.01727229, 0.10994126, 0.00740464, 0.03685796,\n",
       "         0.05455224, 0.03198534, 0.07696674, 0.15488723, 0.03636216,\n",
       "         0.00836384, 0.00644969, 0.10410678, 0.2324588 , 0.0818825 ,\n",
       "         0.12395173, 0.02765645, 0.13184677, 0.08570545, 0.00651495,\n",
       "         0.10616251, 0.05191432, 0.02323601, 0.15480828, 0.03560777,\n",
       "         0.01410871, 0.07448643, 0.04543266, 0.03386728, 0.05312751,\n",
       "         0.12052358, 0.06137941, 0.10543415, 0.22149345, 0.07951795,\n",
       "         0.00986124, 0.04652406, 0.01844955, 0.08282133, 0.16200404,\n",
       "         0.03815644, 0.0462391 , 0.0218762 , 0.03802838, 0.11230076,\n",
       "         0.07374275, 0.00867045, 0.1664184 , 0.06604978, 0.00353022,\n",
       "         0.07605536, 0.03238026, 0.02510119, 0.03942255, 0.10975132,\n",
       "         0.00520809, 0.12841067, 0.16360709, 0.15656956, 0.04084677,\n",
       "         0.05730441, 0.01890378, 0.08347852, 0.07464607, 0.05126067,\n",
       "         0.13647129, 0.05575728, 0.02883511, 0.10011514, 0.16995002,\n",
       "         0.04798866, 0.09890175, 0.04034113, 0.07590222, 0.05426694,\n",
       "         0.02733207, 0.01255782, 0.05737547], dtype=float32)},\n",
       " {'layer_idx': 4,\n",
       "  'num_filters': 256,\n",
       "  'scores': array([6.60784543e-02, 1.83488410e-02, 5.05510382e-02, 8.52404907e-02,\n",
       "         3.72422412e-02, 3.59226838e-02, 3.65927108e-02, 2.57574767e-02,\n",
       "         1.43287107e-02, 1.34629160e-01, 1.55193629e-02, 3.20521072e-02,\n",
       "         7.16808671e-03, 3.33194435e-02, 8.36536102e-03, 6.47092378e-03,\n",
       "         4.76458929e-02, 3.63122970e-02, 3.64153683e-02, 4.04409273e-03,\n",
       "         1.36888370e-01, 4.96779121e-02, 5.06379642e-03, 7.11561413e-04,\n",
       "         6.28847349e-03, 5.47218286e-02, 9.73945484e-02, 3.10482532e-02,\n",
       "         1.50201488e-02, 5.67377433e-02, 2.17627473e-02, 2.91348323e-02,\n",
       "         4.11456302e-02, 1.08578512e-02, 1.17746383e-01, 8.60289037e-02,\n",
       "         1.53191779e-02, 6.86940849e-02, 3.94738913e-02, 3.57681774e-02,\n",
       "         4.25628275e-02, 1.66597534e-02, 6.27003387e-02, 4.91529889e-02,\n",
       "         1.83697266e-03, 5.24215102e-02, 6.02331348e-02, 1.50893152e-01,\n",
       "         1.03432603e-01, 7.20664933e-02, 5.18937632e-02, 8.29818100e-02,\n",
       "         1.41184896e-01, 3.41348015e-02, 1.21239834e-01, 6.03538752e-03,\n",
       "         2.37082578e-02, 1.79298762e-02, 1.14220455e-02, 9.63224024e-02,\n",
       "         6.84923679e-02, 9.99715179e-02, 2.03411356e-02, 6.16547614e-02,\n",
       "         2.29978710e-02, 1.99638584e-04, 5.12800226e-03, 8.07913467e-02,\n",
       "         2.41477937e-02, 2.38880999e-02, 2.61972565e-02, 3.45542580e-02,\n",
       "         1.04163922e-01, 4.37140651e-02, 2.96036124e-01, 4.16272767e-02,\n",
       "         3.05355899e-02, 1.08830750e-01, 3.22493981e-03, 3.59136350e-02,\n",
       "         3.84345092e-02, 9.16939750e-02, 8.60690419e-03, 6.23170026e-02,\n",
       "         3.29540633e-02, 4.09798697e-02, 3.00904177e-03, 8.16774461e-03,\n",
       "         9.26838815e-02, 5.31132929e-02, 2.81381179e-02, 5.77574521e-02,\n",
       "         2.92450339e-02, 1.47287697e-02, 8.21577385e-02, 3.69692664e-03,\n",
       "         1.29089393e-02, 2.31768787e-02, 1.10988602e-01, 4.95959409e-02,\n",
       "         3.16443890e-02, 4.82339486e-02, 7.94641152e-02, 3.98753136e-02,\n",
       "         3.58005776e-03, 3.05452291e-02, 3.75822745e-02, 1.78804956e-02,\n",
       "         2.13443171e-02, 7.88831934e-02, 6.81205764e-02, 8.70044809e-03,\n",
       "         7.92037174e-02, 3.07297744e-02, 8.07884037e-02, 4.99587394e-02,\n",
       "         3.46535817e-02, 7.62933716e-02, 6.06081448e-03, 1.89911332e-02,\n",
       "         3.18050794e-02, 8.55416507e-02, 1.88485887e-02, 1.01632171e-03,\n",
       "         4.15308885e-02, 1.73127335e-02, 4.06507812e-02, 1.79454628e-02,\n",
       "         2.87854206e-02, 1.93657670e-02, 6.73934072e-02, 3.16131562e-02,\n",
       "         1.05969012e-01, 5.17145470e-02, 1.32273406e-01, 8.17734003e-02,\n",
       "         1.31475762e-03, 1.40499910e-02, 2.62420159e-02, 4.58396189e-02,\n",
       "         1.09717110e-02, 6.10017590e-03, 1.85590982e-02, 1.81387570e-02,\n",
       "         2.46302560e-02, 4.29214165e-02, 2.32743416e-02, 3.32905836e-02,\n",
       "         7.56732002e-02, 7.91212078e-03, 1.29138321e-01, 6.03921227e-02,\n",
       "         5.12453094e-02, 6.33208081e-02, 3.17766964e-02, 4.44987742e-03,\n",
       "         2.58543566e-02, 4.72580604e-02, 6.47582188e-02, 5.68810999e-02,\n",
       "         5.69503605e-02, 1.19539008e-01, 1.88903287e-01, 2.91346684e-02,\n",
       "         3.01135164e-02, 2.00445186e-02, 8.86204541e-02, 5.37786521e-02,\n",
       "         9.43303108e-03, 1.37493666e-03, 1.24140168e-02, 3.06975655e-02,\n",
       "         1.90532189e-02, 1.11377016e-01, 5.09261340e-02, 7.69891441e-02,\n",
       "         4.93889898e-02, 7.80133680e-02, 5.22152483e-02, 7.09729865e-02,\n",
       "         1.78976986e-03, 6.53413311e-03, 1.47995800e-02, 1.61507316e-02,\n",
       "         5.21790721e-02, 3.60888094e-02, 2.08702572e-02, 7.04847053e-02,\n",
       "         2.74261162e-02, 4.15808521e-02, 2.08215620e-02, 4.02767817e-03,\n",
       "         8.96587595e-03, 1.43659143e-02, 7.15474859e-02, 1.30665842e-02,\n",
       "         2.43041720e-02, 1.44015864e-01, 2.18289644e-01, 4.64956090e-02,\n",
       "         6.74684644e-02, 4.22596112e-02, 3.63766104e-02, 1.26453042e-02,\n",
       "         2.29406841e-02, 7.22763985e-02, 1.05713001e-02, 1.71225220e-02,\n",
       "         1.11150136e-03, 6.24052398e-02, 1.49600683e-02, 2.72281859e-02,\n",
       "         5.87800592e-02, 3.65448594e-02, 8.68041441e-02, 9.41150114e-02,\n",
       "         7.64581710e-02, 4.97666523e-02, 9.81441699e-03, 5.06418012e-03,\n",
       "         1.03455521e-01, 5.86436428e-02, 1.04388103e-01, 1.55619040e-01,\n",
       "         4.62221503e-02, 8.28619897e-02, 4.47744951e-02, 1.31038070e-01,\n",
       "         7.54736662e-02, 2.59991102e-02, 3.43314111e-02, 4.86613624e-02,\n",
       "         9.43622962e-02, 9.05788392e-02, 1.69277694e-02, 8.41411278e-02,\n",
       "         1.84309052e-03, 8.53670463e-02, 2.57799518e-04, 3.36814253e-03,\n",
       "         5.06687835e-02, 1.30806053e-02, 2.56343018e-02, 7.34323859e-02,\n",
       "         1.15791913e-02, 1.17772736e-01, 5.51003031e-02, 1.25370756e-01,\n",
       "         2.34349389e-02, 4.44259727e-03, 7.17310458e-02, 6.67719841e-02,\n",
       "         3.28633636e-02, 4.28293366e-03, 3.36750410e-02, 1.22282077e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 5,\n",
       "  'num_filters': 256,\n",
       "  'scores': array([6.33656234e-02, 4.68583591e-02, 5.51292747e-02, 1.58975706e-01,\n",
       "         5.86332083e-02, 1.12421168e-02, 1.76262587e-01, 8.07779282e-02,\n",
       "         1.28570059e-02, 3.45419571e-02, 1.50814780e-03, 1.14729144e-01,\n",
       "         1.18351879e-03, 5.46201244e-02, 7.47559965e-02, 1.53997187e-02,\n",
       "         2.27391571e-01, 7.69854710e-02, 7.75306225e-02, 5.41151548e-03,\n",
       "         6.67325482e-02, 4.84227166e-02, 5.74860685e-02, 2.87304390e-02,\n",
       "         6.49075583e-02, 7.05120899e-03, 9.39292163e-02, 4.75769229e-02,\n",
       "         4.35034074e-02, 1.25068072e-02, 3.47393788e-02, 1.23589085e-02,\n",
       "         2.62659937e-02, 6.18440956e-02, 5.46045564e-02, 7.21038952e-02,\n",
       "         1.04717061e-01, 1.03375763e-02, 2.79437415e-02, 2.91019212e-02,\n",
       "         6.28221557e-02, 5.50111756e-02, 5.98755432e-03, 5.81223704e-02,\n",
       "         7.14449026e-03, 2.21257489e-02, 1.90022904e-02, 4.01971787e-02,\n",
       "         1.22329425e-02, 4.20665443e-02, 4.10734974e-02, 1.23847378e-02,\n",
       "         5.32366298e-02, 4.51941825e-02, 7.04523697e-02, 1.13798723e-01,\n",
       "         7.15948567e-02, 3.76491919e-02, 4.65729311e-02, 1.49710849e-01,\n",
       "         1.07697830e-01, 6.84941486e-02, 1.32542387e-01, 2.86385529e-02,\n",
       "         3.50708552e-02, 6.45753071e-02, 2.08894871e-02, 1.41967505e-01,\n",
       "         1.08743012e-02, 1.37811452e-02, 3.46550206e-03, 3.21058184e-02,\n",
       "         7.95513615e-02, 3.30843478e-02, 7.60086719e-03, 5.88134341e-02,\n",
       "         2.80012731e-02, 1.19290590e-01, 7.21450672e-02, 3.13112251e-02,\n",
       "         5.67225963e-02, 4.60325778e-02, 4.85821404e-02, 2.41476507e-03,\n",
       "         2.04570740e-02, 2.00279430e-02, 3.01953568e-03, 1.59607064e-02,\n",
       "         1.51734918e-01, 9.19904336e-02, 1.07683785e-01, 4.63152900e-02,\n",
       "         1.16020367e-02, 3.93462740e-02, 8.64438936e-02, 6.36541396e-02,\n",
       "         6.60374984e-02, 1.01086469e-02, 3.54001634e-02, 2.38910988e-02,\n",
       "         4.85709636e-03, 3.12824845e-02, 3.84501554e-03, 8.11902955e-02,\n",
       "         2.94264629e-02, 1.54687375e-01, 5.95022589e-02, 4.56079952e-02,\n",
       "         7.82095268e-03, 7.87407625e-03, 7.70639777e-02, 5.20996563e-02,\n",
       "         2.23888885e-02, 2.30883285e-02, 9.20207053e-02, 5.45178987e-02,\n",
       "         2.53654122e-02, 1.41584380e-02, 9.48585346e-02, 5.18390834e-02,\n",
       "         1.38930574e-01, 3.27645522e-03, 5.11188284e-02, 2.52860058e-02,\n",
       "         1.93272959e-02, 3.24121746e-03, 4.88724969e-02, 2.02374995e-01,\n",
       "         1.54270383e-03, 4.99654375e-02, 2.34466337e-04, 2.08345857e-02,\n",
       "         6.30563051e-02, 4.05769311e-02, 4.49067466e-02, 9.11737978e-02,\n",
       "         4.43132827e-03, 1.15637621e-02, 7.69365504e-02, 2.63259113e-02,\n",
       "         6.20360821e-02, 2.96176430e-02, 5.40123135e-02, 2.37751946e-01,\n",
       "         8.46162736e-02, 7.05515547e-03, 3.67971845e-02, 8.68563056e-02,\n",
       "         1.83998309e-02, 1.16490256e-02, 9.00447443e-02, 7.54641816e-02,\n",
       "         3.98619734e-02, 2.93587260e-02, 2.92841308e-02, 9.61707067e-03,\n",
       "         2.58978289e-02, 4.06762920e-02, 1.66449174e-02, 2.65686810e-02,\n",
       "         8.48598927e-02, 5.13676777e-02, 6.12886362e-02, 6.03837371e-02,\n",
       "         2.45330483e-02, 6.40063733e-03, 6.71566203e-02, 4.66255620e-02,\n",
       "         1.04325488e-01, 1.35441869e-03, 4.56154235e-02, 5.16283140e-02,\n",
       "         2.88947951e-02, 3.15931700e-02, 1.45120290e-03, 1.48263490e-02,\n",
       "         6.21182621e-02, 5.21609187e-02, 5.59768304e-02, 8.25684071e-02,\n",
       "         1.26014113e-01, 3.53767462e-02, 7.74009153e-03, 5.70493527e-02,\n",
       "         2.22237371e-02, 2.83934129e-03, 6.28888309e-02, 5.53896325e-03,\n",
       "         2.59226169e-02, 1.26041099e-01, 3.77719291e-02, 2.76097152e-02,\n",
       "         2.07317322e-02, 3.71330902e-02, 1.46638993e-02, 5.95823824e-02,\n",
       "         3.97337154e-02, 8.73795003e-02, 4.20971401e-02, 4.48021060e-03,\n",
       "         1.98384225e-02, 6.86485693e-02, 2.08375473e-02, 6.34253472e-02,\n",
       "         3.80189307e-02, 7.61573538e-02, 7.92807434e-03, 6.85241073e-02,\n",
       "         5.96238524e-02, 3.61166075e-02, 1.09102599e-01, 1.25991255e-02,\n",
       "         4.74120826e-02, 1.86103862e-02, 8.94252881e-02, 1.84485316e-02,\n",
       "         9.21848267e-02, 1.41066194e-01, 2.04336885e-02, 4.76269498e-02,\n",
       "         2.28550117e-02, 3.48540917e-02, 1.79860238e-02, 1.38033833e-02,\n",
       "         1.35398842e-02, 2.07128171e-02, 1.83683839e-02, 8.59818682e-02,\n",
       "         4.11136560e-02, 2.50996687e-02, 4.63117659e-02, 3.77380871e-03,\n",
       "         2.93540861e-02, 1.13512218e-01, 2.80936379e-02, 8.28626454e-02,\n",
       "         5.40867373e-02, 5.04437760e-02, 2.00119782e-02, 4.08549979e-02,\n",
       "         1.76943745e-02, 7.45435655e-02, 2.36102119e-02, 4.17566821e-02,\n",
       "         4.57867468e-03, 3.47093418e-02, 4.51920852e-02, 7.68809989e-02,\n",
       "         2.29443256e-02, 6.85159210e-03, 5.85519746e-02, 4.28928882e-02,\n",
       "         4.92031872e-02, 5.64526655e-02, 9.31564067e-03, 8.07587430e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 6,\n",
       "  'num_filters': 256,\n",
       "  'scores': array([0.08755918, 0.04206894, 0.09994372, 0.04909987, 0.06371129,\n",
       "         0.0376517 , 0.05115645, 0.00462552, 0.00654022, 0.07201538,\n",
       "         0.00886522, 0.05669586, 0.02510976, 0.03698466, 0.07400779,\n",
       "         0.03875916, 0.03188992, 0.06426153, 0.0259735 , 0.01580383,\n",
       "         0.06526733, 0.12462506, 0.05846632, 0.11790965, 0.07393409,\n",
       "         0.0043671 , 0.04246971, 0.00389736, 0.02633285, 0.00873394,\n",
       "         0.14492472, 0.03381735, 0.0300925 , 0.01297892, 0.12986496,\n",
       "         0.00866703, 0.02539778, 0.09104423, 0.01394333, 0.00238235,\n",
       "         0.09491935, 0.03997644, 0.00868955, 0.0364204 , 0.23609735,\n",
       "         0.04032416, 0.05129237, 0.0385725 , 0.11039868, 0.13063888,\n",
       "         0.00972484, 0.07093543, 0.18181191, 0.10777686, 0.01805643,\n",
       "         0.03607782, 0.02002707, 0.03607592, 0.10126065, 0.0794394 ,\n",
       "         0.00732414, 0.04833849, 0.00476281, 0.00472347, 0.05369507,\n",
       "         0.0923516 , 0.01182524, 0.03984512, 0.11242294, 0.07288362,\n",
       "         0.03203051, 0.03134862, 0.00579961, 0.01766791, 0.02035673,\n",
       "         0.14865912, 0.10102826, 0.02750461, 0.00275537, 0.12681133,\n",
       "         0.00687422, 0.17264839, 0.00711424, 0.05687489, 0.02922962,\n",
       "         0.00653287, 0.01513531, 0.0052466 , 0.05084763, 0.02601263,\n",
       "         0.08871432, 0.03509988, 0.0103774 , 0.01958384, 0.03805125,\n",
       "         0.00056589, 0.00251859, 0.07482725, 0.026966  , 0.07876769,\n",
       "         0.01231668, 0.01703908, 0.12622143, 0.01900914, 0.03976517,\n",
       "         0.00788271, 0.00246476, 0.00543767, 0.05665741, 0.00101116,\n",
       "         0.05808184, 0.04564879, 0.05963466, 0.0489183 , 0.03621602,\n",
       "         0.05345722, 0.03811068, 0.06988151, 0.12150318, 0.02449429,\n",
       "         0.13333648, 0.00059348, 0.02846305, 0.1725334 , 0.06690878,\n",
       "         0.00990656, 0.00877938, 0.02453308, 0.03660544, 0.03400804,\n",
       "         0.0127424 , 0.038166  , 0.09676418, 0.03567516, 0.13527797,\n",
       "         0.15975459, 0.01062088, 0.0207518 , 0.05794512, 0.03630657,\n",
       "         0.01304799, 0.07222828, 0.09974695, 0.18389659, 0.06020728,\n",
       "         0.09589713, 0.01918542, 0.02746173, 0.04254318, 0.07137414,\n",
       "         0.08609446, 0.00369373, 0.00713124, 0.01203999, 0.01031633,\n",
       "         0.0358955 , 0.08154939, 0.00205841, 0.10029156, 0.0115974 ,\n",
       "         0.03387974, 0.02680329, 0.05628914, 0.0629715 , 0.03421014,\n",
       "         0.07145059, 0.01457362, 0.07162813, 0.10791884, 0.00210087,\n",
       "         0.00440037, 0.04926119, 0.14154296, 0.04796561, 0.08178447,\n",
       "         0.02635513, 0.00479794, 0.0131068 , 0.05944263, 0.01542159,\n",
       "         0.0043785 , 0.00918353, 0.01624715, 0.04215463, 0.0892823 ,\n",
       "         0.01818787, 0.0461031 , 0.03136801, 0.0428795 , 0.08539747,\n",
       "         0.0085149 , 0.02772483, 0.04674849, 0.04391321, 0.01186092,\n",
       "         0.00727068, 0.0194302 , 0.0186939 , 0.03765945, 0.02742976,\n",
       "         0.02971273, 0.05629597, 0.05092939, 0.04939897, 0.07390835,\n",
       "         0.00792539, 0.01113807, 0.0080564 , 0.02533178, 0.0321543 ,\n",
       "         0.0382971 , 0.16006368, 0.01914017, 0.05652369, 0.04322119,\n",
       "         0.07236712, 0.00993356, 0.00853422, 0.05728583, 0.04545649,\n",
       "         0.07906254, 0.04974579, 0.06233085, 0.02730207, 0.03685698,\n",
       "         0.01289823, 0.02769533, 0.0097866 , 0.01333381, 0.08455779,\n",
       "         0.01039147, 0.05107455, 0.08442803, 0.09370244, 0.01316017,\n",
       "         0.01106412, 0.05125802, 0.08577538, 0.07964839, 0.02071653,\n",
       "         0.02895484, 0.02263341, 0.10033288, 0.05642144, 0.00865185,\n",
       "         0.03367139, 0.00341935, 0.05848289, 0.02839768, 0.02815948,\n",
       "         0.05428019, 0.04096842, 0.04458372, 0.05365748, 0.00452763,\n",
       "         0.11322047], dtype=float32)},\n",
       " {'layer_idx': 7,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([1.18783787e-02, 4.16266806e-02, 2.99292430e-02, 4.78744619e-02,\n",
       "         9.24495980e-03, 2.48839278e-02, 3.79279405e-02, 2.72251386e-02,\n",
       "         3.91477868e-02, 3.59113701e-02, 1.97248012e-02, 9.20299217e-02,\n",
       "         6.78897413e-05, 9.21845436e-03, 3.38173211e-02, 2.59629097e-02,\n",
       "         9.44927037e-02, 2.22604685e-02, 3.73701453e-02, 5.32729775e-02,\n",
       "         5.65161109e-02, 5.08730020e-03, 8.05973783e-02, 2.34602913e-02,\n",
       "         1.51110627e-02, 3.24884243e-02, 1.38869146e-02, 5.60784005e-02,\n",
       "         2.20040698e-03, 6.63186535e-02, 4.24293913e-02, 7.92513564e-02,\n",
       "         6.01990893e-02, 6.40348196e-02, 4.38796915e-02, 1.09369949e-01,\n",
       "         2.89056487e-02, 7.54567515e-03, 3.38096619e-02, 1.27853202e-02,\n",
       "         3.86415422e-02, 2.42544394e-02, 3.16025540e-02, 1.67785175e-02,\n",
       "         4.02034558e-02, 2.00174078e-02, 6.00096285e-02, 1.21076424e-02,\n",
       "         1.23940478e-03, 2.19965503e-02, 4.08340059e-02, 3.21487002e-02,\n",
       "         4.12710346e-02, 7.91517645e-02, 2.42867302e-02, 3.56365135e-03,\n",
       "         2.77263913e-02, 1.18780229e-02, 5.77442981e-02, 1.93288457e-02,\n",
       "         6.23158924e-03, 4.12077121e-02, 4.90798168e-02, 1.66566283e-01,\n",
       "         4.60592620e-02, 2.07177969e-03, 1.07578179e-02, 2.38093827e-02,\n",
       "         2.98812911e-02, 5.87735251e-02, 2.41115876e-02, 4.95241694e-02,\n",
       "         2.50494666e-02, 3.42956446e-02, 5.25364764e-02, 1.10612800e-02,\n",
       "         1.61437690e-02, 5.31369783e-02, 4.38592583e-02, 1.87835991e-02,\n",
       "         1.39334714e-02, 3.37032117e-02, 1.69564486e-02, 2.43331548e-02,\n",
       "         7.36631230e-02, 3.25733386e-02, 3.55399959e-02, 6.12271801e-02,\n",
       "         7.05937855e-04, 2.75550783e-03, 1.93857122e-02, 8.06892216e-02,\n",
       "         6.50994061e-03, 7.52307996e-02, 3.46603543e-02, 8.29526111e-02,\n",
       "         3.57907712e-02, 6.29979223e-02, 1.09407986e-02, 6.91308305e-02,\n",
       "         4.07391898e-02, 3.87152145e-03, 7.83003308e-03, 7.35389115e-03,\n",
       "         5.43614998e-02, 1.21647129e-02, 2.68461499e-02, 4.55875928e-03,\n",
       "         7.11291097e-03, 3.02393176e-02, 2.34027393e-02, 5.39816543e-03,\n",
       "         1.08568510e-02, 2.19314732e-02, 3.41074690e-02, 1.90444489e-03,\n",
       "         7.36146839e-03, 4.66287648e-03, 1.11149110e-01, 6.45595640e-02,\n",
       "         1.71719622e-02, 3.72183137e-02, 2.43099034e-02, 6.19808137e-02,\n",
       "         5.66274337e-02, 2.89620385e-02, 5.93690854e-03, 1.13946758e-01,\n",
       "         5.17499680e-03, 2.23776940e-02, 1.50968909e-01, 1.86923333e-02,\n",
       "         1.24905072e-01, 1.10814404e-02, 8.72093253e-03, 7.17912242e-02,\n",
       "         3.16652469e-02, 1.10584078e-02, 3.32518071e-02, 3.19097228e-02,\n",
       "         2.61808112e-02, 2.45519206e-02, 2.23975871e-02, 1.07234204e-02,\n",
       "         1.88857559e-02, 2.89334357e-02, 7.67116621e-03, 4.90753315e-02,\n",
       "         1.00618741e-03, 6.22703321e-02, 1.14035187e-02, 2.38062572e-02,\n",
       "         3.00897341e-02, 1.80281829e-02, 4.44366261e-02, 2.74196058e-03,\n",
       "         2.14610044e-02, 2.60723550e-02, 1.54353026e-02, 4.94419336e-02,\n",
       "         8.47432390e-02, 1.34169543e-02, 1.33736571e-02, 1.97319146e-02,\n",
       "         1.03099391e-01, 8.25945362e-02, 4.33088932e-03, 5.82303200e-03,\n",
       "         4.59211655e-02, 6.58770725e-02, 3.58481593e-02, 1.59250498e-02,\n",
       "         3.37685039e-03, 3.90347317e-02, 6.63223816e-03, 5.90142533e-02,\n",
       "         6.93975836e-02, 3.19028436e-03, 4.09520827e-02, 3.11181415e-03,\n",
       "         3.85199934e-02, 7.52907060e-03, 6.66927174e-02, 6.84260903e-03,\n",
       "         8.92681330e-02, 1.45146428e-02, 3.69645469e-03, 3.02057080e-02,\n",
       "         2.33371928e-02, 5.79814892e-03, 3.77200916e-02, 4.83350568e-02,\n",
       "         1.15037942e-02, 2.92228255e-03, 7.07343826e-03, 2.22217292e-02,\n",
       "         1.15190698e-02, 6.32286444e-02, 6.58597378e-03, 4.23725508e-02,\n",
       "         4.68085483e-02, 9.05716978e-03, 1.22638810e-02, 3.48561816e-02,\n",
       "         5.54893501e-02, 4.67425771e-02, 3.11020277e-02, 1.16408793e-02,\n",
       "         1.02947569e-02, 2.02369466e-02, 2.09206790e-02, 3.30502167e-02,\n",
       "         5.86862303e-02, 3.16804945e-02, 3.05970889e-02, 3.49563397e-02,\n",
       "         8.46423358e-02, 5.51195294e-02, 4.41586077e-02, 8.41208082e-03,\n",
       "         7.72584043e-03, 2.65781232e-03, 7.15856403e-02, 8.17861408e-03,\n",
       "         3.55108492e-02, 3.25412303e-02, 3.80809940e-02, 5.04116863e-02,\n",
       "         4.50484976e-02, 7.48435268e-03, 4.66884226e-02, 3.21480259e-02,\n",
       "         2.95893159e-02, 6.20725891e-03, 4.87474352e-02, 7.24222213e-02,\n",
       "         5.26551455e-02, 1.11973118e-02, 3.27829979e-02, 7.17334636e-03,\n",
       "         2.29011774e-02, 4.90758680e-02, 1.61542427e-02, 1.03178527e-02,\n",
       "         4.40893956e-02, 4.27719019e-02, 2.28662486e-03, 7.69670382e-02,\n",
       "         3.26959640e-02, 4.58165482e-02, 4.87703225e-03, 2.33841687e-02,\n",
       "         2.26507261e-02, 7.98946805e-03, 1.24119269e-02, 3.57348435e-02,\n",
       "         6.17607310e-03, 4.89126332e-02, 6.36201426e-02, 9.95466858e-02,\n",
       "         1.89910568e-02, 4.89644408e-02, 2.73344833e-02, 3.00247390e-02,\n",
       "         1.21626034e-02, 1.16714099e-02, 1.00582866e-02, 3.92921455e-02,\n",
       "         4.60771173e-02, 1.33295497e-02, 6.22003302e-02, 2.21907236e-02,\n",
       "         4.96816523e-02, 1.44378562e-02, 7.14279413e-02, 7.77720958e-02,\n",
       "         6.68982714e-02, 2.38160882e-02, 1.65954418e-02, 4.45056036e-02,\n",
       "         5.65872602e-02, 8.18369016e-02, 3.46207470e-02, 2.66937464e-02,\n",
       "         2.29789335e-02, 3.67498584e-02, 2.63982415e-02, 5.52247129e-02,\n",
       "         7.90449631e-05, 2.24728100e-02, 3.78988273e-02, 4.96755168e-03,\n",
       "         9.92271081e-02, 9.88207944e-03, 8.02885517e-02, 1.03185354e-02,\n",
       "         2.83923242e-02, 5.75292781e-02, 1.90292764e-02, 4.60772477e-02,\n",
       "         1.54438233e-02, 1.29083358e-03, 5.41302282e-03, 4.73903790e-02,\n",
       "         6.29873872e-02, 7.82965624e-04, 1.87658034e-02, 3.47129591e-02,\n",
       "         2.03391146e-02, 2.04535779e-02, 3.05124884e-03, 4.16785739e-02,\n",
       "         4.65071313e-02, 4.60901186e-02, 4.27341424e-02, 1.47793515e-04,\n",
       "         3.42582888e-03, 1.17228394e-02, 2.70488076e-02, 3.54577862e-02,\n",
       "         7.49258175e-02, 6.24072328e-02, 1.12146568e-02, 5.64701483e-02,\n",
       "         4.92516439e-03, 6.09427644e-03, 1.76245458e-02, 5.96924387e-02,\n",
       "         1.48644177e-02, 5.44140823e-02, 2.33316631e-03, 3.50310989e-02,\n",
       "         6.47218525e-02, 1.60348341e-02, 4.08194810e-02, 1.41248759e-03,\n",
       "         6.81157783e-02, 3.26916091e-02, 8.48745480e-02, 1.27419336e-02,\n",
       "         1.08373147e-02, 8.33989233e-02, 1.61010865e-02, 4.70137335e-02,\n",
       "         1.44973528e-02, 7.07264617e-02, 3.84258404e-02, 1.15336835e-01,\n",
       "         1.25807868e-02, 3.34209874e-02, 2.71225553e-02, 4.11980264e-02,\n",
       "         8.33187811e-03, 2.51990836e-02, 8.89322534e-03, 6.19046316e-02,\n",
       "         5.62153831e-02, 1.09045710e-02, 3.49535830e-02, 6.58174930e-03,\n",
       "         4.03565466e-02, 5.98826818e-03, 4.71617235e-03, 3.79601158e-02,\n",
       "         6.47666901e-02, 5.69584183e-02, 1.06019162e-01, 8.95130262e-02,\n",
       "         1.12746200e-02, 5.05164601e-02, 9.24303979e-02, 5.28905680e-03,\n",
       "         5.78057840e-02, 2.29943562e-02, 5.14592640e-02, 1.67318694e-02,\n",
       "         2.97287907e-02, 4.96069044e-02, 1.81543380e-02, 2.33601872e-02,\n",
       "         1.56165920e-02, 6.56039082e-03, 1.41553124e-02, 2.18927059e-02,\n",
       "         1.27372593e-02, 5.31531870e-02, 7.91139230e-02, 2.71120598e-03,\n",
       "         1.31173134e-02, 4.02579829e-02, 8.05474166e-03, 5.81150092e-02,\n",
       "         3.53432707e-02, 1.04991041e-01, 8.30137357e-02, 2.89705247e-02,\n",
       "         3.56523297e-03, 3.06280069e-02, 2.72455160e-02, 2.24143788e-02,\n",
       "         2.95715854e-02, 6.41586781e-02, 6.17271056e-03, 3.28406543e-02,\n",
       "         1.70990869e-01, 4.70448695e-02, 2.87627261e-02, 1.39309883e-01,\n",
       "         9.80888400e-03, 5.80184674e-03, 1.13639776e-02, 7.46927783e-02,\n",
       "         1.29811028e-02, 6.65121898e-03, 8.12741816e-02, 9.19702351e-02,\n",
       "         2.04892643e-02, 3.41269001e-02, 5.53501248e-02, 1.11761959e-02,\n",
       "         5.71403839e-02, 3.41495387e-02, 3.43625955e-02, 3.06410082e-02,\n",
       "         5.39802536e-02, 1.14808362e-02, 4.74230722e-02, 3.84020731e-02,\n",
       "         3.72846313e-02, 1.42534021e-02, 1.98916011e-02, 5.58217652e-02,\n",
       "         6.76434021e-03, 4.25503738e-02, 4.67592776e-02, 1.48324370e-01,\n",
       "         3.62800471e-02, 6.37825057e-02, 3.37809362e-02, 1.20041676e-01,\n",
       "         2.51174029e-02, 3.42890085e-03, 5.13156205e-02, 3.99916656e-02,\n",
       "         1.57299060e-02, 3.83717380e-02, 2.29727533e-02, 4.70124278e-03,\n",
       "         9.29525495e-03, 1.99980754e-02, 5.54265082e-02, 5.14169820e-02,\n",
       "         7.37381261e-03, 5.18144965e-02, 2.40378734e-02, 6.67176917e-02,\n",
       "         9.04313773e-02, 1.81708094e-02, 5.23295552e-02, 3.12301349e-02,\n",
       "         1.66167300e-02, 1.40717402e-02, 5.83220534e-02, 3.66137084e-03,\n",
       "         2.30212137e-02, 1.91899911e-02, 6.12069853e-02, 3.00575811e-02,\n",
       "         8.40775948e-03, 3.74153070e-02, 2.64084956e-04, 8.74882657e-03,\n",
       "         3.51355746e-02, 5.59018217e-02, 1.22863858e-03, 1.99810751e-02,\n",
       "         5.25360964e-02, 3.34941363e-03, 3.09459609e-03, 2.02112850e-02,\n",
       "         2.30407622e-02, 6.31303638e-02, 2.51673851e-02, 2.02085041e-02,\n",
       "         5.76486290e-02, 3.39105316e-02, 5.30735888e-02, 1.92996897e-02,\n",
       "         7.49952905e-03, 6.85136020e-03, 3.85923940e-03, 7.34628960e-02,\n",
       "         1.73374899e-02, 1.63066965e-02, 2.67318040e-02, 1.43124266e-02,\n",
       "         2.23419536e-02, 2.90612746e-02, 2.76927091e-02, 4.84951260e-03,\n",
       "         2.95944382e-02, 6.37777299e-02, 5.29312529e-04, 3.12187877e-02,\n",
       "         8.94727651e-03, 3.57784703e-02, 1.02745751e-02, 5.03653139e-02,\n",
       "         6.51701839e-06, 4.46201116e-02, 4.70754169e-02, 7.27237090e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 8,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([1.36480322e-02, 2.66706385e-02, 3.70865092e-02, 1.83489732e-03,\n",
       "         1.61676668e-02, 8.10757130e-02, 9.03233960e-02, 5.78427576e-02,\n",
       "         6.57878071e-03, 6.65270612e-02, 5.22614047e-02, 2.28726566e-02,\n",
       "         1.22335173e-01, 1.00106432e-03, 3.50624770e-02, 3.49143595e-02,\n",
       "         7.10677728e-02, 5.45988828e-02, 6.98925778e-02, 1.07912032e-03,\n",
       "         1.21374810e-02, 5.21112792e-02, 3.96970101e-03, 5.39893620e-02,\n",
       "         4.58676293e-02, 2.20400114e-02, 1.03617162e-02, 4.14927974e-02,\n",
       "         1.18567161e-01, 1.79597922e-02, 5.94578125e-02, 2.89441273e-02,\n",
       "         2.32178196e-02, 3.84259373e-02, 5.07114306e-02, 6.09913981e-03,\n",
       "         2.76334174e-02, 7.11683035e-02, 9.61697195e-03, 2.58929990e-02,\n",
       "         2.67945025e-02, 8.75999406e-03, 1.15397237e-01, 3.42869870e-02,\n",
       "         8.13698545e-02, 6.63540065e-02, 3.28997262e-02, 1.08546959e-02,\n",
       "         4.10155728e-02, 7.94731975e-02, 8.93206988e-03, 3.25430520e-02,\n",
       "         9.11297798e-02, 1.06956340e-01, 7.97672793e-02, 5.49624786e-02,\n",
       "         7.50957290e-03, 7.16844015e-03, 8.31575133e-03, 6.84271306e-02,\n",
       "         4.63898294e-02, 8.95821583e-03, 2.13299636e-02, 3.72408740e-02,\n",
       "         6.87801931e-03, 6.45398423e-02, 3.37717123e-02, 5.53947277e-02,\n",
       "         9.49693471e-03, 4.73567378e-03, 9.86792706e-03, 3.01006958e-02,\n",
       "         8.20133612e-02, 1.39148040e-02, 1.62494667e-02, 3.61311138e-02,\n",
       "         2.20954884e-02, 7.91546851e-02, 2.72234511e-02, 7.00996071e-02,\n",
       "         8.06891359e-03, 6.43110424e-02, 1.37716727e-02, 1.87648963e-02,\n",
       "         4.66074087e-02, 2.66177803e-02, 1.05627310e-02, 5.32738976e-02,\n",
       "         2.79122032e-02, 3.80257554e-02, 6.71913773e-02, 1.10873505e-02,\n",
       "         4.33810018e-02, 1.60525981e-02, 2.46445592e-02, 2.89535383e-03,\n",
       "         2.95465384e-02, 7.14272261e-03, 1.08904038e-02, 3.66746597e-02,\n",
       "         3.71588841e-02, 1.25849620e-02, 1.12767899e-02, 4.43803631e-02,\n",
       "         3.94979818e-03, 1.83178019e-02, 2.65522227e-02, 2.79648080e-02,\n",
       "         7.60263875e-02, 2.31618360e-02, 1.66158557e-01, 1.17776087e-02,\n",
       "         4.83334251e-02, 2.51166523e-02, 1.80952474e-02, 1.90881453e-02,\n",
       "         5.09010255e-02, 2.19620578e-02, 6.17676862e-02, 2.42927745e-02,\n",
       "         2.38444749e-02, 1.03944086e-01, 2.19531823e-02, 1.95411686e-02,\n",
       "         6.59892112e-02, 3.12472251e-03, 9.92681738e-03, 4.01176512e-02,\n",
       "         5.57875484e-02, 5.54678217e-02, 2.75314972e-03, 4.22202609e-02,\n",
       "         1.90346967e-02, 1.80432741e-02, 1.24159863e-03, 1.96536221e-02,\n",
       "         2.51620039e-02, 4.41417620e-02, 4.64833900e-02, 9.51560885e-02,\n",
       "         6.84413835e-02, 2.79574972e-02, 3.91464308e-02, 1.18350200e-02,\n",
       "         1.79328918e-02, 3.51375416e-02, 3.77290696e-02, 1.47568276e-02,\n",
       "         2.77211536e-02, 1.83914229e-02, 8.54218611e-04, 3.52858491e-02,\n",
       "         1.82061736e-02, 2.68860403e-02, 1.48759438e-02, 1.66858733e-02,\n",
       "         2.75248531e-02, 1.82673673e-03, 3.80582549e-02, 3.71858552e-02,\n",
       "         4.10464704e-02, 1.72181949e-02, 8.52097664e-03, 2.83843540e-02,\n",
       "         8.96637980e-03, 8.24883766e-03, 4.90201812e-04, 7.13771733e-05,\n",
       "         4.48880941e-02, 4.07045297e-02, 7.06928840e-04, 3.77637818e-02,\n",
       "         3.72081734e-02, 3.48177627e-02, 1.47897284e-02, 1.83628965e-02,\n",
       "         2.81898701e-03, 3.71579826e-02, 1.41721843e-02, 1.72970612e-02,\n",
       "         1.21715488e-02, 1.27467476e-02, 9.99075640e-03, 2.35717390e-02,\n",
       "         2.76915170e-02, 2.38798056e-02, 2.89132651e-02, 2.13105977e-02,\n",
       "         1.10760117e-02, 1.83888490e-03, 2.70754211e-02, 5.65810967e-03,\n",
       "         1.40791303e-02, 2.58365478e-02, 9.94216930e-03, 7.12412782e-03,\n",
       "         4.63003479e-02, 1.67788323e-02, 8.73762816e-02, 4.50657979e-02,\n",
       "         3.87556851e-04, 3.54558080e-02, 1.71620883e-02, 1.30879199e-02,\n",
       "         6.04271144e-02, 3.63992550e-03, 6.27414957e-02, 7.93130398e-02,\n",
       "         1.91304497e-02, 2.30184067e-02, 3.02625485e-02, 9.82405804e-03,\n",
       "         1.90296471e-02, 3.21062803e-02, 2.46490110e-02, 9.56036709e-03,\n",
       "         7.12655438e-03, 4.97979186e-02, 2.76847985e-02, 1.42030623e-02,\n",
       "         2.95908283e-02, 1.72419641e-02, 7.17953593e-02, 5.64007647e-02,\n",
       "         2.20921189e-02, 4.45775501e-02, 2.31311582e-02, 2.02331524e-02,\n",
       "         4.79044132e-02, 1.17969513e-01, 2.68641440e-03, 1.30067002e-02,\n",
       "         3.12067047e-02, 6.49787635e-02, 1.62037760e-02, 2.78121307e-02,\n",
       "         1.18116863e-01, 6.04756223e-03, 5.45061380e-02, 3.66702564e-02,\n",
       "         1.45359110e-04, 5.38010336e-03, 4.20897678e-02, 1.62047818e-02,\n",
       "         9.58995149e-02, 6.04608692e-02, 1.75572820e-02, 2.07044664e-04,\n",
       "         9.97134252e-04, 1.42341219e-02, 4.40863036e-02, 6.39657155e-02,\n",
       "         3.04886848e-02, 4.46143858e-02, 3.22812572e-02, 1.59290619e-04,\n",
       "         1.86688136e-02, 1.53612755e-02, 2.27672961e-02, 3.83664928e-02,\n",
       "         4.09859940e-02, 4.87852618e-02, 4.89673391e-02, 3.71551961e-02,\n",
       "         3.03873792e-02, 4.06558216e-02, 4.06204816e-03, 4.78182212e-02,\n",
       "         7.52645433e-02, 3.94310504e-02, 2.05909624e-03, 2.22641900e-02,\n",
       "         2.94337352e-03, 9.18082800e-03, 7.16731101e-02, 1.80306938e-02,\n",
       "         1.70188174e-02, 6.85310662e-02, 1.84876379e-02, 6.52342569e-03,\n",
       "         2.30867118e-02, 4.14824821e-02, 9.25523136e-03, 1.57415755e-02,\n",
       "         2.39970032e-02, 1.63842943e-02, 2.11201943e-02, 8.29131603e-02,\n",
       "         2.82684285e-02, 2.41179653e-02, 6.05134033e-02, 8.46637879e-03,\n",
       "         1.01538794e-02, 1.38992500e-02, 3.06576770e-02, 5.67255281e-02,\n",
       "         4.39360254e-02, 4.21182923e-02, 9.92363133e-03, 4.22774032e-02,\n",
       "         2.80703623e-02, 2.03510933e-02, 1.79280306e-03, 7.84074515e-03,\n",
       "         2.69034989e-02, 8.34750012e-02, 4.40370804e-03, 3.91622186e-02,\n",
       "         8.35590139e-02, 3.01893819e-02, 5.32841161e-02, 1.18559850e-02,\n",
       "         3.71139236e-02, 2.42126938e-02, 6.02423437e-02, 7.84139428e-03,\n",
       "         9.03156679e-03, 4.06910852e-02, 7.96110090e-03, 1.19769229e-02,\n",
       "         4.89337184e-02, 9.11809355e-02, 5.51100112e-02, 4.78354916e-02,\n",
       "         8.00740719e-03, 1.50368747e-03, 1.79001689e-02, 4.36002314e-02,\n",
       "         3.68274655e-03, 2.39995196e-02, 8.07957202e-02, 5.61280139e-02,\n",
       "         2.91496180e-02, 4.23449799e-02, 2.52705421e-02, 9.74900555e-03,\n",
       "         2.73679644e-02, 5.08874916e-02, 2.47056112e-02, 1.43527985e-02,\n",
       "         8.07819292e-02, 4.73559089e-02, 1.01893349e-02, 6.49051592e-02,\n",
       "         3.70238200e-02, 3.14698718e-03, 3.40376459e-02, 3.26683857e-02,\n",
       "         5.60532100e-02, 1.73387991e-03, 3.15775089e-02, 2.82125659e-02,\n",
       "         5.62182032e-02, 1.12803774e-02, 1.39356097e-02, 1.04392637e-02,\n",
       "         7.58493319e-02, 5.96001334e-02, 4.16082405e-02, 1.43397581e-02,\n",
       "         8.84382334e-03, 2.94341277e-02, 6.66237772e-02, 9.93165746e-03,\n",
       "         1.02605373e-02, 4.71244492e-02, 3.05981785e-02, 2.91234851e-02,\n",
       "         3.57233323e-02, 2.31832229e-02, 4.84859347e-02, 4.44973782e-02,\n",
       "         9.29652900e-02, 7.59867206e-02, 3.38457674e-02, 1.28628900e-02,\n",
       "         4.99342084e-02, 4.42648493e-02, 9.09934565e-03, 2.41011176e-02,\n",
       "         4.35389206e-02, 1.13759516e-02, 1.94260389e-01, 1.44384829e-02,\n",
       "         4.80468832e-02, 9.31224087e-04, 1.97366819e-01, 1.13414712e-01,\n",
       "         1.63532227e-01, 2.78688632e-02, 1.95328016e-02, 1.03904858e-01,\n",
       "         4.15529646e-02, 3.50793032e-03, 3.73312309e-02, 5.76407760e-02,\n",
       "         1.15992781e-02, 2.78003840e-03, 3.14269438e-02, 7.73049425e-03,\n",
       "         1.48735242e-02, 4.69614146e-03, 8.50264728e-03, 4.07373197e-02,\n",
       "         6.36124015e-02, 3.12252380e-02, 8.46726969e-02, 8.12002632e-04,\n",
       "         7.72279277e-02, 2.13812869e-02, 9.43242833e-02, 2.71530971e-02,\n",
       "         1.09527176e-02, 2.55277641e-02, 6.58254372e-03, 7.79830813e-02,\n",
       "         2.55387323e-03, 8.76148492e-02, 1.49993328e-02, 2.30327174e-02,\n",
       "         2.23047230e-02, 1.47205135e-02, 1.23137347e-02, 3.20473202e-02,\n",
       "         5.59948273e-02, 6.51742471e-03, 9.51099116e-03, 1.58266667e-02,\n",
       "         6.58135787e-02, 3.03908680e-02, 2.23577302e-02, 4.21971752e-04,\n",
       "         3.28831747e-02, 4.66689393e-02, 1.78790782e-02, 1.36526977e-03,\n",
       "         1.39865894e-02, 1.05154596e-03, 1.55481568e-03, 5.94068505e-03,\n",
       "         9.09992028e-03, 6.70994446e-02, 1.41998231e-02, 3.91810350e-02,\n",
       "         4.68014441e-02, 1.78149845e-02, 1.15603551e-01, 2.33405251e-02,\n",
       "         3.28199230e-02, 2.24277023e-02, 1.15724485e-02, 3.22769135e-02,\n",
       "         1.28390072e-02, 5.07971384e-02, 6.69911951e-02, 5.17413579e-02,\n",
       "         1.57138892e-02, 3.35883200e-02, 2.87061720e-03, 1.81422457e-02,\n",
       "         9.73812565e-02, 4.97635305e-02, 4.35182266e-02, 3.76477428e-02,\n",
       "         1.20043661e-02, 2.44299951e-03, 2.82540619e-02, 5.94694205e-02,\n",
       "         1.12285756e-01, 1.30282603e-02, 9.25580447e-04, 2.05679517e-02,\n",
       "         4.02956717e-02, 3.54056887e-04, 1.26216579e-02, 1.00005092e-03,\n",
       "         7.56892795e-03, 5.10608666e-02, 4.25749496e-02, 1.24280024e-02,\n",
       "         5.68090826e-02, 1.30275879e-02, 1.20589854e-02, 3.20830569e-02,\n",
       "         6.32710606e-02, 1.05552144e-01, 7.70843914e-03, 2.29288097e-02,\n",
       "         2.81420499e-02, 2.75476277e-02, 5.23878112e-02, 1.58545934e-02,\n",
       "         1.07581224e-02, 5.67963421e-02, 5.68301789e-02, 3.42573458e-03,\n",
       "         1.00182071e-02, 9.60669294e-02, 2.22943164e-02, 2.56181248e-02,\n",
       "         3.69941853e-02, 5.45714870e-02, 7.53196329e-03, 2.91243084e-02,\n",
       "         2.62789391e-02, 4.90197092e-02, 1.41477948e-02, 2.24119965e-02,\n",
       "         4.87802252e-02, 3.01028639e-02, 8.84402171e-03, 2.88758837e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 9,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([2.35137437e-03, 1.49564417e-02, 7.64522376e-03, 6.30220249e-02,\n",
       "         1.13142662e-01, 8.48244596e-03, 3.74818854e-02, 5.96913956e-02,\n",
       "         3.23337987e-02, 4.78214845e-02, 1.36813279e-02, 1.28555754e-02,\n",
       "         2.61712447e-03, 6.97496533e-02, 1.76993888e-02, 4.05417196e-02,\n",
       "         1.90210156e-02, 2.85917073e-02, 3.21874134e-02, 8.99398923e-02,\n",
       "         4.26010042e-02, 3.06836851e-02, 2.04916354e-02, 7.63605461e-02,\n",
       "         1.88181326e-02, 8.50302652e-02, 5.07232919e-02, 4.61545819e-03,\n",
       "         2.76162457e-02, 1.69292204e-02, 3.39367874e-02, 5.26108630e-02,\n",
       "         5.85378110e-02, 2.23277118e-02, 3.96524966e-02, 4.84052226e-02,\n",
       "         2.79489215e-02, 9.14826244e-03, 9.93993506e-02, 1.38043603e-02,\n",
       "         5.46226837e-02, 1.11167796e-01, 7.56353932e-03, 7.27979392e-02,\n",
       "         3.24089974e-02, 2.38807101e-04, 1.15090817e-01, 1.08353071e-01,\n",
       "         2.08003316e-02, 2.75571328e-02, 3.94201418e-03, 4.06496711e-02,\n",
       "         1.42138209e-02, 2.80969925e-02, 4.26897733e-03, 7.22958520e-03,\n",
       "         9.49440673e-02, 2.28499994e-03, 2.02414729e-02, 4.45986912e-02,\n",
       "         1.30067002e-02, 2.81494893e-02, 5.86981885e-03, 2.57015210e-02,\n",
       "         1.17382575e-02, 4.65032179e-03, 2.30131038e-02, 6.57250313e-03,\n",
       "         3.10802925e-02, 5.55568524e-02, 1.50680747e-02, 1.91311184e-02,\n",
       "         7.63794547e-03, 4.34363149e-02, 4.03626356e-03, 3.88438031e-02,\n",
       "         3.78476828e-02, 4.81557772e-02, 5.26054986e-02, 9.20996349e-03,\n",
       "         1.19183147e-02, 3.18526477e-03, 1.19893126e-01, 7.60571100e-03,\n",
       "         1.18893543e-02, 4.79613524e-03, 6.11161739e-02, 1.12551386e-02,\n",
       "         2.87672002e-02, 5.34940045e-03, 3.78339668e-03, 5.94570767e-03,\n",
       "         7.43177459e-02, 2.75453385e-02, 5.88927232e-02, 6.36296794e-02,\n",
       "         3.85706685e-02, 2.40025073e-02, 2.50074100e-02, 1.70468409e-02,\n",
       "         1.96612813e-02, 1.84926949e-03, 1.49465203e-02, 6.41382858e-02,\n",
       "         3.03531308e-02, 2.07802672e-02, 1.36311492e-02, 5.10307588e-03,\n",
       "         9.18348134e-03, 1.95352677e-02, 2.41339169e-02, 5.56540340e-02,\n",
       "         1.13391355e-02, 1.25819696e-02, 5.20657115e-02, 7.89455101e-02,\n",
       "         1.81040280e-02, 6.15631230e-02, 3.35940942e-02, 3.40769514e-02,\n",
       "         1.40885273e-02, 7.00123282e-03, 6.91037774e-02, 1.08494602e-01,\n",
       "         4.03191447e-02, 4.28021792e-03, 7.89284892e-03, 1.87835295e-03,\n",
       "         1.44251883e-01, 4.16830815e-02, 2.04344187e-03, 8.73270258e-02,\n",
       "         1.71102528e-02, 1.91742424e-02, 8.62041637e-02, 1.49674220e-02,\n",
       "         1.46456892e-02, 7.49294236e-02, 1.92382634e-02, 2.71332078e-02,\n",
       "         3.12766917e-02, 1.18486080e-02, 3.38633521e-03, 2.04618908e-02,\n",
       "         2.80826744e-02, 9.94096161e-04, 9.79615655e-03, 4.37579900e-02,\n",
       "         5.88079616e-02, 1.08569749e-02, 1.50081953e-02, 3.12387738e-02,\n",
       "         4.62816842e-02, 6.62449449e-02, 1.94106810e-02, 3.57951149e-02,\n",
       "         5.44601306e-02, 1.08933644e-02, 1.06705338e-01, 8.82059634e-02,\n",
       "         7.06259999e-03, 3.93396355e-02, 1.16409846e-01, 6.27401993e-02,\n",
       "         2.44070943e-02, 1.06377723e-02, 7.57645071e-03, 9.42535400e-02,\n",
       "         1.46569517e-02, 2.33844779e-02, 4.65465337e-02, 4.97808866e-02,\n",
       "         1.80723853e-02, 2.29025763e-02, 6.07363544e-02, 3.42513882e-02,\n",
       "         4.94647920e-02, 3.67099904e-02, 9.22583975e-03, 1.63255236e-03,\n",
       "         2.76776087e-02, 4.03104722e-02, 8.59796107e-02, 1.13857584e-02,\n",
       "         7.34287277e-02, 5.74096898e-03, 1.08895898e-02, 3.54993492e-02,\n",
       "         1.28222620e-02, 4.75024246e-02, 3.60981971e-02, 1.10928722e-01,\n",
       "         2.85627730e-02, 3.16327401e-02, 8.19376297e-03, 5.52450567e-02,\n",
       "         9.31749493e-03, 6.79999441e-02, 7.23948404e-02, 4.97481041e-02,\n",
       "         1.33223068e-02, 2.52767876e-02, 2.05919165e-02, 4.11202908e-02,\n",
       "         2.96149752e-03, 1.41858738e-02, 2.38491390e-02, 2.24606525e-02,\n",
       "         9.98791121e-03, 2.43934095e-02, 6.65453300e-02, 5.57407693e-05,\n",
       "         7.89074786e-03, 1.95557419e-02, 4.72933054e-02, 6.56848401e-02,\n",
       "         4.29651588e-02, 5.47915101e-02, 7.62323439e-02, 2.21991092e-02,\n",
       "         1.09498715e-02, 2.43294658e-03, 5.55084944e-02, 3.79093015e-03,\n",
       "         1.80661753e-02, 4.12699487e-03, 2.63966564e-02, 2.27972749e-03,\n",
       "         4.65760529e-02, 4.77798237e-03, 1.36875845e-02, 5.35433821e-04,\n",
       "         7.51378685e-02, 2.09267978e-02, 2.68483721e-02, 1.32982489e-02,\n",
       "         1.01108812e-01, 1.80023666e-02, 3.42510231e-02, 6.87795039e-03,\n",
       "         4.37774770e-02, 6.40302151e-02, 8.10584892e-03, 9.79050547e-02,\n",
       "         4.89143468e-02, 4.48332541e-02, 1.52046727e-02, 9.02694091e-03,\n",
       "         6.30661249e-02, 4.96177236e-03, 4.75197695e-02, 1.68583468e-02,\n",
       "         7.01638730e-03, 1.04866233e-02, 1.22721144e-03, 2.72469260e-02,\n",
       "         3.46277133e-02, 1.03849899e-02, 5.63764386e-03, 3.99550013e-02,\n",
       "         6.34287372e-02, 1.01158535e-02, 6.11530058e-03, 1.22746602e-02,\n",
       "         4.82594296e-02, 1.08555835e-02, 6.50944980e-03, 6.44137040e-02,\n",
       "         2.47884672e-02, 3.13588344e-02, 7.20059220e-03, 1.33951555e-03,\n",
       "         4.88417298e-02, 9.20236483e-02, 6.22081477e-03, 1.48276817e-02,\n",
       "         6.68363972e-03, 5.36087044e-02, 2.28837365e-03, 3.69538255e-02,\n",
       "         4.14572991e-02, 8.99168383e-03, 9.36880112e-02, 2.04041749e-02,\n",
       "         5.97815812e-02, 6.66042492e-02, 2.41398849e-02, 6.27800673e-02,\n",
       "         1.00078350e-02, 8.45318362e-02, 3.79062109e-02, 4.98892814e-02,\n",
       "         2.34278645e-02, 5.21431491e-02, 1.85058787e-02, 1.37080287e-03,\n",
       "         8.56853798e-02, 5.07102907e-02, 3.81189846e-02, 8.10110867e-02,\n",
       "         2.41895169e-02, 4.18964624e-02, 6.95628822e-02, 2.41836663e-02,\n",
       "         5.35437427e-02, 1.75998919e-02, 1.93199143e-02, 8.57885778e-02,\n",
       "         1.26001555e-02, 3.90316434e-02, 3.97733599e-02, 6.90072924e-02,\n",
       "         5.54835540e-04, 2.29232423e-02, 3.67116067e-03, 1.10009091e-03,\n",
       "         1.13881052e-01, 2.26995405e-02, 1.65653657e-02, 1.03053628e-02,\n",
       "         8.77469629e-02, 3.71498168e-02, 1.21558802e-02, 3.78232487e-02,\n",
       "         1.26545057e-02, 9.26981419e-02, 1.12768160e-02, 3.41385044e-02,\n",
       "         7.60638192e-02, 1.45913698e-02, 4.14845571e-02, 2.56900042e-02,\n",
       "         2.08495115e-03, 4.27229963e-02, 1.40835792e-02, 6.62988098e-03,\n",
       "         2.50901338e-02, 9.77295935e-02, 3.48651665e-03, 8.10449757e-03,\n",
       "         3.45246145e-03, 6.83487430e-02, 1.82614494e-02, 2.41619498e-02,\n",
       "         3.84816303e-05, 4.55821157e-02, 2.06500553e-02, 1.35152824e-02,\n",
       "         7.91935176e-02, 3.58173847e-02, 8.49435180e-02, 3.70937027e-02,\n",
       "         2.00232957e-02, 3.78699927e-03, 4.24489379e-02, 1.24365825e-03,\n",
       "         1.16025023e-02, 2.53092777e-02, 2.54305229e-02, 6.27565086e-02,\n",
       "         6.74653053e-02, 1.16486736e-02, 8.63764882e-02, 6.59145787e-03,\n",
       "         5.36696287e-03, 1.32753691e-02, 1.09255314e-02, 6.11817203e-02,\n",
       "         5.86059084e-03, 4.27558348e-02, 5.47892554e-03, 7.49715194e-02,\n",
       "         1.32120922e-02, 6.81084469e-02, 1.28017049e-02, 1.62563764e-03,\n",
       "         6.27828762e-02, 1.12045325e-01, 3.01669911e-03, 2.30450351e-02,\n",
       "         2.36537475e-02, 6.51838705e-02, 7.35966563e-02, 2.68747397e-02,\n",
       "         2.84087863e-02, 1.16210338e-02, 8.53520259e-02, 7.23718554e-02,\n",
       "         3.35507654e-02, 8.30177292e-02, 7.93409243e-04, 5.36083477e-03,\n",
       "         8.32210407e-02, 1.39757534e-02, 1.01175308e-02, 5.09426892e-02,\n",
       "         6.90276101e-02, 3.12136784e-02, 6.48007914e-02, 3.66383530e-02,\n",
       "         4.40188460e-02, 3.22682150e-02, 2.47091223e-02, 6.80719595e-03,\n",
       "         1.55264267e-03, 3.73418182e-02, 4.81527932e-02, 2.26712283e-02,\n",
       "         5.24770655e-02, 3.37277055e-02, 3.67028154e-02, 2.19517313e-02,\n",
       "         1.15176933e-02, 2.00519674e-02, 3.52409370e-02, 3.67284194e-02,\n",
       "         1.28476303e-02, 1.35409366e-02, 1.25191985e-02, 9.26023126e-02,\n",
       "         5.97358961e-03, 6.93249553e-02, 1.06809800e-02, 1.36144925e-02,\n",
       "         2.33216919e-02, 7.76500534e-03, 3.13269384e-02, 8.11112300e-02,\n",
       "         1.56257749e-02, 2.50260662e-02, 1.49509078e-02, 6.33239597e-02,\n",
       "         2.52923090e-03, 1.53005468e-02, 1.54828932e-02, 5.71433343e-02,\n",
       "         1.17944274e-02, 1.78194828e-02, 4.77373600e-03, 1.52671374e-02,\n",
       "         4.99082319e-02, 8.32396224e-02, 1.87774971e-02, 2.94351075e-02,\n",
       "         4.50939313e-02, 3.67179550e-02, 2.54061399e-03, 1.98253039e-02,\n",
       "         3.27292159e-02, 2.30714306e-02, 5.71164563e-02, 1.25710055e-01,\n",
       "         4.58278321e-02, 2.65179873e-02, 5.90401664e-02, 4.82941754e-02,\n",
       "         2.14665961e-02, 7.78817339e-04, 4.21150997e-02, 2.37329807e-02,\n",
       "         4.12085233e-03, 3.67593579e-03, 9.96312723e-02, 4.99331839e-02,\n",
       "         7.77037740e-02, 7.10721612e-02, 1.92940310e-02, 6.93773329e-02,\n",
       "         2.76938621e-02, 1.88076701e-02, 7.55631477e-02, 5.40562114e-03,\n",
       "         3.64459269e-02, 3.34246680e-02, 5.10565750e-02, 1.97710861e-02,\n",
       "         1.99428219e-02, 1.31699704e-02, 1.09754410e-02, 4.48627956e-02,\n",
       "         2.66063050e-03, 1.25837801e-02, 5.61865093e-03, 4.55252416e-02,\n",
       "         3.53279822e-02, 2.50892658e-02, 2.28301082e-02, 1.81594733e-02,\n",
       "         3.99374589e-02, 8.19903333e-03, 3.01582776e-02, 2.63254270e-02,\n",
       "         4.86096507e-03, 3.15454453e-02, 1.45083532e-01, 8.77908990e-02,\n",
       "         4.58071157e-02, 1.67435333e-02, 7.53628835e-03, 5.40145580e-03,\n",
       "         1.53245693e-02, 2.86769196e-02, 2.62447763e-02, 4.23266403e-02,\n",
       "         6.22858992e-03, 2.12359782e-02, 2.68882085e-02, 3.58877182e-02,\n",
       "         8.68690982e-02, 4.57353238e-03, 3.49198445e-03, 2.15569250e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 10,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([1.36272116e-02, 1.16326753e-03, 5.00219315e-02, 2.26518307e-02,\n",
       "         5.45251183e-03, 4.94826846e-02, 2.19303183e-02, 1.06385507e-01,\n",
       "         1.00543745e-01, 5.93871204e-03, 3.96000105e-04, 2.58553661e-02,\n",
       "         2.64139641e-02, 2.49160593e-03, 3.28354239e-02, 1.31932925e-02,\n",
       "         6.86507970e-02, 4.84360047e-02, 6.46497905e-02, 5.55621553e-03,\n",
       "         7.56680593e-03, 2.69605424e-02, 7.52854068e-03, 2.57415939e-02,\n",
       "         6.41211271e-02, 1.31061235e-02, 5.39596342e-02, 1.23221748e-01,\n",
       "         3.30343586e-03, 4.12241705e-02, 2.83752739e-01, 4.96835224e-02,\n",
       "         1.12980762e-02, 1.25104114e-01, 7.35713635e-03, 6.80615306e-02,\n",
       "         8.16091429e-03, 4.22739908e-02, 6.03239909e-02, 2.20692772e-02,\n",
       "         3.77485082e-02, 9.53518134e-03, 7.84534262e-04, 2.45646611e-02,\n",
       "         4.94010150e-02, 1.65789966e-02, 3.42918606e-03, 2.93668895e-03,\n",
       "         4.81831916e-02, 4.66259522e-03, 4.00233129e-03, 4.98117954e-02,\n",
       "         2.63852291e-02, 1.67029016e-02, 7.64488708e-03, 4.44230400e-02,\n",
       "         2.04837136e-02, 1.77009758e-02, 1.44755910e-03, 4.64663357e-02,\n",
       "         3.64004746e-02, 3.74453217e-02, 4.01679426e-02, 2.49277428e-02,\n",
       "         5.44378236e-02, 1.63379256e-02, 9.79148969e-03, 7.38567263e-02,\n",
       "         2.87782550e-02, 1.83458498e-03, 3.26953679e-02, 3.40387560e-02,\n",
       "         5.80952615e-02, 6.84642866e-02, 6.87412175e-05, 1.60126537e-02,\n",
       "         5.45394141e-03, 9.23597254e-03, 2.96400413e-02, 1.93293169e-02,\n",
       "         9.45578590e-02, 2.91580483e-02, 3.96267185e-03, 7.98060596e-02,\n",
       "         2.62087062e-02, 1.72563642e-02, 2.86064129e-02, 4.61038984e-02,\n",
       "         7.04279318e-02, 1.05576022e-02, 4.85606156e-02, 1.88437272e-02,\n",
       "         2.41551490e-04, 1.76375136e-01, 6.19329326e-02, 2.02845410e-02,\n",
       "         1.82833299e-02, 2.20899936e-02, 1.01126567e-01, 1.21033443e-02,\n",
       "         1.64514035e-02, 2.91086361e-02, 1.35473870e-02, 1.88702699e-02,\n",
       "         6.28627241e-02, 4.14517894e-02, 2.50133337e-03, 2.89264005e-02,\n",
       "         4.54059578e-02, 2.40424983e-02, 1.24656260e-01, 1.29512742e-01,\n",
       "         5.76663725e-02, 1.68012530e-02, 3.71164419e-02, 1.34730050e-02,\n",
       "         5.97374514e-02, 2.21693050e-03, 5.42011298e-03, 2.60008662e-03,\n",
       "         1.39895629e-03, 1.37654189e-02, 6.70940662e-03, 2.47495007e-02,\n",
       "         5.50752096e-02, 4.84060980e-02, 7.88427740e-02, 1.98596101e-02,\n",
       "         5.71905933e-02, 4.08379659e-02, 5.04284874e-02, 2.36331876e-02,\n",
       "         5.02224490e-02, 2.02307906e-02, 9.74026788e-03, 5.05678467e-02,\n",
       "         5.11578210e-02, 7.12193325e-02, 3.85731161e-02, 5.61917722e-02,\n",
       "         6.65401369e-02, 1.72806811e-02, 4.52939793e-02, 2.91241575e-02,\n",
       "         3.32013145e-02, 2.80328691e-02, 8.35165307e-02, 2.33458243e-02,\n",
       "         1.00559905e-01, 2.64231139e-03, 2.03014780e-02, 5.41008823e-02,\n",
       "         2.16197148e-02, 7.58179724e-02, 1.44581413e-02, 5.01644164e-02,\n",
       "         3.55062708e-02, 3.29028405e-02, 1.99723095e-02, 4.71775532e-02,\n",
       "         2.17644796e-02, 1.68571249e-02, 1.16406865e-01, 3.25674787e-02,\n",
       "         4.47809622e-02, 1.09216543e-02, 1.43905850e-02, 1.84014514e-02,\n",
       "         2.23230105e-02, 3.76009755e-02, 4.96659800e-03, 3.92564274e-02,\n",
       "         2.46266983e-02, 7.39823505e-02, 1.00664003e-02, 2.19703186e-02,\n",
       "         8.52967128e-02, 2.26319712e-02, 2.34078448e-02, 2.09725611e-02,\n",
       "         3.61605845e-02, 4.47607860e-02, 8.52476433e-03, 1.62702873e-02,\n",
       "         1.93849541e-02, 8.56441911e-03, 2.13189144e-02, 6.86229998e-03,\n",
       "         9.13235825e-03, 2.30309796e-02, 1.71127375e-02, 4.36594523e-02,\n",
       "         7.42639452e-02, 1.29384333e-02, 2.14240118e-03, 1.01425350e-02,\n",
       "         2.08073948e-02, 1.09801777e-02, 3.93854268e-02, 7.41558746e-02,\n",
       "         3.27908583e-02, 5.15195318e-02, 2.61925794e-02, 6.13677055e-02,\n",
       "         4.77786351e-04, 6.12450019e-03, 2.38602757e-02, 2.28928030e-03,\n",
       "         1.26249818e-02, 5.53869605e-02, 2.07835436e-02, 4.39256802e-02,\n",
       "         8.65950286e-02, 1.18978620e-02, 8.94173235e-02, 3.09763886e-02,\n",
       "         1.12346731e-01, 2.51291674e-02, 1.08336108e-02, 1.08484933e-02,\n",
       "         2.79476661e-02, 3.03872935e-02, 1.29206441e-02, 2.57193204e-02,\n",
       "         1.24676190e-02, 8.75188503e-03, 4.13438911e-03, 3.58777530e-02,\n",
       "         1.00907227e-02, 4.62579392e-02, 2.01887190e-02, 1.88831724e-02,\n",
       "         3.90736805e-03, 5.88163920e-02, 3.42754312e-02, 2.14911196e-02,\n",
       "         6.79733406e-04, 4.07374166e-02, 4.11736481e-02, 6.70599565e-03,\n",
       "         5.03849536e-02, 8.74473061e-03, 3.71340290e-02, 2.14786977e-02,\n",
       "         5.55784255e-02, 1.55225350e-02, 2.19633598e-02, 3.55397165e-02,\n",
       "         4.16678041e-02, 3.90816741e-02, 5.77326119e-02, 5.06521352e-02,\n",
       "         2.33498178e-02, 1.10653557e-01, 4.82219458e-02, 4.82740849e-02,\n",
       "         5.28044738e-02, 3.93872261e-02, 4.54997867e-02, 1.00017944e-02,\n",
       "         9.27811302e-03, 1.77524798e-02, 6.84828088e-02, 2.99481675e-02,\n",
       "         4.54545617e-02, 1.79308355e-02, 4.54251794e-03, 4.08889577e-02,\n",
       "         1.97018757e-02, 8.76490120e-03, 2.81039663e-02, 4.99156397e-03,\n",
       "         4.54271883e-02, 1.99236628e-02, 5.14652058e-02, 6.67791590e-02,\n",
       "         4.79061678e-02, 6.44081272e-03, 3.98696251e-02, 9.32248961e-03,\n",
       "         3.06239463e-02, 8.95829580e-04, 2.03295425e-02, 4.11449792e-03,\n",
       "         4.22174558e-02, 5.78525737e-02, 5.25914282e-02, 4.19649482e-02,\n",
       "         4.99524698e-02, 1.27687864e-02, 3.71391065e-02, 1.26576815e-02,\n",
       "         1.04238559e-02, 3.11880372e-02, 3.86689678e-02, 6.59281202e-03,\n",
       "         2.23135285e-04, 1.47647047e-02, 5.25836833e-03, 1.12278166e-03,\n",
       "         8.35880339e-02, 3.62878032e-02, 4.64269072e-02, 1.48890363e-02,\n",
       "         5.79448715e-02, 8.65531266e-02, 2.51795612e-02, 5.69619127e-02,\n",
       "         9.79452860e-03, 9.73573513e-03, 5.50028980e-02, 3.68894599e-02,\n",
       "         6.84119901e-03, 2.05088053e-02, 2.21210364e-02, 8.99741948e-02,\n",
       "         7.36512020e-02, 1.98491756e-02, 7.05822036e-02, 3.75908501e-02,\n",
       "         1.68106481e-02, 6.75058737e-02, 4.81670685e-02, 4.88170944e-02,\n",
       "         1.73546616e-02, 1.48932328e-02, 2.42049601e-02, 1.59131009e-02,\n",
       "         1.88727416e-02, 1.42822564e-02, 7.31339604e-02, 9.19832941e-03,\n",
       "         3.68578844e-02, 1.13589019e-01, 1.24626998e-02, 8.60237032e-02,\n",
       "         6.04006927e-03, 5.09615950e-02, 7.25306850e-03, 2.96986802e-03,\n",
       "         8.77117068e-02, 1.39280725e-02, 2.75984649e-02, 2.11944841e-02,\n",
       "         7.71718239e-03, 5.51068448e-02, 8.85270070e-03, 3.96347381e-02,\n",
       "         8.58237181e-05, 6.67730300e-03, 2.24461649e-02, 4.09714878e-02,\n",
       "         2.11054180e-02, 2.55436022e-02, 3.10061127e-02, 1.77128240e-02,\n",
       "         2.33052913e-02, 1.84745211e-02, 8.25826749e-02, 2.02659927e-02,\n",
       "         1.10063488e-02, 1.13206189e-02, 4.28280374e-03, 1.70319956e-02,\n",
       "         7.77712911e-02, 2.14747433e-02, 3.78544554e-02, 1.02226669e-02,\n",
       "         9.47643667e-02, 2.23493241e-02, 4.42157127e-02, 3.92888524e-02,\n",
       "         4.15467732e-02, 9.79273543e-02, 5.61875897e-03, 9.79584269e-03,\n",
       "         2.53485069e-02, 6.01402810e-03, 1.45289088e-02, 2.95991860e-02,\n",
       "         1.71419047e-02, 6.14129268e-02, 9.08644032e-03, 2.61972677e-02,\n",
       "         3.55191194e-02, 1.32925650e-02, 1.50105945e-04, 2.10893285e-02,\n",
       "         2.01560929e-02, 1.61204655e-02, 5.13792783e-02, 5.97366579e-02,\n",
       "         1.06795579e-02, 3.57530601e-02, 1.57504324e-02, 3.18217352e-02,\n",
       "         1.39599815e-02, 1.58299077e-02, 2.36014719e-03, 5.76503128e-02,\n",
       "         3.56676616e-02, 4.88025397e-02, 6.82346849e-03, 3.30507606e-02,\n",
       "         1.49200605e-02, 2.40411647e-02, 2.49764882e-02, 2.40258733e-03,\n",
       "         6.31666183e-02, 7.40379319e-02, 4.12671231e-02, 5.36676459e-02,\n",
       "         4.94510075e-03, 2.81851403e-02, 2.51501575e-02, 5.32246195e-02,\n",
       "         4.95801680e-03, 5.14115542e-02, 3.62910628e-02, 1.09099820e-02,\n",
       "         7.10101351e-02, 1.69755071e-02, 3.78098339e-02, 8.46522395e-03,\n",
       "         7.62056699e-03, 6.38104603e-02, 2.62751281e-02, 7.19627813e-02,\n",
       "         1.17447920e-01, 1.74554251e-02, 2.16186661e-02, 1.82933342e-02,\n",
       "         8.24428629e-03, 9.10130739e-02, 1.42505101e-03, 5.22722155e-02,\n",
       "         2.80158725e-02, 6.10456131e-02, 9.47444439e-02, 2.12989263e-02,\n",
       "         9.49283093e-02, 1.49880657e-02, 4.42919834e-03, 4.42881277e-03,\n",
       "         1.01286583e-01, 1.03328063e-03, 2.92898108e-06, 4.70893607e-02,\n",
       "         9.19349119e-02, 3.66023481e-02, 3.46713554e-04, 5.87616675e-02,\n",
       "         3.63911688e-03, 6.41062297e-03, 3.36951413e-03, 2.05394030e-02,\n",
       "         3.18260565e-02, 1.15002384e-02, 1.70340209e-04, 2.33870167e-02,\n",
       "         9.48106311e-03, 2.69067250e-02, 2.95113716e-02, 1.31742489e-02,\n",
       "         3.86635885e-02, 4.05370221e-02, 3.64218578e-02, 3.63730229e-02,\n",
       "         2.04108506e-02, 8.62550829e-03, 5.32515300e-03, 1.34874918e-02,\n",
       "         7.15050921e-02, 9.86865535e-02, 2.01634802e-02, 6.86599128e-03,\n",
       "         4.74617593e-02, 1.39218848e-02, 1.81654084e-03, 1.77928600e-02,\n",
       "         2.84151044e-02, 4.01032716e-02, 4.64250147e-02, 7.55867688e-04,\n",
       "         2.70289481e-02, 1.22551294e-02, 8.12680367e-03, 1.74042899e-02,\n",
       "         5.23053035e-02, 2.42619440e-02, 5.29533364e-02, 4.30931337e-02,\n",
       "         1.41749363e-02, 7.15104938e-02, 2.69773304e-02, 6.13774592e-03,\n",
       "         7.83846434e-03, 1.42550552e-02, 2.13442463e-02, 3.29120420e-02,\n",
       "         4.65846248e-03, 4.05918136e-02, 3.74364778e-02, 2.53275037e-02,\n",
       "         8.82045999e-02, 6.34366497e-02, 4.16971855e-02, 3.02925408e-02,\n",
       "         4.48677205e-02, 4.58274968e-02, 3.00389100e-02, 2.06215438e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 11,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([5.04152775e-02, 1.80287156e-02, 5.31228296e-02, 1.01837315e-01,\n",
       "         1.25880763e-01, 1.47883398e-02, 3.68666984e-02, 2.28749551e-02,\n",
       "         3.43106017e-02, 2.75405333e-03, 3.25667597e-02, 3.81215215e-02,\n",
       "         3.06014158e-02, 2.51368713e-02, 7.61075458e-03, 4.90106177e-03,\n",
       "         3.75680365e-02, 5.78195378e-02, 6.40828013e-02, 1.96246654e-02,\n",
       "         5.30879153e-03, 2.27201637e-03, 4.20562699e-02, 3.03453431e-02,\n",
       "         3.81819606e-02, 2.57239770e-02, 5.76986969e-02, 6.89196438e-02,\n",
       "         3.91809642e-02, 1.04952669e-02, 5.76390997e-02, 2.11195387e-02,\n",
       "         4.27977182e-03, 3.84512730e-02, 5.18684722e-02, 1.56043291e-01,\n",
       "         1.15404548e-02, 4.80976328e-02, 1.70046650e-02, 2.47384273e-02,\n",
       "         6.57383353e-02, 8.71953648e-03, 5.13129197e-02, 2.41259821e-02,\n",
       "         8.46969150e-03, 4.93097901e-02, 4.52201217e-02, 1.48559911e-02,\n",
       "         2.50775237e-02, 1.74065288e-02, 1.01944255e-02, 7.68108070e-02,\n",
       "         6.24960363e-02, 3.97958122e-02, 3.94933224e-02, 6.01072935e-03,\n",
       "         1.86157301e-02, 2.99997684e-02, 4.89591584e-02, 6.99902102e-02,\n",
       "         9.69897285e-02, 4.23293561e-02, 5.60290180e-03, 9.20510478e-03,\n",
       "         1.17475670e-02, 3.57609279e-02, 3.73025462e-02, 4.83845510e-02,\n",
       "         3.86405177e-02, 2.86843199e-02, 1.61251705e-03, 4.14352585e-03,\n",
       "         8.02125856e-02, 5.72679145e-03, 2.04945114e-02, 3.64203639e-02,\n",
       "         1.40863890e-02, 4.06395551e-03, 3.57076004e-02, 3.21698263e-02,\n",
       "         8.99367407e-03, 1.33123547e-02, 5.53884730e-02, 6.36490807e-02,\n",
       "         1.23075617e-03, 4.64979820e-02, 3.45460139e-02, 6.53917715e-02,\n",
       "         7.06217587e-02, 4.13547494e-02, 2.87743658e-02, 1.44317625e-02,\n",
       "         8.02971125e-02, 1.88325741e-03, 6.41505495e-02, 2.50112209e-02,\n",
       "         4.59227674e-02, 9.70459078e-03, 9.26053673e-02, 4.95841019e-02,\n",
       "         4.41590548e-02, 1.15018465e-01, 1.84656102e-02, 1.46151250e-02,\n",
       "         1.66298710e-02, 1.59408990e-02, 9.45861042e-02, 6.64913654e-02,\n",
       "         9.68329236e-03, 1.75438523e-02, 8.45356099e-03, 1.01752020e-02,\n",
       "         3.06762196e-02, 2.22832058e-02, 2.74468772e-02, 4.31705778e-03,\n",
       "         3.98104005e-02, 3.71721312e-02, 3.36000230e-03, 4.97912616e-02,\n",
       "         1.10205248e-01, 3.18376832e-02, 3.92845422e-02, 1.08978096e-02,\n",
       "         1.78700313e-02, 3.81301902e-03, 5.86915612e-02, 3.46123278e-02,\n",
       "         2.23803520e-02, 1.01922736e-01, 4.39514592e-02, 2.28062477e-02,\n",
       "         4.33792174e-02, 4.41385359e-02, 6.21265732e-02, 1.50800627e-02,\n",
       "         3.40848118e-02, 5.37205068e-03, 6.06974810e-02, 7.19133839e-02,\n",
       "         1.60511751e-02, 6.78744391e-02, 2.72851381e-02, 8.23060498e-02,\n",
       "         4.86388505e-02, 1.48981973e-03, 5.32959029e-02, 7.06951926e-03,\n",
       "         4.97577228e-02, 3.49036343e-02, 2.73719598e-02, 6.84835017e-02,\n",
       "         3.69774271e-03, 1.29762353e-04, 3.21478350e-03, 3.14676650e-02,\n",
       "         8.50698818e-03, 4.72136810e-02, 1.45894168e-02, 2.69132350e-02,\n",
       "         7.94326607e-03, 4.89221653e-03, 3.08495620e-03, 1.29624736e-02,\n",
       "         1.83932297e-02, 9.29528251e-02, 1.92013998e-02, 5.90772592e-02,\n",
       "         3.75395231e-02, 7.77877122e-02, 5.18166386e-02, 4.85342480e-02,\n",
       "         1.18845664e-02, 5.08167073e-02, 2.64534466e-02, 2.60474738e-02,\n",
       "         2.60817520e-02, 4.15773969e-03, 4.78793345e-02, 4.20293920e-02,\n",
       "         4.83840480e-02, 7.23155681e-05, 3.50483172e-02, 2.49718670e-02,\n",
       "         2.58719977e-02, 3.81843676e-03, 1.61406044e-02, 3.39890607e-02,\n",
       "         4.91755605e-02, 3.22460420e-02, 9.60550737e-03, 5.87733537e-02,\n",
       "         2.57944670e-02, 1.93669498e-02, 5.66739589e-02, 3.78275514e-02,\n",
       "         4.89683151e-02, 6.31872565e-02, 2.56296862e-02, 6.51025549e-02,\n",
       "         4.05866951e-02, 5.30001568e-03, 4.27980945e-02, 3.13928016e-02,\n",
       "         3.55761521e-03, 2.10098196e-02, 2.38533015e-03, 4.49300446e-02,\n",
       "         2.22094096e-02, 3.52413617e-02, 2.48319600e-02, 2.26529483e-02,\n",
       "         1.93032511e-02, 5.36107719e-02, 8.23388174e-02, 1.49869416e-02,\n",
       "         1.41880959e-01, 5.57269268e-02, 3.88656706e-02, 3.20775285e-02,\n",
       "         1.63544714e-02, 4.97113653e-02, 2.97674481e-02, 4.09531891e-02,\n",
       "         6.28257543e-02, 7.20369071e-02, 1.10104829e-02, 1.65396929e-02,\n",
       "         3.38106640e-02, 3.07767224e-02, 4.07851562e-02, 8.03631637e-03,\n",
       "         2.39183009e-02, 2.02252567e-02, 4.43501258e-03, 1.54751702e-03,\n",
       "         4.40904833e-02, 4.21476178e-02, 5.66458404e-02, 3.08755301e-02,\n",
       "         5.98729998e-02, 1.37211844e-01, 6.75545260e-02, 4.20209132e-02,\n",
       "         6.51684357e-03, 1.27640665e-02, 1.97639577e-02, 5.44711053e-02,\n",
       "         3.69748734e-02, 3.83495316e-02, 2.81092990e-02, 2.89365686e-02,\n",
       "         6.57937825e-02, 1.01865483e-02, 2.55726813e-03, 3.06167360e-02,\n",
       "         1.12493355e-02, 2.48480644e-02, 3.09370784e-03, 2.15382800e-02,\n",
       "         6.51249960e-02, 2.84817796e-02, 1.65700689e-02, 8.69678147e-03,\n",
       "         9.98665299e-03, 3.35554667e-02, 4.17197831e-02, 8.25572759e-03,\n",
       "         2.15679836e-02, 1.15117244e-02, 2.62303688e-02, 3.03076562e-02,\n",
       "         4.23570424e-02, 1.09940112e-01, 9.35454965e-02, 1.50951231e-02,\n",
       "         5.86269125e-02, 2.61346390e-03, 3.62480357e-02, 6.88597187e-02,\n",
       "         7.05867782e-02, 3.77453379e-02, 2.18398087e-02, 5.85384816e-02,\n",
       "         1.79691538e-02, 1.36569347e-02, 2.87912767e-02, 4.69142050e-02,\n",
       "         3.12789120e-02, 4.77907248e-02, 6.73968941e-02, 1.47052454e-02,\n",
       "         7.00210314e-03, 1.90365929e-02, 3.62356678e-02, 4.03130762e-02,\n",
       "         1.30688902e-02, 7.78177902e-02, 2.27221344e-02, 4.07693684e-02,\n",
       "         8.21435545e-03, 6.30586296e-02, 3.29069570e-02, 1.26070427e-02,\n",
       "         2.94633280e-03, 4.46139239e-02, 3.73906605e-02, 1.51668852e-02,\n",
       "         1.90264639e-02, 6.71823025e-02, 7.46296123e-02, 2.66108029e-02,\n",
       "         3.32670994e-02, 1.17705138e-02, 7.90249482e-02, 4.93588159e-03,\n",
       "         4.13126238e-02, 7.38348521e-04, 2.50429269e-02, 1.64997429e-02,\n",
       "         8.76447186e-02, 2.37096082e-02, 2.89675836e-02, 5.83328791e-02,\n",
       "         1.38151515e-02, 3.21890824e-02, 8.83444026e-03, 5.76685406e-02,\n",
       "         1.66471917e-02, 1.28833866e-02, 1.59581024e-02, 4.09002341e-02,\n",
       "         1.35029322e-02, 7.53681175e-03, 1.41599281e-02, 1.41432565e-02,\n",
       "         4.17019473e-03, 1.48236863e-02, 8.68699979e-03, 1.84413772e-02,\n",
       "         4.98012863e-02, 3.08403652e-03, 4.37020101e-02, 2.12716572e-02,\n",
       "         1.62366126e-02, 3.89946662e-02, 1.02834096e-02, 7.36162160e-03,\n",
       "         8.26176314e-04, 2.82867495e-02, 5.55146895e-02, 3.54185961e-02,\n",
       "         3.27896178e-02, 6.84628189e-02, 4.19961810e-02, 7.68549591e-02,\n",
       "         7.98743814e-02, 1.39079187e-02, 3.99574563e-02, 1.51551645e-02,\n",
       "         1.25805885e-02, 1.28425241e-01, 6.91612577e-03, 4.41182442e-02,\n",
       "         3.40224430e-02, 5.90349436e-02, 3.11118048e-02, 6.82050828e-03,\n",
       "         3.21292989e-02, 1.95690654e-02, 3.84373330e-02, 2.82203015e-02,\n",
       "         1.70205999e-02, 5.49557954e-02, 2.53785737e-02, 2.57408451e-02,\n",
       "         1.23022497e-01, 3.50243300e-02, 5.73698618e-02, 1.38867181e-03,\n",
       "         1.16159916e-02, 4.58519943e-02, 4.19754200e-02, 3.00030783e-02,\n",
       "         6.42818213e-02, 6.62020966e-03, 1.89988595e-02, 1.18210100e-01,\n",
       "         1.17710400e-02, 3.82802524e-02, 3.13398018e-02, 1.17917191e-02,\n",
       "         3.38985808e-02, 2.92156562e-02, 1.72004178e-02, 5.65275587e-02,\n",
       "         2.66381516e-03, 4.19224054e-03, 5.07111549e-02, 2.89071165e-02,\n",
       "         4.20923792e-02, 3.97495441e-02, 1.41059551e-02, 1.92503762e-02,\n",
       "         8.85246613e-04, 1.42980386e-02, 4.10973057e-02, 3.54872607e-02,\n",
       "         1.95956398e-02, 2.75133792e-02, 2.03053374e-03, 4.78287227e-02,\n",
       "         3.17454822e-02, 7.73363048e-03, 3.69835505e-03, 3.80954668e-02,\n",
       "         4.04491127e-02, 6.58982620e-02, 6.25382811e-02, 4.36191782e-02,\n",
       "         2.65170112e-02, 1.39039964e-03, 4.79830690e-02, 1.03664972e-01,\n",
       "         1.27261095e-02, 2.22743042e-02, 1.70227475e-02, 1.38745178e-02,\n",
       "         1.62096955e-02, 4.31357473e-02, 3.40367854e-02, 4.22281660e-02,\n",
       "         9.48557779e-02, 3.50672677e-02, 7.91255906e-02, 1.03782034e-02,\n",
       "         5.63144684e-02, 7.43147358e-02, 2.07746923e-02, 6.46988153e-02,\n",
       "         2.64601745e-02, 3.59584354e-02, 3.95087786e-02, 6.15089871e-02,\n",
       "         3.02398559e-02, 2.75126304e-02, 5.78583516e-02, 3.21226716e-02,\n",
       "         7.41395801e-02, 3.43104824e-02, 9.89477113e-02, 3.98455523e-02,\n",
       "         1.97501741e-02, 2.24897619e-02, 3.96456197e-02, 1.14906244e-01,\n",
       "         8.23249854e-03, 4.98963073e-02, 3.32153067e-02, 4.41297926e-02,\n",
       "         6.86951587e-03, 1.40075535e-02, 8.13911762e-03, 1.86233036e-02,\n",
       "         4.31288444e-02, 7.19825923e-02, 8.38810019e-03, 2.38817688e-02,\n",
       "         7.00688213e-02, 5.15660755e-02, 2.18405519e-02, 9.97680426e-03,\n",
       "         2.88781114e-02, 1.02146911e-02, 4.93515655e-02, 2.42696814e-02,\n",
       "         1.22449370e-02, 5.19426800e-02, 2.24408749e-02, 2.64869258e-02,\n",
       "         4.60411683e-02, 4.86763157e-02, 1.13075254e-02, 1.83495954e-02,\n",
       "         7.47606829e-02, 1.00084037e-01, 1.39785791e-02, 1.73113476e-02,\n",
       "         3.25245224e-02, 4.53273430e-02, 2.13686898e-02, 7.79709255e-04,\n",
       "         5.11753261e-02, 8.41847360e-02, 6.31835759e-02, 6.70525655e-02,\n",
       "         1.24290595e-02, 7.42115900e-02, 3.66962478e-02, 3.42034339e-03,\n",
       "         9.67616141e-02, 5.50084151e-02, 2.79293535e-03, 4.31654640e-02,\n",
       "         2.49419063e-02, 5.68209477e-02, 4.20629010e-02, 1.20762968e-03,\n",
       "         2.12947782e-02, 1.30295262e-01, 1.22838551e-02, 1.11696329e-02],\n",
       "        dtype=float32)},\n",
       " {'layer_idx': 12,\n",
       "  'num_filters': 512,\n",
       "  'scores': array([0.04839082, 0.04051949, 0.0476842 , 0.04353967, 0.04599054,\n",
       "         0.04971477, 0.03844222, 0.03681399, 0.06395033, 0.05439623,\n",
       "         0.04300738, 0.04185522, 0.03546749, 0.03727844, 0.04649539,\n",
       "         0.03816675, 0.04157858, 0.03878619, 0.03744332, 0.04460508,\n",
       "         0.04694525, 0.04830824, 0.04772465, 0.0389294 , 0.0380483 ,\n",
       "         0.03637335, 0.03947621, 0.04134468, 0.04210853, 0.04256196,\n",
       "         0.02852198, 0.04052885, 0.05150764, 0.03959928, 0.04023007,\n",
       "         0.03517135, 0.05187062, 0.03718488, 0.03210909, 0.04050965,\n",
       "         0.03757366, 0.04352154, 0.04334864, 0.0374242 , 0.04931208,\n",
       "         0.04952614, 0.05362179, 0.04487845, 0.03058885, 0.05973344,\n",
       "         0.04144393, 0.0553008 , 0.03958012, 0.04391192, 0.04341954,\n",
       "         0.04779644, 0.03795032, 0.0411182 , 0.0426482 , 0.04645869,\n",
       "         0.04126531, 0.04346207, 0.04986065, 0.04331366, 0.04458906,\n",
       "         0.04498387, 0.03760678, 0.04575816, 0.03280278, 0.03824217,\n",
       "         0.05010113, 0.04138296, 0.04066041, 0.04370027, 0.04452352,\n",
       "         0.04060507, 0.04565666, 0.0502402 , 0.04239872, 0.04421775,\n",
       "         0.0457236 , 0.03554906, 0.03831436, 0.04568977, 0.04634516,\n",
       "         0.03821843, 0.04096396, 0.03994181, 0.04414222, 0.04300558,\n",
       "         0.04341518, 0.04795678, 0.03427271, 0.04656284, 0.0412894 ,\n",
       "         0.04310562, 0.03330834, 0.04291062, 0.03877631, 0.0462299 ,\n",
       "         0.0429575 , 0.05358782, 0.04787045, 0.05247334, 0.04617084,\n",
       "         0.03830885, 0.05808004, 0.04105717, 0.03919681, 0.03871864,\n",
       "         0.04054184, 0.03889732, 0.03883341, 0.04464975, 0.04423358,\n",
       "         0.04524504, 0.0400195 , 0.03185195, 0.04907155, 0.04098472,\n",
       "         0.04120901, 0.03400346, 0.04273703, 0.03249675, 0.04241881,\n",
       "         0.03775341, 0.04842659, 0.04638172, 0.04451657, 0.04193696,\n",
       "         0.04136926, 0.04416285, 0.05703407, 0.03858813, 0.04306572,\n",
       "         0.0416641 , 0.04269179, 0.03842031, 0.04123398, 0.04907414,\n",
       "         0.03283798, 0.03234773, 0.04864027, 0.05261185, 0.05215131,\n",
       "         0.03845251, 0.03358449, 0.03985699, 0.04861332, 0.04019548,\n",
       "         0.06298308, 0.03573367, 0.04113998, 0.0475137 , 0.04606108,\n",
       "         0.04373728, 0.05806664, 0.03601028, 0.05360281, 0.04831491,\n",
       "         0.04166085, 0.05213683, 0.03760386, 0.04205177, 0.05032229,\n",
       "         0.05151921, 0.03965034, 0.04812388, 0.04294634, 0.04788046,\n",
       "         0.05398725, 0.04113974, 0.03143116, 0.03482739, 0.04446812,\n",
       "         0.03591558, 0.05036633, 0.03920407, 0.03933582, 0.04342156,\n",
       "         0.04519592, 0.0496131 , 0.03671023, 0.04284004, 0.03419252,\n",
       "         0.04452939, 0.0412843 , 0.0587525 , 0.03805474, 0.04592116,\n",
       "         0.03673613, 0.05280409, 0.04758386, 0.04858228, 0.04366091,\n",
       "         0.04002244, 0.05023289, 0.05844085, 0.0513482 , 0.04214013,\n",
       "         0.04225776, 0.03101285, 0.0423332 , 0.0428159 , 0.04530174,\n",
       "         0.03232846, 0.0414881 , 0.03288477, 0.04201494, 0.04932639,\n",
       "         0.03878507, 0.03657591, 0.0568798 , 0.0370654 , 0.05022076,\n",
       "         0.04710764, 0.05519427, 0.04025321, 0.04099453, 0.04358312,\n",
       "         0.04135671, 0.05734502, 0.04998674, 0.03694427, 0.05347517,\n",
       "         0.04382944, 0.05081286, 0.0360324 , 0.04629506, 0.04706341,\n",
       "         0.05083233, 0.04014361, 0.04002315, 0.04236494, 0.05444089,\n",
       "         0.03266562, 0.03501074, 0.04615358, 0.04974431, 0.05048065,\n",
       "         0.05161294, 0.05056146, 0.04576367, 0.06095219, 0.04007227,\n",
       "         0.04733201, 0.04326551, 0.04023024, 0.03327096, 0.04238763,\n",
       "         0.03976497, 0.03828429, 0.04572506, 0.04611222, 0.03936601,\n",
       "         0.03005433, 0.04165863, 0.04154677, 0.04037508, 0.04870966,\n",
       "         0.04158672, 0.04021532, 0.04484421, 0.04452442, 0.04845528,\n",
       "         0.06804021, 0.03620851, 0.03530486, 0.04229964, 0.04688687,\n",
       "         0.03425075, 0.04960687, 0.04506458, 0.05491053, 0.04638508,\n",
       "         0.03350473, 0.03351181, 0.04202725, 0.03975733, 0.04244018,\n",
       "         0.04530941, 0.04696083, 0.0496223 , 0.04433249, 0.04865931,\n",
       "         0.04276673, 0.04631533, 0.04569963, 0.05001953, 0.04002184,\n",
       "         0.04150826, 0.04827132, 0.0454191 , 0.0450381 , 0.03734209,\n",
       "         0.04472054, 0.04614869, 0.04504222, 0.04793978, 0.04414765,\n",
       "         0.02527921, 0.04440196, 0.04758909, 0.05394854, 0.04055457,\n",
       "         0.04596188, 0.04739437, 0.04397793, 0.04359095, 0.03942477,\n",
       "         0.03996179, 0.04690287, 0.05303498, 0.04578879, 0.04594243,\n",
       "         0.04711012, 0.03905404, 0.0405086 , 0.03014977, 0.04908264,\n",
       "         0.03898796, 0.04311313, 0.04443112, 0.04397535, 0.04454932,\n",
       "         0.05504844, 0.04375834, 0.04812474, 0.04753514, 0.06173855,\n",
       "         0.05267836, 0.04070698, 0.04922054, 0.04090244, 0.04334424,\n",
       "         0.05165289, 0.04054806, 0.04175283, 0.0441061 , 0.04445682,\n",
       "         0.03778434, 0.04642621, 0.03880982, 0.04683091, 0.04043789,\n",
       "         0.04211557, 0.04682901, 0.04112173, 0.04134809, 0.03042152,\n",
       "         0.05084557, 0.04744956, 0.0431937 , 0.04124741, 0.03594266,\n",
       "         0.03405327, 0.03793537, 0.04072667, 0.04017801, 0.05101118,\n",
       "         0.02945717, 0.04465926, 0.04963602, 0.04699022, 0.04744373,\n",
       "         0.05211905, 0.05415331, 0.05397979, 0.04587897, 0.04528759,\n",
       "         0.0247253 , 0.04606384, 0.04185402, 0.0365351 , 0.04991965,\n",
       "         0.04573235, 0.04797156, 0.03405544, 0.03642524, 0.04416181,\n",
       "         0.04004575, 0.04438691, 0.03755757, 0.05593538, 0.04059406,\n",
       "         0.04202616, 0.03075038, 0.04335076, 0.03325362, 0.05142312,\n",
       "         0.06671517, 0.05257709, 0.04470742, 0.03686921, 0.04281773,\n",
       "         0.04155854, 0.06350463, 0.03318822, 0.04872334, 0.05495328,\n",
       "         0.04658776, 0.04542536, 0.03706556, 0.06968331, 0.0422613 ,\n",
       "         0.04007171, 0.05630985, 0.03886158, 0.05118893, 0.04378952,\n",
       "         0.05506717, 0.03790819, 0.04773654, 0.03816383, 0.03859718,\n",
       "         0.04061663, 0.03802124, 0.03812475, 0.04669973, 0.05296126,\n",
       "         0.03586677, 0.0379451 , 0.04021009, 0.05455668, 0.05352602,\n",
       "         0.03516359, 0.04192715, 0.0379434 , 0.04596895, 0.04325829,\n",
       "         0.04111949, 0.0505854 , 0.04180957, 0.05079323, 0.04590263,\n",
       "         0.04849336, 0.03603611, 0.03907439, 0.04670434, 0.04496328,\n",
       "         0.04628586, 0.0464799 , 0.06219895, 0.03599988, 0.03392233,\n",
       "         0.03832181, 0.04931647, 0.04222429, 0.0488694 , 0.03446986,\n",
       "         0.04462077, 0.03641188, 0.0499171 , 0.03625979, 0.03729781,\n",
       "         0.05642389, 0.05407965, 0.04326087, 0.03768601, 0.04178695,\n",
       "         0.0437369 , 0.03941744, 0.04791521, 0.03835606, 0.04692733,\n",
       "         0.04722974, 0.04019324, 0.03468401, 0.04655768, 0.05690794,\n",
       "         0.04905646, 0.03772458, 0.03905891, 0.05404992, 0.03827021,\n",
       "         0.04106007, 0.04093283, 0.03631176, 0.0423936 , 0.04773471,\n",
       "         0.03499908, 0.04388653, 0.04336067, 0.04044522, 0.04693763,\n",
       "         0.04398632, 0.04350635, 0.03927801, 0.04124354, 0.03426563,\n",
       "         0.04601605, 0.04020052, 0.04251679, 0.03709489, 0.04221728,\n",
       "         0.04661985, 0.03570305, 0.0387538 , 0.04018201, 0.04277258,\n",
       "         0.0435849 , 0.04475252, 0.04450726, 0.04596805, 0.04428743,\n",
       "         0.03954041, 0.04895646, 0.04489402, 0.05376951, 0.04206614,\n",
       "         0.05197887, 0.05023611], dtype=float32)}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CIFAR 10 + GA\n",
    "# ==============================================================================\n",
    "# 1. Cáº¥u hÃ¬nh & Load Model\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "config.classifier_type = 'B' # Hoáº·c loáº¡i báº¡n Ä‘Ã£ train\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Táº¡o model vÃ  load weight cÅ©\n",
    "model = ModifiedVGG16Model(config).to(config.device)\n",
    "checkpoint = torch.load(\"./checkpoint/vgg16_cifar10_baseline.pth\", map_location=config.device) # ÄÆ°á»ng dáº«n file ckpt cá»§a báº¡n\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "print(f\"âœ… Loaded model from epoch {checkpoint['epoch']} with Acc: {checkpoint['acc']}%\")\n",
    "\n",
    "# 2. Khá»Ÿi táº¡o Helper cÅ© (Ä‘á»ƒ láº¥y dá»¯ liá»‡u train tÃ­nh rank)\n",
    "# Helper nÃ y chá»‹u trÃ¡ch nhiá»‡m tÃ­nh Taylor Score\n",
    "old_pruner = VGG16Pruner(config, model, save_name='ga_optimise_ckpt.pth') \n",
    "\n",
    "# 3. Khá»Ÿi táº¡o GA Pruner (Code má»›i)\n",
    "input_size = torch.randn(1, 3, 32, 32).to(config.device)\n",
    "ga_pruner = GAPruner(model, config, input_size)\n",
    "\n",
    "ga_pruner.strategy_mode = 'layer_wise'\n",
    "\n",
    "# 4. XÃ¢y dá»±ng báº£ng tra cá»©u (BÆ°á»›c nÃ y cháº¡y ~1 epoch Ä‘á»ƒ tÃ­nh rank, máº¥t khoáº£ng 1-2 phÃºt)\n",
    "ga_pruner.build_ranking_tables(old_pruner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874fcef",
   "metadata": {},
   "source": [
    "#### Check score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920744de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Building Ranking Tables (Pre-computation)...\n",
      "   Running ranking epoch (accumulating gradients)...\n",
      "\n",
      "ðŸ“Š --- SMART MATCHING LAYERS ---\n",
      "   Raw ranks key type: <class 'int'>\n",
      "   Layer 12: max=0.0680, mean=0.0437\n",
      "   Layer 11: max=0.1628, mean=0.0340\n",
      "   Layer 2: max=0.2792, mean=0.0681\n",
      "   Layer 1: max=0.4717, mean=0.0858\n",
      "   Layer 0: max=0.6061, mean=0.0835\n",
      "âœ… Successfully built ranking tables for 13 layers.\n",
      "ðŸ”¥ Calculated Global Threshold (80%): 0.059312\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Gá»ŒI HÃ€M \n",
    "try:\n",
    "    layer_data = ga_pruner.build_ranking_tables(old_pruner) \n",
    "except NameError:\n",
    "    layer_data = ga_pruner.build_ranking_tables(pruner_helper)\n",
    "\n",
    "# Kiá»ƒm tra dá»¯ liá»‡u\n",
    "if not layer_data:\n",
    "    print(\"âŒ Layer Data is empty! Cannot plot.\")\n",
    "else:\n",
    "    # 2. Xá»¬ LÃ Dá»® LIá»†U\n",
    "    all_scores_flat = [] \n",
    "    plot_data = []       \n",
    "\n",
    "    for item in layer_data:\n",
    "        scores = item['scores']\n",
    "        all_scores_flat.extend(scores)\n",
    "        plot_data.append(scores)\n",
    "\n",
    "    all_scores_flat = np.array(all_scores_flat)\n",
    "\n",
    "    # TÃ­nh Global Threshold\n",
    "    global_threshold = np.percentile(all_scores_flat, 80)\n",
    "    print(f\"ðŸ”¥ Calculated Global Threshold (80%): {global_threshold:.6f}\")\n",
    "\n",
    "    # 3. Váº¼ BIá»‚U Äá»’ SCATTER\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    layer_indices = range(len(plot_data))\n",
    "    layer_labels = [f\"L{item['layer_idx']}\\n({item['num_filters']})\" for item in layer_data]\n",
    "\n",
    "    for i, scores in enumerate(plot_data):\n",
    "        x_jitter = np.random.normal(i, 0.05, size=len(scores))\n",
    "        colors = ['#ff9999' if s < global_threshold else '#66b3ff' for s in scores]\n",
    "        plt.scatter(x_jitter, scores, s=15, c=colors, alpha=0.7, edgecolors='none')\n",
    "        plt.scatter(i, np.max(scores), s=60, c='red', marker='*', edgecolors='black', zorder=10)\n",
    "\n",
    "    plt.axhline(y=global_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({global_threshold:.4f})')\n",
    "\n",
    "    plt.title('Real Layer-wise Score Distribution (VGG16)', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Taylor Importance Score', fontsize=14)\n",
    "    plt.xlabel('Layer Index', fontsize=14)\n",
    "    plt.xticks(layer_indices, layer_labels, fontsize=11)\n",
    "    \n",
    "    # Annotations L10 & L12\n",
    "    if len(plot_data) > 10:\n",
    "        l10_max = np.max(plot_data[10])\n",
    "        plt.annotate('L10 Spike', xy=(10, l10_max), xytext=(8, l10_max+0.05), arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    if len(plot_data) > 12:\n",
    "        l12_max = np.max(plot_data[12])\n",
    "        plt.annotate('COLLAPSE', xy=(12, l12_max), xytext=(11, l12_max+0.1), arrowprops=dict(facecolor='red', shrink=0.05), color='red')\n",
    "\n",
    "    plt.grid(True, linestyle=':', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('real_score_distribution.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5e81b",
   "metadata": {},
   "source": [
    "### 5.2. Starting GA Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdfe026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting GA Evolution... Target MACs <= 20.0%\n",
      "Gen 1/30 | Fit: -0.8587 | MACs: 0.09G | Params: 5.76M\n",
      "Gen 2/30 | Fit: -0.6035 | MACs: 0.09G | Params: 3.93M\n",
      "Gen 3/30 | Fit: -0.5348 | MACs: 0.09G | Params: 5.30M\n",
      "Gen 4/30 | Fit: -0.1144 | MACs: 0.07G | Params: 3.23M\n",
      "Gen 5/30 | Fit: 0.0982 | MACs: 0.06G | Params: 3.67M\n",
      "Gen 6/30 | Fit: 0.0982 | MACs: 0.06G | Params: 3.67M\n",
      "Gen 7/30 | Fit: 0.1600 | MACs: 0.07G | Params: 2.93M\n",
      "Gen 8/30 | Fit: 0.1600 | MACs: 0.07G | Params: 2.93M\n",
      "Gen 9/30 | Fit: 0.1600 | MACs: 0.07G | Params: 2.93M\n",
      "Gen 10/30 | Fit: 0.2222 | MACs: 0.06G | Params: 2.84M\n",
      "Gen 11/30 | Fit: 0.2237 | MACs: 0.06G | Params: 2.85M\n",
      "Gen 12/30 | Fit: 0.2673 | MACs: 0.07G | Params: 3.89M\n",
      "Gen 13/30 | Fit: 0.3449 | MACs: 0.06G | Params: 2.77M\n",
      "Gen 14/30 | Fit: 0.3562 | MACs: 0.07G | Params: 3.85M\n",
      "Gen 15/30 | Fit: 0.3614 | MACs: 0.07G | Params: 3.92M\n",
      "Gen 16/30 | Fit: 0.3619 | MACs: 0.06G | Params: 4.23M\n",
      "Gen 17/30 | Fit: 0.3619 | MACs: 0.06G | Params: 4.23M\n",
      "Gen 18/30 | Fit: 0.3619 | MACs: 0.06G | Params: 4.23M\n",
      "Gen 19/30 | Fit: 0.3634 | MACs: 0.06G | Params: 4.34M\n",
      "Gen 20/30 | Fit: 0.3640 | MACs: 0.06G | Params: 4.35M\n",
      "Gen 21/30 | Fit: 0.3654 | MACs: 0.07G | Params: 4.75M\n",
      "Gen 22/30 | Fit: 0.3654 | MACs: 0.07G | Params: 4.75M\n",
      "Gen 23/30 | Fit: 0.7403 | MACs: 0.06G | Params: 4.34M\n",
      "Gen 24/30 | Fit: 0.7420 | MACs: 0.06G | Params: 4.35M\n",
      "Gen 25/30 | Fit: 0.7454 | MACs: 0.07G | Params: 4.75M\n",
      "Gen 26/30 | Fit: 0.7454 | MACs: 0.07G | Params: 4.75M\n",
      "Gen 27/30 | Fit: 0.7473 | MACs: 0.07G | Params: 4.76M\n",
      "Gen 28/30 | Fit: 0.7703 | MACs: 0.07G | Params: 4.99M\n",
      "Gen 29/30 | Fit: 0.7703 | MACs: 0.07G | Params: 4.99M\n",
      "Gen 30/30 | Fit: 0.7703 | MACs: 0.07G | Params: 4.99M\n",
      "\n",
      "ðŸ† Evolution Finished.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SEED = 2001\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Cháº¡y GA: VÃ­ dá»¥ muá»‘n giáº£m cÃ²n 30% MACs (giáº£m 70%)\n",
    "# target_macs_ratio = 0.3 nghÄ©a lÃ  giá»¯ láº¡i 30% MACs gá»‘c\n",
    "best_chrom = ga_pruner.evolve(pop_size=50, n_generations=30, target_macs_ratio=0.2)\n",
    "\n",
    "# Váº½ biá»ƒu Ä‘á»“ bÃ¡o cÃ¡o\n",
    "ga_pruner.plot_results(best_chrom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43acf0",
   "metadata": {},
   "source": [
    "### 5.3. Physical pruning GA Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0266c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ‚ï¸ Applying GA-consistent Physical Pruning...\n",
      "\n",
      "âœ‚ï¸ Pruning Plan:\n",
      "Layer 0: Keep 24 (min)\n",
      "Layer 1: Keep 32 (min)\n",
      "Layer 2: Keep 48 (min)\n",
      "Layer 3: Keep 56 (min)\n",
      "Layer 4: Keep 96 (min)\n",
      "Layer 5: Keep 104 (min)\n",
      "Layer 6: Keep 96 (min)\n",
      "Layer 7: Keep 320 (min)\n",
      "Layer 8: Keep 200 (min)\n",
      "Layer 9: Keep 264 (min)\n",
      "Layer 10: Keep 312 (min)\n",
      "Layer 11: Keep 448 (min)\n",
      "Layer 12: Keep 344 (min)\n",
      "   GA-Layer 0 â†’ Model-Layer 0: Pruning 40/64 [min]\n",
      "   GA-Layer 1 â†’ Model-Layer 3: Pruning 32/64 [min]\n",
      "   GA-Layer 2 â†’ Model-Layer 7: Pruning 80/128 [min]\n",
      "   GA-Layer 3 â†’ Model-Layer 10: Pruning 72/128 [min]\n",
      "   GA-Layer 4 â†’ Model-Layer 14: Pruning 160/256 [min]\n",
      "   GA-Layer 5 â†’ Model-Layer 17: Pruning 152/256 [min]\n",
      "   GA-Layer 6 â†’ Model-Layer 20: Pruning 160/256 [min]\n",
      "   GA-Layer 7 â†’ Model-Layer 24: Pruning 192/512 [min]\n",
      "   GA-Layer 8 â†’ Model-Layer 27: Pruning 312/512 [min]\n",
      "   GA-Layer 9 â†’ Model-Layer 30: Pruning 248/512 [min]\n",
      "   GA-Layer 10 â†’ Model-Layer 34: Pruning 200/512 [min]\n",
      "   GA-Layer 11 â†’ Model-Layer 37: Pruning 64/512 [min]\n",
      "   GA-Layer 12 â†’ Model-Layer 40: Pruning 168/512 [min]\n",
      "\n",
      "âœ… Physical Pruning Finished.\n",
      "ðŸ“‰ Final Model Stats: MACs=0.086G | Params=23.599M\n",
      "ðŸ§® Expected MACs (GA): 0.067G | Actual MACs: 0.086G\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: PHYSICAL PRUNING (GA-CONSISTENT EXECUTION)\n",
    "# ==============================================================================\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "\n",
    "print(\"\\nâœ‚ï¸ Applying GA-consistent Physical Pruning...\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0. Chuáº©n bá»‹: snapshot model gá»‘c & ranking GLOBAL\n",
    "# ------------------------------------------------------------------\n",
    "base_model = copy.deepcopy(model).cpu()\n",
    "base_model.eval()\n",
    "\n",
    "# Láº¤Y GLOBAL RANKING (Ä‘Ãºng vá»›i GA fitness)\n",
    "# KhÃ´ng bao giá» dÃ¹ng ranking sau prune\n",
    "global_layer_stats = ga_pruner.layer_stats        # chá»©a raw + norm\n",
    "global_ranking_tables = ga_pruner.ranking_tables  # min + avg (global ref)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Decode pruning plan tá»« GA\n",
    "# ------------------------------------------------------------------\n",
    "pruning_plan = ga_pruner.apply_best_pruning(best_chrom)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Khá»Ÿi táº¡o model má»›i Ä‘á»ƒ prune (NO CASCADE)\n",
    "# ------------------------------------------------------------------\n",
    "pruned_model = copy.deepcopy(base_model)\n",
    "\n",
    "# Helper: map GA-layer-idx â†’ real Conv2d idx trong model.features\n",
    "def map_ga_to_model_conv(model, ga_idx):\n",
    "    conv_count = 0\n",
    "    for idx, m in enumerate(model.features):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if conv_count == ga_idx:\n",
    "                return idx\n",
    "            conv_count += 1\n",
    "    return -1\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Prune tá»«ng layer Äá»˜C Láº¬P â€“ Ä‘Ãºng GA assumption\n",
    "# ------------------------------------------------------------------\n",
    "for item in pruning_plan:\n",
    "    layer_idx = item['layer_idx']\n",
    "    n_keep = item['n_keep']\n",
    "    strategy = item['strategy']\n",
    "\n",
    "    # Láº¥y ranking GLOBAL (model gá»‘c)\n",
    "    raw_scores = global_layer_stats[layer_idx]['raw']\n",
    "    total_filters = len(raw_scores)\n",
    "\n",
    "    n_prune = total_filters - n_keep\n",
    "    if n_prune <= 0:\n",
    "        continue\n",
    "\n",
    "    # Sort index theo Taylor score (weak â†’ strong)\n",
    "    sorted_idx = np.argsort(raw_scores)\n",
    "\n",
    "    # -------------------------------\n",
    "    # STRATEGY HANDLING (PAPER-LEVEL)\n",
    "    # -------------------------------\n",
    "    if strategy == 'min':\n",
    "        # Cáº¯t filter yáº¿u nháº¥t cá»§a layer\n",
    "        indices_to_prune = sorted_idx[:n_prune]\n",
    "\n",
    "    elif strategy == 'avg':\n",
    "        # GLOBAL MEDIAN STRATEGY\n",
    "        # Giá»¯ band trung vá»‹ cá»§a GLOBAL distribution\n",
    "        global_ref = global_ranking_tables['avg'][layer_idx]\n",
    "\n",
    "        g_low = np.percentile(global_ref, 25)\n",
    "        g_high = np.percentile(global_ref, 75)\n",
    "\n",
    "        # Prune filter quÃ¡ yáº¿u hoáº·c quÃ¡ máº¡nh\n",
    "        indices_to_prune = np.where(\n",
    "            (raw_scores < g_low) | (raw_scores > g_high)\n",
    "        )[0]\n",
    "\n",
    "        # Safety: náº¿u prune quÃ¡ tay â†’ fallback sang min\n",
    "        if len(indices_to_prune) > n_prune:\n",
    "            indices_to_prune = indices_to_prune[:n_prune]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    # Reverse sort Ä‘á»ƒ prune an toÃ n\n",
    "    indices_to_prune = sorted(indices_to_prune, reverse=True)\n",
    "\n",
    "    # Map sang layer thá»±c trong model\n",
    "    real_layer_idx = map_ga_to_model_conv(pruned_model, layer_idx)\n",
    "    assert real_layer_idx >= 0, f\"Cannot map GA layer {layer_idx}\"\n",
    "\n",
    "    print(\n",
    "        f\"   GA-Layer {layer_idx} â†’ Model-Layer {real_layer_idx}: \"\n",
    "        f\"Pruning {len(indices_to_prune)}/{total_filters} [{strategy}]\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # THá»°C HIá»†N PRUNING\n",
    "    # -------------------------------\n",
    "    for fidx in indices_to_prune:\n",
    "        prune_vgg16_conv_layer_BN(\n",
    "            pruned_model,\n",
    "            real_layer_idx,\n",
    "            fidx,\n",
    "            use_cuda=False\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Chuyá»ƒn model vá» device & kiá»ƒm tra\n",
    "# ------------------------------------------------------------------\n",
    "pruned_model = pruned_model.to(config.device)\n",
    "pruned_model.eval()\n",
    "\n",
    "final_macs, final_params = tp.utils.count_ops_and_params(\n",
    "    pruned_model,\n",
    "    input_size\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Physical Pruning Finished.\")\n",
    "print(\n",
    "    f\"ðŸ“‰ Final Model Stats: \"\n",
    "    f\"MACs={final_macs/1e9:.3f}G | Params={final_params/1e6:.3f}M\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Log Expected vs Actual (Ráº¤T QUAN TRá»ŒNG CHO PAPER)\n",
    "# ------------------------------------------------------------------\n",
    "expected_macs, expected_params = ga_pruner._estimate_metrics(\n",
    "    ga_pruner._decode_chromosome(best_chrom)[0]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"ðŸ§® Expected MACs (GA): {expected_macs/1e9:.3f}G | \"\n",
    "    f\"Actual MACs: {final_macs/1e9:.3f}G\"\n",
    ")\n",
    "\n",
    "# Snapshot chuáº©n sau pruning (KHÃ”NG Ä‘Æ°á»£c mutate)\n",
    "final_pruned_model = copy.deepcopy(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4d59a",
   "metadata": {},
   "source": [
    "### 5.4. Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc12f2f",
   "metadata": {},
   "source": [
    "#### 5.4.1. Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11dbff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Fine-tuning NORMAL (from pruned snapshot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NORMAL] Epoch 001 | Train 69.27% | Val 80.79%\n",
      "[NORMAL] Epoch 002 | Train 84.58% | Val 83.15%\n",
      "[NORMAL] Epoch 003 | Train 87.81% | Val 85.22%\n",
      "[NORMAL] Epoch 004 | Train 89.69% | Val 86.65%\n",
      "[NORMAL] Epoch 005 | Train 90.93% | Val 87.50%\n",
      "[NORMAL] Epoch 006 | Train 91.59% | Val 87.28%\n",
      "[NORMAL] Epoch 007 | Train 92.51% | Val 88.07%\n",
      "[NORMAL] Epoch 008 | Train 93.03% | Val 88.30%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnormal_ft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 491\u001b[0m, in \u001b[0;36mModifiedVGG16Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 491\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    493\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2817\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2815\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2817\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3A: FINE-TUNING NORMAL (INDEPENDENT)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nðŸš€ Fine-tuning NORMAL (from pruned snapshot)\")\n",
    "\n",
    "device = config.device\n",
    "\n",
    "# âš ï¸ QUAN TRá»ŒNG: deepcopy\n",
    "normal_ft_model = copy.deepcopy(final_pruned_model).to(device)\n",
    "normal_ft_model.train()\n",
    "\n",
    "train_loader = old_pruner.train_loader\n",
    "test_loader  = old_pruner.test_loader\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    normal_ft_model.parameters(),\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=150\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc_normal = 0.0\n",
    "\n",
    "for epoch in range(150):\n",
    "    normal_ft_model.train()\n",
    "    correct, total, train_loss = 0, 0, 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = normal_ft_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(normal_ft_model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    normal_ft_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = normal_ft_model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct / total\n",
    "\n",
    "    print(f\"[NORMAL] Epoch {epoch+1:03d} | Train {train_acc:.2f}% | Val {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_acc_normal:\n",
    "        best_acc_normal = val_acc\n",
    "        torch.save({\n",
    "            'model': normal_ft_model.state_dict(),\n",
    "            'acc': best_acc_normal\n",
    "        }, \"./checkpoint/pruned_finetuned_normal.pth\")\n",
    "\n",
    "print(f\"âœ… NORMAL FT Best Acc = {best_acc_normal:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0329c",
   "metadata": {},
   "source": [
    "#### 5.4.2. KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35483eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Äang khá»Ÿi táº¡o Knowledge Distillation...\n",
      "âœ… Teacher loaded (Acc: 93.62%)\n",
      "âœ… Student (Pruned Model) ready.\n",
      "\n",
      "ðŸ”¥ Báº¯t Ä‘áº§u Training KD trong 150 epochs...\n",
      "Epoch 1/150 | Loss: 4.844 | Train Acc: 65.64% | Val Acc: 67.99%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 67.99%)\n",
      "Epoch 2/150 | Loss: 3.306 | Train Acc: 76.75% | Val Acc: 75.12%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 75.12%)\n",
      "Epoch 3/150 | Loss: 2.865 | Train Acc: 79.85% | Val Acc: 77.75%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 77.75%)\n",
      "Epoch 4/150 | Loss: 2.567 | Train Acc: 82.00% | Val Acc: 82.54%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 82.54%)\n",
      "Epoch 5/150 | Loss: 2.367 | Train Acc: 83.64% | Val Acc: 79.46%\n",
      "Epoch 6/150 | Loss: 2.200 | Train Acc: 84.64% | Val Acc: 81.90%\n",
      "Epoch 7/150 | Loss: 2.061 | Train Acc: 85.59% | Val Acc: 80.92%\n",
      "Epoch 8/150 | Loss: 1.947 | Train Acc: 86.44% | Val Acc: 78.85%\n",
      "Epoch 9/150 | Loss: 1.868 | Train Acc: 87.11% | Val Acc: 83.69%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 83.69%)\n",
      "Epoch 10/150 | Loss: 1.783 | Train Acc: 87.57% | Val Acc: 83.90%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 83.90%)\n",
      "Epoch 11/150 | Loss: 1.702 | Train Acc: 88.12% | Val Acc: 85.70%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 85.70%)\n",
      "Epoch 12/150 | Loss: 1.634 | Train Acc: 88.47% | Val Acc: 85.57%\n",
      "Epoch 13/150 | Loss: 1.593 | Train Acc: 88.80% | Val Acc: 85.83%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 85.83%)\n",
      "Epoch 14/150 | Loss: 1.539 | Train Acc: 89.24% | Val Acc: 84.41%\n",
      "Epoch 15/150 | Loss: 1.519 | Train Acc: 89.28% | Val Acc: 86.23%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 86.23%)\n",
      "Epoch 16/150 | Loss: 1.446 | Train Acc: 90.08% | Val Acc: 87.47%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 87.47%)\n",
      "Epoch 17/150 | Loss: 1.413 | Train Acc: 90.17% | Val Acc: 86.16%\n",
      "Epoch 18/150 | Loss: 1.346 | Train Acc: 90.65% | Val Acc: 85.80%\n",
      "Epoch 19/150 | Loss: 1.329 | Train Acc: 90.85% | Val Acc: 86.08%\n",
      "Epoch 20/150 | Loss: 1.308 | Train Acc: 91.03% | Val Acc: 86.37%\n",
      "Epoch 21/150 | Loss: 1.255 | Train Acc: 91.33% | Val Acc: 87.39%\n",
      "Epoch 22/150 | Loss: 1.236 | Train Acc: 91.54% | Val Acc: 87.33%\n",
      "Epoch 23/150 | Loss: 1.200 | Train Acc: 91.80% | Val Acc: 86.10%\n",
      "Epoch 24/150 | Loss: 1.180 | Train Acc: 91.88% | Val Acc: 87.18%\n",
      "Epoch 25/150 | Loss: 1.146 | Train Acc: 92.15% | Val Acc: 87.53%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 87.53%)\n",
      "Epoch 26/150 | Loss: 1.126 | Train Acc: 92.19% | Val Acc: 87.27%\n",
      "Epoch 27/150 | Loss: 1.093 | Train Acc: 92.46% | Val Acc: 88.07%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 88.07%)\n",
      "Epoch 28/150 | Loss: 1.065 | Train Acc: 92.57% | Val Acc: 87.46%\n",
      "Epoch 29/150 | Loss: 1.052 | Train Acc: 92.77% | Val Acc: 88.21%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 88.21%)\n",
      "Epoch 30/150 | Loss: 1.033 | Train Acc: 92.98% | Val Acc: 88.50%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 88.50%)\n",
      "Epoch 31/150 | Loss: 1.034 | Train Acc: 92.80% | Val Acc: 88.31%\n",
      "Epoch 32/150 | Loss: 0.990 | Train Acc: 93.34% | Val Acc: 88.15%\n",
      "Epoch 33/150 | Loss: 0.966 | Train Acc: 93.33% | Val Acc: 86.05%\n",
      "Epoch 34/150 | Loss: 0.952 | Train Acc: 93.50% | Val Acc: 88.37%\n",
      "Epoch 35/150 | Loss: 0.934 | Train Acc: 93.68% | Val Acc: 87.28%\n",
      "Epoch 36/150 | Loss: 0.924 | Train Acc: 93.76% | Val Acc: 88.22%\n",
      "Epoch 37/150 | Loss: 0.910 | Train Acc: 93.74% | Val Acc: 88.10%\n",
      "Epoch 38/150 | Loss: 0.893 | Train Acc: 93.94% | Val Acc: 86.86%\n",
      "Epoch 39/150 | Loss: 0.885 | Train Acc: 93.90% | Val Acc: 88.25%\n",
      "Epoch 40/150 | Loss: 0.857 | Train Acc: 94.16% | Val Acc: 88.96%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 88.96%)\n",
      "Epoch 41/150 | Loss: 0.840 | Train Acc: 94.35% | Val Acc: 87.43%\n",
      "Epoch 42/150 | Loss: 0.829 | Train Acc: 94.37% | Val Acc: 87.30%\n",
      "Epoch 43/150 | Loss: 0.818 | Train Acc: 94.36% | Val Acc: 88.63%\n",
      "Epoch 44/150 | Loss: 0.776 | Train Acc: 94.75% | Val Acc: 88.48%\n",
      "Epoch 45/150 | Loss: 0.784 | Train Acc: 94.77% | Val Acc: 88.66%\n",
      "Epoch 46/150 | Loss: 0.780 | Train Acc: 94.67% | Val Acc: 87.75%\n",
      "Epoch 47/150 | Loss: 0.749 | Train Acc: 94.89% | Val Acc: 88.95%\n",
      "Epoch 48/150 | Loss: 0.754 | Train Acc: 94.90% | Val Acc: 89.48%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 89.48%)\n",
      "Epoch 49/150 | Loss: 0.734 | Train Acc: 95.08% | Val Acc: 88.77%\n",
      "Epoch 50/150 | Loss: 0.721 | Train Acc: 95.16% | Val Acc: 88.79%\n",
      "Epoch 51/150 | Loss: 0.719 | Train Acc: 95.13% | Val Acc: 89.30%\n",
      "Epoch 52/150 | Loss: 0.675 | Train Acc: 95.55% | Val Acc: 88.50%\n",
      "Epoch 53/150 | Loss: 0.680 | Train Acc: 95.46% | Val Acc: 89.17%\n",
      "Epoch 54/150 | Loss: 0.671 | Train Acc: 95.58% | Val Acc: 88.00%\n",
      "Epoch 55/150 | Loss: 0.662 | Train Acc: 95.56% | Val Acc: 89.18%\n",
      "Epoch 56/150 | Loss: 0.654 | Train Acc: 95.59% | Val Acc: 88.39%\n",
      "Epoch 57/150 | Loss: 0.618 | Train Acc: 95.91% | Val Acc: 89.02%\n",
      "Epoch 58/150 | Loss: 0.621 | Train Acc: 95.80% | Val Acc: 88.82%\n",
      "Epoch 59/150 | Loss: 0.613 | Train Acc: 95.93% | Val Acc: 88.86%\n",
      "Epoch 60/150 | Loss: 0.602 | Train Acc: 95.96% | Val Acc: 89.11%\n",
      "Epoch 61/150 | Loss: 0.586 | Train Acc: 96.08% | Val Acc: 88.64%\n",
      "Epoch 62/150 | Loss: 0.580 | Train Acc: 96.09% | Val Acc: 88.95%\n",
      "Epoch 63/150 | Loss: 0.555 | Train Acc: 96.38% | Val Acc: 89.09%\n",
      "Epoch 64/150 | Loss: 0.546 | Train Acc: 96.34% | Val Acc: 89.58%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 89.58%)\n",
      "Epoch 65/150 | Loss: 0.539 | Train Acc: 96.45% | Val Acc: 89.31%\n",
      "Epoch 66/150 | Loss: 0.519 | Train Acc: 96.54% | Val Acc: 89.62%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 89.62%)\n",
      "Epoch 67/150 | Loss: 0.518 | Train Acc: 96.62% | Val Acc: 89.98%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 89.98%)\n",
      "Epoch 68/150 | Loss: 0.499 | Train Acc: 96.73% | Val Acc: 89.40%\n",
      "Epoch 69/150 | Loss: 0.498 | Train Acc: 96.74% | Val Acc: 89.48%\n",
      "Epoch 70/150 | Loss: 0.484 | Train Acc: 96.82% | Val Acc: 89.78%\n",
      "Epoch 71/150 | Loss: 0.479 | Train Acc: 96.86% | Val Acc: 89.27%\n",
      "Epoch 72/150 | Loss: 0.445 | Train Acc: 97.16% | Val Acc: 90.01%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.01%)\n",
      "Epoch 73/150 | Loss: 0.455 | Train Acc: 97.03% | Val Acc: 89.30%\n",
      "Epoch 74/150 | Loss: 0.453 | Train Acc: 97.08% | Val Acc: 89.65%\n",
      "Epoch 75/150 | Loss: 0.429 | Train Acc: 97.22% | Val Acc: 89.69%\n",
      "Epoch 76/150 | Loss: 0.415 | Train Acc: 97.32% | Val Acc: 89.60%\n",
      "Epoch 77/150 | Loss: 0.413 | Train Acc: 97.30% | Val Acc: 89.37%\n",
      "Epoch 78/150 | Loss: 0.399 | Train Acc: 97.50% | Val Acc: 89.47%\n",
      "Epoch 79/150 | Loss: 0.399 | Train Acc: 97.44% | Val Acc: 89.44%\n",
      "Epoch 80/150 | Loss: 0.386 | Train Acc: 97.54% | Val Acc: 89.90%\n",
      "Epoch 81/150 | Loss: 0.366 | Train Acc: 97.65% | Val Acc: 89.85%\n",
      "Epoch 82/150 | Loss: 0.357 | Train Acc: 97.77% | Val Acc: 89.76%\n",
      "Epoch 83/150 | Loss: 0.366 | Train Acc: 97.68% | Val Acc: 90.41%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.41%)\n",
      "Epoch 84/150 | Loss: 0.341 | Train Acc: 98.00% | Val Acc: 90.05%\n",
      "Epoch 85/150 | Loss: 0.343 | Train Acc: 97.83% | Val Acc: 90.09%\n",
      "Epoch 86/150 | Loss: 0.333 | Train Acc: 97.92% | Val Acc: 89.74%\n",
      "Epoch 87/150 | Loss: 0.320 | Train Acc: 98.05% | Val Acc: 90.59%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.59%)\n",
      "Epoch 88/150 | Loss: 0.314 | Train Acc: 98.13% | Val Acc: 90.00%\n",
      "Epoch 89/150 | Loss: 0.293 | Train Acc: 98.23% | Val Acc: 90.07%\n",
      "Epoch 90/150 | Loss: 0.293 | Train Acc: 98.26% | Val Acc: 90.30%\n",
      "Epoch 91/150 | Loss: 0.281 | Train Acc: 98.36% | Val Acc: 89.97%\n",
      "Epoch 92/150 | Loss: 0.280 | Train Acc: 98.35% | Val Acc: 90.51%\n",
      "Epoch 93/150 | Loss: 0.251 | Train Acc: 98.57% | Val Acc: 90.37%\n",
      "Epoch 94/150 | Loss: 0.263 | Train Acc: 98.48% | Val Acc: 90.12%\n",
      "Epoch 95/150 | Loss: 0.244 | Train Acc: 98.66% | Val Acc: 90.39%\n",
      "Epoch 96/150 | Loss: 0.241 | Train Acc: 98.63% | Val Acc: 90.64%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.64%)\n",
      "Epoch 97/150 | Loss: 0.241 | Train Acc: 98.60% | Val Acc: 90.56%\n",
      "Epoch 98/150 | Loss: 0.222 | Train Acc: 98.78% | Val Acc: 90.85%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.85%)\n",
      "Epoch 99/150 | Loss: 0.226 | Train Acc: 98.77% | Val Acc: 90.73%\n",
      "Epoch 100/150 | Loss: 0.209 | Train Acc: 98.86% | Val Acc: 90.58%\n",
      "Epoch 101/150 | Loss: 0.208 | Train Acc: 98.94% | Val Acc: 90.93%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.93%)\n",
      "Epoch 102/150 | Loss: 0.202 | Train Acc: 98.96% | Val Acc: 90.84%\n",
      "Epoch 103/150 | Loss: 0.189 | Train Acc: 99.03% | Val Acc: 90.66%\n",
      "Epoch 104/150 | Loss: 0.186 | Train Acc: 99.07% | Val Acc: 90.82%\n",
      "Epoch 105/150 | Loss: 0.178 | Train Acc: 99.18% | Val Acc: 91.04%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.04%)\n",
      "Epoch 106/150 | Loss: 0.172 | Train Acc: 99.16% | Val Acc: 90.72%\n",
      "Epoch 107/150 | Loss: 0.164 | Train Acc: 99.22% | Val Acc: 90.76%\n",
      "Epoch 108/150 | Loss: 0.157 | Train Acc: 99.29% | Val Acc: 90.95%\n",
      "Epoch 109/150 | Loss: 0.156 | Train Acc: 99.29% | Val Acc: 91.04%\n",
      "Epoch 110/150 | Loss: 0.153 | Train Acc: 99.32% | Val Acc: 91.32%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.32%)\n",
      "Epoch 111/150 | Loss: 0.143 | Train Acc: 99.41% | Val Acc: 91.09%\n",
      "Epoch 112/150 | Loss: 0.139 | Train Acc: 99.42% | Val Acc: 90.98%\n",
      "Epoch 113/150 | Loss: 0.139 | Train Acc: 99.41% | Val Acc: 91.56%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.56%)\n",
      "Epoch 114/150 | Loss: 0.127 | Train Acc: 99.52% | Val Acc: 91.17%\n",
      "Epoch 115/150 | Loss: 0.130 | Train Acc: 99.48% | Val Acc: 91.47%\n",
      "Epoch 116/150 | Loss: 0.129 | Train Acc: 99.51% | Val Acc: 91.27%\n",
      "Epoch 117/150 | Loss: 0.120 | Train Acc: 99.57% | Val Acc: 91.41%\n",
      "Epoch 118/150 | Loss: 0.117 | Train Acc: 99.59% | Val Acc: 91.17%\n",
      "Epoch 119/150 | Loss: 0.111 | Train Acc: 99.62% | Val Acc: 91.27%\n",
      "Epoch 120/150 | Loss: 0.106 | Train Acc: 99.70% | Val Acc: 91.46%\n",
      "Epoch 121/150 | Loss: 0.104 | Train Acc: 99.69% | Val Acc: 91.24%\n",
      "Epoch 122/150 | Loss: 0.106 | Train Acc: 99.67% | Val Acc: 91.49%\n",
      "Epoch 123/150 | Loss: 0.102 | Train Acc: 99.70% | Val Acc: 91.31%\n",
      "Epoch 124/150 | Loss: 0.098 | Train Acc: 99.73% | Val Acc: 91.57%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.57%)\n",
      "Epoch 125/150 | Loss: 0.096 | Train Acc: 99.77% | Val Acc: 91.58%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.58%)\n",
      "Epoch 126/150 | Loss: 0.095 | Train Acc: 99.75% | Val Acc: 91.65%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.65%)\n",
      "Epoch 127/150 | Loss: 0.092 | Train Acc: 99.77% | Val Acc: 91.49%\n",
      "Epoch 128/150 | Loss: 0.093 | Train Acc: 99.76% | Val Acc: 91.72%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.72%)\n",
      "Epoch 129/150 | Loss: 0.089 | Train Acc: 99.80% | Val Acc: 91.72%\n",
      "Epoch 130/150 | Loss: 0.093 | Train Acc: 99.75% | Val Acc: 91.66%\n",
      "Epoch 131/150 | Loss: 0.089 | Train Acc: 99.80% | Val Acc: 91.58%\n",
      "Epoch 132/150 | Loss: 0.088 | Train Acc: 99.79% | Val Acc: 91.69%\n",
      "Epoch 133/150 | Loss: 0.083 | Train Acc: 99.85% | Val Acc: 91.94%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.94%)\n",
      "Epoch 134/150 | Loss: 0.082 | Train Acc: 99.84% | Val Acc: 91.90%\n",
      "Epoch 135/150 | Loss: 0.078 | Train Acc: 99.87% | Val Acc: 91.74%\n",
      "Epoch 136/150 | Loss: 0.083 | Train Acc: 99.83% | Val Acc: 91.69%\n",
      "Epoch 137/150 | Loss: 0.082 | Train Acc: 99.85% | Val Acc: 91.79%\n",
      "Epoch 138/150 | Loss: 0.084 | Train Acc: 99.84% | Val Acc: 91.81%\n",
      "Epoch 139/150 | Loss: 0.083 | Train Acc: 99.85% | Val Acc: 91.83%\n",
      "Epoch 140/150 | Loss: 0.078 | Train Acc: 99.89% | Val Acc: 91.72%\n",
      "Epoch 141/150 | Loss: 0.080 | Train Acc: 99.85% | Val Acc: 91.79%\n",
      "Epoch 142/150 | Loss: 0.080 | Train Acc: 99.87% | Val Acc: 91.87%\n",
      "Epoch 143/150 | Loss: 0.084 | Train Acc: 99.83% | Val Acc: 91.69%\n",
      "Epoch 144/150 | Loss: 0.079 | Train Acc: 99.87% | Val Acc: 91.82%\n",
      "Epoch 145/150 | Loss: 0.079 | Train Acc: 99.86% | Val Acc: 91.83%\n",
      "Epoch 146/150 | Loss: 0.080 | Train Acc: 99.86% | Val Acc: 91.83%\n",
      "Epoch 147/150 | Loss: 0.077 | Train Acc: 99.88% | Val Acc: 91.89%\n",
      "Epoch 148/150 | Loss: 0.076 | Train Acc: 99.89% | Val Acc: 91.93%\n",
      "Epoch 149/150 | Loss: 0.083 | Train Acc: 99.83% | Val Acc: 91.91%\n",
      "Epoch 150/150 | Loss: 0.081 | Train Acc: 99.86% | Val Acc: 91.87%\n",
      "\n",
      "ðŸŽ‰ HOÃ€N Táº¤T DISTILLATION! Káº¿t quáº£ tá»‘t nháº¥t: 91.94%\n",
      "File saved at: ./checkpoint/pruned_student_distilled.pth\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: FINE-TUNING Vá»šI KNOWLEDGE DISTILLATION (KD)\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# --- 1. Cáº¥u hÃ¬nh KD ---\n",
    "# Hyper-parameters\n",
    "KD_EPOCHS = 150           # Sá»‘ epoch train láº¡i (nÃªn train ká»¹)\n",
    "KD_LR = 0.01              # Learning rate (tháº¥p hÆ¡n train from scratch)\n",
    "KD_TEMP = 4.0             # Temperature (Ä‘á»™ má»m cá»§a xÃ¡c suáº¥t)\n",
    "KD_ALPHA = 0.9            # 90% há»c tá»« Teacher, 10% há»c tá»« Label tháº­t\n",
    "TEACHER_PATH = \"./checkpoint/vgg16_cifar10_baseline.pth\" # ÄÆ°á»ng dáº«n model gá»‘c\n",
    "# TEACHER_PATH = \"./checkpoint/vgg16_cifar100_baseline.pth\" # ÄÆ°á»ng dáº«n model gá»‘c\n",
    "\n",
    "# HÃ m Loss Distillation\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=4.0, alpha=0.9):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.T = temperature\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Loss 1: Hard Loss (Cross Entropy vá»›i nhÃ£n tháº­t)\n",
    "        hard_loss = F.cross_entropy(student_logits, labels)\n",
    "        \n",
    "        # Loss 2: Soft Loss (KL Divergence vá»›i Teacher)\n",
    "        soft_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits / self.T, dim=1),\n",
    "            F.softmax(teacher_logits / self.T, dim=1),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.T * self.T)\n",
    "        \n",
    "        return self.alpha * soft_loss + (1. - self.alpha) * hard_loss\n",
    "\n",
    "# --- 2. Chuáº©n bá»‹ Teacher & Student ---\n",
    "print(\"ðŸš€ Äang khá»Ÿi táº¡o Knowledge Distillation...\")\n",
    "\n",
    "# A. Load Teacher (MÃ´ hÃ¬nh gá»‘c chÆ°a cáº¯t)\n",
    "if not os.path.exists(TEACHER_PATH):\n",
    "    raise FileNotFoundError(f\"âŒ KhÃ´ng tÃ¬m tháº¥y file checkpoint Teacher táº¡i: {TEACHER_PATH}\")\n",
    "\n",
    "teacher_model = ModifiedVGG16Model(config).to(config.device)\n",
    "ckpt_t = torch.load(TEACHER_PATH, map_location=config.device)\n",
    "teacher_model.load_state_dict(ckpt_t['model'])\n",
    "teacher_model.eval() # Teacher chá»‰ dáº¡y, khÃ´ng há»c\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False # ÄÃ³ng bÄƒng Teacher\n",
    "\n",
    "print(f\"âœ… Teacher loaded (Acc: {ckpt_t.get('acc', 'N/A')}%)\")\n",
    "\n",
    "# B. Student (LÃ  model Ä‘ang náº±m trong bá»™ nhá»› sau khi cháº¡y Cell 3)\n",
    "# LÆ°u Ã½: Biáº¿n 'model' á»Ÿ Ä‘Ã¢y lÃ  model Ä‘Ã£ bá»‹ cáº¯t tá»‰a váº­t lÃ½ á»Ÿ cell trÆ°á»›c\n",
    "student_model = copy.deepcopy(final_pruned_model).to(config.device) \n",
    "student_model.train()\n",
    "print(\"âœ… Student (Pruned Model) ready.\")\n",
    "\n",
    "# --- 3. Thiáº¿t láº­p Training ---\n",
    "criterion_kd = DistillationLoss(temperature=KD_TEMP, alpha=KD_ALPHA)\n",
    "optimizer_kd = optim.SGD(student_model.parameters(), lr=KD_LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_kd = optim.lr_scheduler.CosineAnnealingLR(optimizer_kd, T_max=KD_EPOCHS)\n",
    "\n",
    "# Táº­n dá»¥ng Data Loader cÃ³ sáºµn tá»« old_pruner\n",
    "train_loader = old_pruner.train_loader\n",
    "test_loader = old_pruner.test_loader\n",
    "\n",
    "# --- 4. VÃ²ng láº·p Training KD ---\n",
    "best_acc_kd = 0\n",
    "save_name_kd = 'pruned_student_distilled.pth'\n",
    "# save_name_kd = 'pruned_student_distilled_100.pth'\n",
    "\n",
    "print(f\"\\nðŸ”¥ Báº¯t Ä‘áº§u Training KD trong {KD_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student_model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
    "        \n",
    "        optimizer_kd.zero_grad()\n",
    "        \n",
    "        # Forward Teacher (No grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(inputs)\n",
    "            \n",
    "        # Forward Student\n",
    "        student_logits = student_model(inputs)\n",
    "        \n",
    "        # TÃ­nh Loss tá»•ng há»£p\n",
    "        loss = criterion_kd(student_logits, teacher_logits, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Káº¹p gradient Ä‘á»ƒ trÃ¡nh lá»—i exploding (quan trá»ng cho máº¡ng Ä‘Ã£ cáº¯t)\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=5.0)\n",
    "        optimizer_kd.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = student_logits.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    # Update LR\n",
    "    scheduler_kd.step()\n",
    "    \n",
    "    # --- Test Student ---\n",
    "    student_model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
    "            outputs = student_model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    acc_test = 100. * test_correct / test_total\n",
    "    \n",
    "    # Log gá»n gÃ ng\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} | Loss: {train_loss/(batch_idx+1):.3f} | Train Acc: {100.*correct/total:.2f}% | Val Acc: {acc_test:.2f}%\")\n",
    "    \n",
    "    # LÆ°u Best Checkpoint\n",
    "    if acc_test > best_acc_kd:\n",
    "        best_acc_kd = acc_test\n",
    "        print(f\"   ---> ðŸ’¾ Saving Best Student (Acc: {best_acc_kd:.2f}%)\")\n",
    "        torch.save({\n",
    "            'model': student_model.state_dict(),\n",
    "            'acc': best_acc_kd,\n",
    "            'epoch': epoch\n",
    "        }, f'./checkpoint/{save_name_kd}')\n",
    "\n",
    "print(f\"\\nðŸŽ‰ HOÃ€N Táº¤T DISTILLATION! Káº¿t quáº£ tá»‘t nháº¥t: {best_acc_kd:.2f}%\")\n",
    "print(f\"File saved at: ./checkpoint/{save_name_kd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a06eb1",
   "metadata": {},
   "source": [
    "## 6. NSGAPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55dcab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#CELL 1: NSGA-II PRUNER (CORE)\n",
    "#==============================================================================\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class NSGASolution:\n",
    "    chrom: list\n",
    "    acc: float = None\n",
    "    macs: float = None\n",
    "    params: float = None\n",
    "    rank: int = None\n",
    "    crowding: float = 0.0\n",
    "\n",
    "\n",
    "class NSGAPruner:\n",
    "    def __init__(self, base_model, ga_pruner,\n",
    "                 pop_size=40, generations=30,\n",
    "                 min_ratio=0.1):\n",
    "        self.base_model = base_model\n",
    "        self.ga_pruner = ga_pruner\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.min_ratio = min_ratio\n",
    "\n",
    "        self.n_layers = len(ga_pruner.conv_layers)\n",
    "        self.population = []\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Chromosome: [ratio_0, strategy_0, ..., ratio_L, strategy_L]\n",
    "    # --------------------------------------------------\n",
    "    def random_chrom(self):\n",
    "        chrom = []\n",
    "        for _ in range(self.n_layers):\n",
    "            chrom.append(random.uniform(self.min_ratio, 1.0))  # ratio\n",
    "            chrom.append(random.random())                       # strategy gene\n",
    "        return chrom\n",
    "\n",
    "    def init_population(self):\n",
    "        self.population = [\n",
    "            NSGASolution(self.random_chrom())\n",
    "            for _ in range(self.pop_size)\n",
    "        ]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Fitness evaluation (PROXY â€“ GA level)\n",
    "    # --------------------------------------------------\n",
    "    def evaluate(self, sol):\n",
    "        channels, strategies = self.ga_pruner._decode_chromosome(sol.chrom)\n",
    "\n",
    "        macs, params = self.ga_pruner._estimate_metrics(channels)\n",
    "        proxy_acc = self.ga_pruner.proxy_capacity_score(channels, strategies)\n",
    "\n",
    "        sol.acc = proxy_acc\n",
    "        sol.macs = macs\n",
    "        sol.params = params\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # NSGA-II: Non-dominated sorting\n",
    "    # --------------------------------------------------\n",
    "    def dominates(self, a, b):\n",
    "        return (\n",
    "            (a.acc >= b.acc and a.macs <= b.macs and a.params <= b.params) and\n",
    "            (a.acc > b.acc or a.macs < b.macs or a.params < b.params)\n",
    "        )\n",
    "\n",
    "    def fast_nondominated_sort(self):\n",
    "        fronts = [[]]\n",
    "        for p in self.population:\n",
    "            p.dom_count = 0\n",
    "            p.dom_set = []\n",
    "            for q in self.population:\n",
    "                if self.dominates(p, q):\n",
    "                    p.dom_set.append(q)\n",
    "                elif self.dominates(q, p):\n",
    "                    p.dom_count += 1\n",
    "            if p.dom_count == 0:\n",
    "                p.rank = 0\n",
    "                fronts[0].append(p)\n",
    "\n",
    "        i = 0\n",
    "        while fronts[i]:\n",
    "            next_front = []\n",
    "            for p in fronts[i]:\n",
    "                for q in p.dom_set:\n",
    "                    q.dom_count -= 1\n",
    "                    if q.dom_count == 0:\n",
    "                        q.rank = i + 1\n",
    "                        next_front.append(q)\n",
    "            i += 1\n",
    "            fronts.append(next_front)\n",
    "        return fronts[:-1]\n",
    "\n",
    "    def crowding_distance(self, front):\n",
    "        if not front:\n",
    "            return\n",
    "        for s in front:\n",
    "            s.crowding = 0\n",
    "\n",
    "        for key in ['acc', 'macs', 'params']:\n",
    "            front.sort(key=lambda x: getattr(x, key))\n",
    "            front[0].crowding = front[-1].crowding = float('inf')\n",
    "            min_v = getattr(front[0], key)\n",
    "            max_v = getattr(front[-1], key)\n",
    "            if max_v == min_v:\n",
    "                continue\n",
    "            for i in range(1, len(front)-1):\n",
    "                prev_v = getattr(front[i-1], key)\n",
    "                next_v = getattr(front[i+1], key)\n",
    "                front[i].crowding += (next_v - prev_v) / (max_v - min_v)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def select_next_population(self, fronts):\n",
    "        new_pop = []\n",
    "        for f in fronts:\n",
    "            self.crowding_distance(f)\n",
    "            if len(new_pop) + len(f) <= self.pop_size:\n",
    "                new_pop.extend(f)\n",
    "            else:\n",
    "                f.sort(key=lambda x: -x.crowding)\n",
    "                new_pop.extend(f[:self.pop_size - len(new_pop)])\n",
    "                break\n",
    "        self.population = new_pop\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def crossover(self, p1, p2):\n",
    "        cut = random.randint(1, len(p1.chrom)-2)\n",
    "        c1 = p1.chrom[:cut] + p2.chrom[cut:]\n",
    "        c2 = p2.chrom[:cut] + p1.chrom[cut:]\n",
    "        return NSGASolution(c1), NSGASolution(c2)\n",
    "\n",
    "    def mutate(self, sol, prob=0.1):\n",
    "        for i in range(len(sol.chrom)):\n",
    "            if random.random() < prob:\n",
    "                if i % 2 == 0:\n",
    "                    sol.chrom[i] = random.uniform(self.min_ratio, 1.0)\n",
    "                else:\n",
    "                    sol.chrom[i] = random.random()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def run(self):\n",
    "        self.init_population()\n",
    "\n",
    "        for g in range(self.generations):\n",
    "            for s in self.population:\n",
    "                self.evaluate(s)\n",
    "\n",
    "            fronts = self.fast_nondominated_sort()\n",
    "            self.select_next_population(fronts)\n",
    "\n",
    "            offspring = []\n",
    "            while len(offspring) < self.pop_size:\n",
    "                p1, p2 = random.sample(self.population, 2)\n",
    "                c1, c2 = self.crossover(p1, p2)\n",
    "                self.mutate(c1)\n",
    "                self.mutate(c2)\n",
    "                offspring.extend([c1, c2])\n",
    "\n",
    "            self.population = offspring[:self.pop_size]\n",
    "\n",
    "        # Final eval\n",
    "        for s in self.population:\n",
    "            self.evaluate(s)\n",
    "\n",
    "    def get_pareto_front(self):\n",
    "        fronts = self.fast_nondominated_sort()\n",
    "        return fronts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba171e4",
   "metadata": {},
   "source": [
    "### 6.1. NSGA executtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c55b66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model from epoch 183 with Acc: 93.62%\n",
      "Base MACs: 0.3334G | Base Params: 33.6467M\n",
      "Analyzed 13 Conv layers for fast estimation.\n",
      "\n",
      "Step 1: Building Ranking Tables (Pre-computation)...\n",
      "   Running ranking epoch (accumulating gradients)...\n",
      "\n",
      "ðŸ“Š --- SMART MATCHING LAYERS ---\n",
      "   Raw ranks key type: <class 'int'>\n",
      "   Layer 12: max=0.0732, mean=0.0438\n",
      "   Layer 11: max=0.1623, mean=0.0343\n",
      "   Layer 2: max=0.3049, mean=0.0709\n",
      "   Layer 1: max=0.3527, mean=0.0888\n",
      "   Layer 0: max=0.4344, mean=0.0783\n",
      "âœ… Successfully built ranking tables for 13 layers.\n",
      "âœ… Pareto size: 27\n",
      "ProxyAcc=0.7825 | MACs=0.085G | Params=5.03M\n",
      "ProxyAcc=0.8574 | MACs=0.101G | Params=5.27M\n",
      "ProxyAcc=0.8727 | MACs=0.114G | Params=4.91M\n",
      "ProxyAcc=0.7957 | MACs=0.101G | Params=2.94M\n",
      "ProxyAcc=0.9152 | MACs=0.138G | Params=8.05M\n",
      "ProxyAcc=0.6660 | MACs=0.062G | Params=2.07M\n",
      "ProxyAcc=0.7907 | MACs=0.088G | Params=3.86M\n",
      "ProxyAcc=0.8226 | MACs=0.106G | Params=3.43M\n",
      "ProxyAcc=0.7338 | MACs=0.084G | Params=2.23M\n",
      "ProxyAcc=0.7390 | MACs=0.071G | Params=3.16M\n",
      "ProxyAcc=0.7193 | MACs=0.049G | Params=3.32M\n",
      "ProxyAcc=0.8097 | MACs=0.100G | Params=4.35M\n",
      "ProxyAcc=0.6157 | MACs=0.049G | Params=1.11M\n",
      "ProxyAcc=0.7391 | MACs=0.079G | Params=3.30M\n",
      "ProxyAcc=0.7650 | MACs=0.071G | Params=3.30M\n",
      "ProxyAcc=0.6968 | MACs=0.062G | Params=1.93M\n",
      "ProxyAcc=0.7178 | MACs=0.073G | Params=2.95M\n",
      "ProxyAcc=0.8930 | MACs=0.135G | Params=6.01M\n",
      "ProxyAcc=0.7704 | MACs=0.082G | Params=2.82M\n",
      "ProxyAcc=0.8403 | MACs=0.137G | Params=4.81M\n",
      "ProxyAcc=0.6805 | MACs=0.053G | Params=3.26M\n",
      "ProxyAcc=0.8280 | MACs=0.114G | Params=4.04M\n",
      "ProxyAcc=0.8249 | MACs=0.106G | Params=4.81M\n",
      "ProxyAcc=0.8087 | MACs=0.088G | Params=3.87M\n",
      "ProxyAcc=0.7716 | MACs=0.071G | Params=4.86M\n",
      "ProxyAcc=0.7579 | MACs=0.099G | Params=2.63M\n",
      "ProxyAcc=0.7884 | MACs=0.081G | Params=5.52M\n"
     ]
    }
   ],
   "source": [
    "SEED = 2026\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 1. Cáº¥u hÃ¬nh & Load Model\n",
    "config = Config()\n",
    "config.dataset_class = 'CIFAR10'\n",
    "# config.dataset_class = 'CIFAR100'\n",
    "config.classifier_type = 'B' # Hoáº·c loáº¡i báº¡n Ä‘Ã£ train\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Táº¡o model vÃ  load weight cÅ©\n",
    "model = ModifiedVGG16Model(config).to(config.device)\n",
    "checkpoint = torch.load(\"./checkpoint/vgg16_cifar10_baseline.pth\", map_location=config.device) # ÄÆ°á»ng dáº«n file ckpt cá»§a báº¡n\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "print(f\"âœ… Loaded model from epoch {checkpoint['epoch']} with Acc: {checkpoint['acc']}%\")\n",
    "\n",
    "# 2. Khá»Ÿi táº¡o Helper cÅ© (Ä‘á»ƒ láº¥y dá»¯ liá»‡u train tÃ­nh rank)\n",
    "# Helper nÃ y chá»‹u trÃ¡ch nhiá»‡m tÃ­nh Taylor Score\n",
    "old_pruner = VGG16Pruner(config, model, save_name='10_ga_optimise_ckpt.pth') \n",
    "\n",
    "# 3. Khá»Ÿi táº¡o GA Pruner (Code má»›i)\n",
    "input_size = torch.randn(1, 3, 32, 32).to(config.device)\n",
    "ga_pruner = GAPruner(model, config, input_size)\n",
    "\n",
    "ga_pruner.strategy_mode = 'layer_wise'\n",
    "\n",
    "# 4. XÃ¢y dá»±ng báº£ng tra cá»©u (BÆ°á»›c nÃ y cháº¡y ~1 epoch Ä‘á»ƒ tÃ­nh rank, máº¥t khoáº£ng 1-2 phÃºt)\n",
    "ga_pruner.build_ranking_tables(old_pruner, old_pruner.train_loader)\n",
    "\n",
    "nsga = NSGAPruner(\n",
    "    base_model=model,\n",
    "    ga_pruner=ga_pruner,\n",
    "    pop_size=50,\n",
    "    generations=30\n",
    ")\n",
    "\n",
    "nsga.run()\n",
    "pareto = nsga.get_pareto_front()\n",
    "\n",
    "print(f\"âœ… Pareto size: {len(pareto)}\")\n",
    "for s in pareto:\n",
    "    print(\n",
    "        f\"ProxyAcc={s.acc:.4f} | \"\n",
    "        f\"MACs={s.macs/1e9:.3f}G | \"\n",
    "        f\"Params={s.params/1e6:.2f}M\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61e1fd",
   "metadata": {},
   "source": [
    "#### Plot Pareto Front + Find Knee Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb04189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ Knee: MACs=0.071G | Params=3.30M | Accâ‰ˆ0.76%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "macs = np.array([s.macs / 1e9 for s in pareto])\n",
    "accs = np.array([s.acc for s in pareto])\n",
    "params = np.array([s.params / 1e6 for s in pareto])\n",
    "\n",
    "# Knee point calculation (3 objectives)\n",
    "m_norm = (macs - macs.min()) / (macs.max() - macs.min())\n",
    "a_norm = (accs.max() - accs) / (accs.max() - accs.min())\n",
    "p_norm = (params - params.min()) / (params.max() - params.min())\n",
    "dist = np.sqrt(m_norm**2 + a_norm**2 + p_norm**2)\n",
    "# dist = np.sqrt(m_norm**2 + a_norm**2)\n",
    "knee_idx = np.argmin(dist)\n",
    "knee_sol = pareto[knee_idx]\n",
    "\n",
    "# ==========================================\n",
    "# OPTION B: 2D Plot vá»›i Params as Color\n",
    "# ==========================================\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Scatter vá»›i color = Params\n",
    "scatter = plt.scatter(macs, accs, c=params, cmap='viridis', s=80, \n",
    "                      edgecolors='black', linewidth=0.5, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Params (M)')\n",
    "\n",
    "# Highlight Knee Point\n",
    "plt.scatter(knee_sol.macs/1e9, knee_sol.acc, \n",
    "            c='red', s=250, marker='*', \n",
    "            edgecolors='black', linewidth=1.5, \n",
    "            zorder=10, label='Knee Point')\n",
    "\n",
    "# Annotation - mÅ©i tÃªn tá»« gÃ³c dÆ°á»›i bÃªn pháº£i chá»‰ lÃªn 45 Ä‘á»™\n",
    "plt.annotate(f'Knee Point\\n({knee_sol.macs/1e9:.3f}G, {knee_sol.acc:.2f}%)',\n",
    "             xy=(knee_sol.macs/1e9, knee_sol.acc),                    # Äiá»ƒm Knee\n",
    "             xytext=(knee_sol.macs/1e9 + 0.025, knee_sol.acc - 0.05), # Text á»Ÿ dÆ°á»›i-pháº£i\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=6),\n",
    "             fontsize=9, backgroundcolor='white',\n",
    "             ha='left', va='top')  # CÄƒn text trÃ¡i-trÃªn\n",
    "\n",
    "plt.xlabel(\"MACs (G)\", fontsize=12)\n",
    "plt.ylabel(\"Proxy Accuracy (%)\", fontsize=12)\n",
    "plt.title(\"Pareto Front with Params as Color (NSGA-II)\", fontsize=13)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# Save for paper\n",
    "plt.savefig('pareto_front_3obj.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"ðŸ Knee: MACs={knee_sol.macs/1e9:.3f}G | \"\n",
    "    f\"Params={knee_sol.params/1e6:.2f}M | \"\n",
    "    f\"Accâ‰ˆ{knee_sol.acc:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132a4e7",
   "metadata": {},
   "source": [
    "### 6.2. Physical pruning NSGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2be44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ‚ï¸ Applying GA-consistent Physical Pruning...\n",
      "\n",
      "âœ‚ï¸ Pruning Plan:\n",
      "Layer 0: Keep 32 (min)\n",
      "Layer 1: Keep 40 (avg)\n",
      "Layer 2: Keep 56 (avg)\n",
      "Layer 3: Keep 64 (min)\n",
      "Layer 4: Keep 40 (avg)\n",
      "Layer 5: Keep 208 (avg)\n",
      "Layer 6: Keep 88 (avg)\n",
      "Layer 7: Keep 272 (min)\n",
      "Layer 8: Keep 264 (min)\n",
      "Layer 9: Keep 216 (min)\n",
      "Layer 10: Keep 288 (avg)\n",
      "Layer 11: Keep 168 (min)\n",
      "Layer 12: Keep 400 (min)\n",
      "   GA-Layer 0 â†’ Model-Layer 0: Pruning 32/64 [min]\n",
      "   GA-Layer 1 â†’ Model-Layer 3: Pruning 24/64 [avg]\n",
      "   GA-Layer 2 â†’ Model-Layer 7: Pruning 72/128 [avg]\n",
      "   GA-Layer 3 â†’ Model-Layer 10: Pruning 64/128 [min]\n",
      "   GA-Layer 4 â†’ Model-Layer 14: Pruning 209/256 [avg]\n",
      "   GA-Layer 5 â†’ Model-Layer 17: Pruning 48/256 [avg]\n",
      "   GA-Layer 6 â†’ Model-Layer 20: Pruning 168/256 [avg]\n",
      "   GA-Layer 7 â†’ Model-Layer 24: Pruning 240/512 [min]\n",
      "   GA-Layer 8 â†’ Model-Layer 27: Pruning 248/512 [min]\n",
      "   GA-Layer 9 â†’ Model-Layer 30: Pruning 296/512 [min]\n",
      "   GA-Layer 10 â†’ Model-Layer 34: Pruning 224/512 [avg]\n",
      "   GA-Layer 11 â†’ Model-Layer 37: Pruning 344/512 [min]\n",
      "   GA-Layer 12 â†’ Model-Layer 40: Pruning 112/512 [min]\n",
      "\n",
      "âœ… Physical Pruning Finished.\n",
      "ðŸ“‰ Final Model Stats: MACs=0.092G | Params=21.791M\n",
      "ðŸ§® Expected MACs (NSGA-II): 0.071G | Actual MACs: 0.092G\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: PHYSICAL PRUNING (GA-CONSISTENT EXECUTION)\n",
    "# ==============================================================================\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "\n",
    "print(\"\\nâœ‚ï¸ Applying GA-consistent Physical Pruning...\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0. Chuáº©n bá»‹: snapshot model gá»‘c & ranking GLOBAL\n",
    "# ------------------------------------------------------------------\n",
    "base_model = copy.deepcopy(model).cpu()\n",
    "base_model.eval()\n",
    "best_chrom = knee_sol.chrom\n",
    "\n",
    "# Láº¤Y GLOBAL RANKING (Ä‘Ãºng vá»›i GA fitness)\n",
    "# KhÃ´ng bao giá» dÃ¹ng ranking sau prune\n",
    "global_layer_stats = ga_pruner.layer_stats        # chá»©a raw + norm\n",
    "global_ranking_tables = ga_pruner.ranking_tables  # min + avg (global ref)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Decode pruning plan tá»« GA\n",
    "# ------------------------------------------------------------------\n",
    "pruning_plan = ga_pruner.apply_best_pruning(best_chrom)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Khá»Ÿi táº¡o model má»›i Ä‘á»ƒ prune (NO CASCADE)\n",
    "# ------------------------------------------------------------------\n",
    "pruned_model = copy.deepcopy(base_model)\n",
    "\n",
    "# Helper: map GA-layer-idx â†’ real Conv2d idx trong model.features\n",
    "def map_ga_to_model_conv(model, ga_idx):\n",
    "    conv_count = 0\n",
    "    for idx, m in enumerate(model.features):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if conv_count == ga_idx:\n",
    "                return idx\n",
    "            conv_count += 1\n",
    "    return -1\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Prune tá»«ng layer Äá»˜C Láº¬P â€“ Ä‘Ãºng GA assumption\n",
    "# ------------------------------------------------------------------\n",
    "for item in pruning_plan:\n",
    "    layer_idx = item['layer_idx']\n",
    "    n_keep = item['n_keep']\n",
    "    strategy = item['strategy']\n",
    "\n",
    "    # Láº¥y ranking GLOBAL (model gá»‘c)\n",
    "    raw_scores = global_layer_stats[layer_idx]['raw']\n",
    "    total_filters = len(raw_scores)\n",
    "\n",
    "    n_prune = total_filters - n_keep\n",
    "    if n_prune <= 0:\n",
    "        continue\n",
    "\n",
    "    # Sort index theo Taylor score (weak â†’ strong)\n",
    "    sorted_idx = np.argsort(raw_scores)\n",
    "\n",
    "    # -------------------------------\n",
    "    # STRATEGY HANDLING (PAPER-LEVEL)\n",
    "    # -------------------------------\n",
    "    if strategy == 'min':\n",
    "        # Cáº¯t filter yáº¿u nháº¥t cá»§a layer\n",
    "        indices_to_prune = sorted_idx[:n_prune]\n",
    "\n",
    "    elif strategy == 'avg':\n",
    "        # GLOBAL MEDIAN STRATEGY\n",
    "        # Giá»¯ band trung vá»‹ cá»§a GLOBAL distribution\n",
    "        global_ref = global_ranking_tables['avg'][layer_idx]\n",
    "\n",
    "        g_low = np.percentile(global_ref, 25)\n",
    "        g_high = np.percentile(global_ref, 75)\n",
    "\n",
    "        # Prune filter quÃ¡ yáº¿u hoáº·c quÃ¡ máº¡nh\n",
    "        indices_to_prune = np.where(\n",
    "            (raw_scores < g_low) | (raw_scores > g_high)\n",
    "        )[0]\n",
    "\n",
    "        # Safety: náº¿u prune quÃ¡ tay â†’ fallback sang min\n",
    "        if len(indices_to_prune) > n_prune:\n",
    "            indices_to_prune = indices_to_prune[:n_prune]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    # Reverse sort Ä‘á»ƒ prune an toÃ n\n",
    "    indices_to_prune = sorted(indices_to_prune, reverse=True)\n",
    "\n",
    "    # Map sang layer thá»±c trong model\n",
    "    real_layer_idx = map_ga_to_model_conv(pruned_model, layer_idx)\n",
    "    assert real_layer_idx >= 0, f\"Cannot map GA layer {layer_idx}\"\n",
    "\n",
    "    print(\n",
    "        f\"   GA-Layer {layer_idx} â†’ Model-Layer {real_layer_idx}: \"\n",
    "        f\"Pruning {len(indices_to_prune)}/{total_filters} [{strategy}]\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # THá»°C HIá»†N PRUNING\n",
    "    # -------------------------------\n",
    "    for fidx in indices_to_prune:\n",
    "        prune_vgg16_conv_layer_BN(\n",
    "            pruned_model,\n",
    "            real_layer_idx,\n",
    "            fidx,\n",
    "            use_cuda=False\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Chuyá»ƒn model vá» device & kiá»ƒm tra\n",
    "# ------------------------------------------------------------------\n",
    "pruned_model = pruned_model.to(config.device)\n",
    "pruned_model.eval()\n",
    "\n",
    "final_macs, final_params = tp.utils.count_ops_and_params(\n",
    "    pruned_model,\n",
    "    input_size\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Physical Pruning Finished.\")\n",
    "print(\n",
    "    f\"ðŸ“‰ Final Model Stats: \"\n",
    "    f\"MACs={final_macs/1e9:.3f}G | Params={final_params/1e6:.3f}M\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Log Expected vs Actual (Ráº¤T QUAN TRá»ŒNG CHO PAPER)\n",
    "# ------------------------------------------------------------------\n",
    "expected_macs, expected_params = ga_pruner._estimate_metrics(\n",
    "    ga_pruner._decode_chromosome(best_chrom)[0]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"ðŸ§® Expected MACs (NSGA-II): {expected_macs/1e9:.3f}G | \"\n",
    "    f\"Actual MACs: {final_macs/1e9:.3f}G\"\n",
    ")\n",
    "\n",
    "# Snapshot chuáº©n sau pruning (KHÃ”NG Ä‘Æ°á»£c mutate)\n",
    "final_pruned_model = copy.deepcopy(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283e1b7",
   "metadata": {},
   "source": [
    "### 6.3. Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b8f19",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe42dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Fine-tuning NORMAL (from pruned snapshot)\n",
      "[NORMAL] Epoch 001 | Train 15.15% | Val 30.00%\n",
      "[NORMAL] Epoch 002 | Train 36.43% | Val 43.07%\n",
      "[NORMAL] Epoch 003 | Train 47.51% | Val 49.42%\n",
      "[NORMAL] Epoch 004 | Train 53.93% | Val 52.05%\n",
      "[NORMAL] Epoch 005 | Train 58.25% | Val 54.80%\n",
      "[NORMAL] Epoch 006 | Train 62.34% | Val 56.11%\n",
      "[NORMAL] Epoch 007 | Train 65.27% | Val 57.93%\n",
      "[NORMAL] Epoch 008 | Train 67.96% | Val 58.98%\n",
      "[NORMAL] Epoch 009 | Train 69.88% | Val 59.39%\n",
      "[NORMAL] Epoch 010 | Train 71.72% | Val 60.50%\n",
      "[NORMAL] Epoch 011 | Train 73.56% | Val 60.70%\n",
      "[NORMAL] Epoch 012 | Train 74.89% | Val 61.39%\n",
      "[NORMAL] Epoch 013 | Train 76.31% | Val 61.70%\n",
      "[NORMAL] Epoch 014 | Train 77.35% | Val 61.99%\n",
      "[NORMAL] Epoch 015 | Train 78.67% | Val 62.36%\n",
      "[NORMAL] Epoch 016 | Train 79.60% | Val 62.05%\n",
      "[NORMAL] Epoch 017 | Train 80.76% | Val 62.48%\n",
      "[NORMAL] Epoch 018 | Train 81.45% | Val 62.54%\n",
      "[NORMAL] Epoch 019 | Train 82.40% | Val 62.62%\n",
      "[NORMAL] Epoch 020 | Train 83.11% | Val 63.25%\n",
      "[NORMAL] Epoch 021 | Train 83.52% | Val 63.35%\n",
      "[NORMAL] Epoch 022 | Train 84.25% | Val 63.38%\n",
      "[NORMAL] Epoch 023 | Train 85.02% | Val 62.69%\n",
      "[NORMAL] Epoch 024 | Train 85.51% | Val 63.31%\n",
      "[NORMAL] Epoch 025 | Train 86.37% | Val 63.44%\n",
      "[NORMAL] Epoch 026 | Train 86.85% | Val 64.46%\n",
      "[NORMAL] Epoch 027 | Train 87.42% | Val 64.19%\n",
      "[NORMAL] Epoch 028 | Train 87.94% | Val 63.90%\n",
      "[NORMAL] Epoch 029 | Train 88.30% | Val 64.20%\n",
      "[NORMAL] Epoch 030 | Train 88.83% | Val 64.08%\n",
      "[NORMAL] Epoch 031 | Train 89.21% | Val 63.91%\n",
      "[NORMAL] Epoch 032 | Train 89.49% | Val 64.08%\n",
      "[NORMAL] Epoch 033 | Train 90.16% | Val 64.66%\n",
      "[NORMAL] Epoch 034 | Train 90.33% | Val 64.29%\n",
      "[NORMAL] Epoch 035 | Train 90.48% | Val 64.49%\n",
      "[NORMAL] Epoch 036 | Train 91.31% | Val 64.07%\n",
      "[NORMAL] Epoch 037 | Train 91.44% | Val 64.30%\n",
      "[NORMAL] Epoch 038 | Train 91.58% | Val 64.06%\n",
      "[NORMAL] Epoch 039 | Train 91.96% | Val 64.63%\n",
      "[NORMAL] Epoch 040 | Train 91.99% | Val 64.31%\n",
      "[NORMAL] Epoch 041 | Train 92.48% | Val 64.85%\n",
      "[NORMAL] Epoch 042 | Train 92.71% | Val 64.47%\n",
      "[NORMAL] Epoch 043 | Train 92.93% | Val 64.41%\n",
      "[NORMAL] Epoch 044 | Train 92.96% | Val 64.08%\n",
      "[NORMAL] Epoch 045 | Train 93.28% | Val 64.82%\n",
      "[NORMAL] Epoch 046 | Train 93.60% | Val 64.55%\n",
      "[NORMAL] Epoch 047 | Train 93.66% | Val 64.26%\n",
      "[NORMAL] Epoch 048 | Train 93.76% | Val 64.34%\n",
      "[NORMAL] Epoch 049 | Train 94.05% | Val 64.68%\n",
      "[NORMAL] Epoch 050 | Train 94.21% | Val 63.92%\n",
      "[NORMAL] Epoch 051 | Train 94.39% | Val 65.26%\n",
      "[NORMAL] Epoch 052 | Train 94.57% | Val 64.69%\n",
      "[NORMAL] Epoch 053 | Train 94.72% | Val 64.55%\n",
      "[NORMAL] Epoch 054 | Train 94.87% | Val 64.55%\n",
      "[NORMAL] Epoch 055 | Train 95.11% | Val 64.68%\n",
      "[NORMAL] Epoch 056 | Train 95.32% | Val 64.35%\n",
      "[NORMAL] Epoch 057 | Train 95.31% | Val 64.55%\n",
      "[NORMAL] Epoch 058 | Train 95.52% | Val 65.27%\n",
      "[NORMAL] Epoch 059 | Train 95.37% | Val 64.77%\n",
      "[NORMAL] Epoch 060 | Train 95.77% | Val 64.67%\n",
      "[NORMAL] Epoch 061 | Train 95.93% | Val 65.03%\n",
      "[NORMAL] Epoch 062 | Train 95.91% | Val 65.26%\n",
      "[NORMAL] Epoch 063 | Train 96.05% | Val 65.05%\n",
      "[NORMAL] Epoch 064 | Train 96.19% | Val 65.54%\n",
      "[NORMAL] Epoch 065 | Train 96.30% | Val 64.88%\n",
      "[NORMAL] Epoch 066 | Train 96.38% | Val 65.45%\n",
      "[NORMAL] Epoch 067 | Train 96.68% | Val 65.01%\n",
      "[NORMAL] Epoch 068 | Train 96.65% | Val 65.04%\n",
      "[NORMAL] Epoch 069 | Train 96.79% | Val 65.66%\n",
      "[NORMAL] Epoch 070 | Train 96.91% | Val 65.39%\n",
      "[NORMAL] Epoch 071 | Train 96.93% | Val 65.23%\n",
      "[NORMAL] Epoch 072 | Train 96.97% | Val 65.00%\n",
      "[NORMAL] Epoch 073 | Train 97.14% | Val 65.19%\n",
      "[NORMAL] Epoch 074 | Train 97.16% | Val 65.43%\n",
      "[NORMAL] Epoch 075 | Train 97.11% | Val 65.30%\n",
      "[NORMAL] Epoch 076 | Train 97.44% | Val 65.34%\n",
      "[NORMAL] Epoch 077 | Train 97.40% | Val 65.54%\n",
      "[NORMAL] Epoch 078 | Train 97.46% | Val 64.97%\n",
      "[NORMAL] Epoch 079 | Train 97.71% | Val 65.73%\n",
      "[NORMAL] Epoch 080 | Train 97.66% | Val 65.63%\n",
      "[NORMAL] Epoch 081 | Train 97.71% | Val 65.34%\n",
      "[NORMAL] Epoch 082 | Train 97.77% | Val 65.62%\n",
      "[NORMAL] Epoch 083 | Train 97.82% | Val 65.52%\n",
      "[NORMAL] Epoch 084 | Train 97.95% | Val 65.52%\n",
      "[NORMAL] Epoch 085 | Train 98.02% | Val 65.39%\n",
      "[NORMAL] Epoch 086 | Train 98.07% | Val 65.36%\n",
      "[NORMAL] Epoch 087 | Train 98.14% | Val 65.32%\n",
      "[NORMAL] Epoch 088 | Train 98.20% | Val 65.61%\n",
      "[NORMAL] Epoch 089 | Train 98.32% | Val 65.72%\n",
      "[NORMAL] Epoch 090 | Train 98.35% | Val 65.69%\n",
      "[NORMAL] Epoch 091 | Train 98.32% | Val 65.44%\n",
      "[NORMAL] Epoch 092 | Train 98.31% | Val 65.86%\n",
      "[NORMAL] Epoch 093 | Train 98.41% | Val 65.64%\n",
      "[NORMAL] Epoch 094 | Train 98.51% | Val 65.53%\n",
      "[NORMAL] Epoch 095 | Train 98.48% | Val 65.54%\n",
      "[NORMAL] Epoch 096 | Train 98.52% | Val 66.05%\n",
      "[NORMAL] Epoch 097 | Train 98.56% | Val 65.67%\n",
      "[NORMAL] Epoch 098 | Train 98.70% | Val 66.00%\n",
      "[NORMAL] Epoch 099 | Train 98.59% | Val 65.91%\n",
      "[NORMAL] Epoch 100 | Train 98.73% | Val 65.82%\n",
      "[NORMAL] Epoch 101 | Train 98.78% | Val 65.79%\n",
      "[NORMAL] Epoch 102 | Train 98.80% | Val 65.99%\n",
      "[NORMAL] Epoch 103 | Train 98.86% | Val 65.73%\n",
      "[NORMAL] Epoch 104 | Train 98.89% | Val 65.86%\n",
      "[NORMAL] Epoch 105 | Train 98.92% | Val 65.97%\n",
      "[NORMAL] Epoch 106 | Train 99.01% | Val 66.02%\n",
      "[NORMAL] Epoch 107 | Train 98.96% | Val 65.84%\n",
      "[NORMAL] Epoch 108 | Train 99.09% | Val 65.95%\n",
      "[NORMAL] Epoch 109 | Train 99.02% | Val 66.10%\n",
      "[NORMAL] Epoch 110 | Train 99.00% | Val 66.08%\n",
      "[NORMAL] Epoch 111 | Train 99.09% | Val 66.19%\n",
      "[NORMAL] Epoch 112 | Train 99.17% | Val 66.23%\n",
      "[NORMAL] Epoch 113 | Train 99.09% | Val 66.55%\n",
      "[NORMAL] Epoch 114 | Train 99.18% | Val 66.28%\n",
      "[NORMAL] Epoch 115 | Train 99.20% | Val 66.10%\n",
      "[NORMAL] Epoch 116 | Train 99.20% | Val 66.20%\n",
      "[NORMAL] Epoch 117 | Train 99.16% | Val 66.39%\n",
      "[NORMAL] Epoch 118 | Train 99.19% | Val 66.22%\n",
      "[NORMAL] Epoch 119 | Train 99.28% | Val 66.11%\n",
      "[NORMAL] Epoch 120 | Train 99.31% | Val 66.17%\n",
      "[NORMAL] Epoch 121 | Train 99.27% | Val 66.32%\n",
      "[NORMAL] Epoch 122 | Train 99.32% | Val 66.30%\n",
      "[NORMAL] Epoch 123 | Train 99.37% | Val 66.31%\n",
      "[NORMAL] Epoch 124 | Train 99.26% | Val 66.06%\n",
      "[NORMAL] Epoch 125 | Train 99.35% | Val 66.36%\n",
      "[NORMAL] Epoch 126 | Train 99.40% | Val 66.26%\n",
      "[NORMAL] Epoch 127 | Train 99.38% | Val 66.37%\n",
      "[NORMAL] Epoch 128 | Train 99.41% | Val 66.35%\n",
      "[NORMAL] Epoch 129 | Train 99.40% | Val 66.48%\n",
      "[NORMAL] Epoch 130 | Train 99.41% | Val 66.42%\n",
      "[NORMAL] Epoch 131 | Train 99.45% | Val 66.34%\n",
      "[NORMAL] Epoch 132 | Train 99.45% | Val 66.33%\n",
      "[NORMAL] Epoch 133 | Train 99.42% | Val 66.36%\n",
      "[NORMAL] Epoch 134 | Train 99.40% | Val 66.44%\n",
      "[NORMAL] Epoch 135 | Train 99.39% | Val 66.43%\n",
      "[NORMAL] Epoch 136 | Train 99.42% | Val 66.52%\n",
      "[NORMAL] Epoch 137 | Train 99.41% | Val 66.56%\n",
      "[NORMAL] Epoch 138 | Train 99.44% | Val 66.54%\n",
      "[NORMAL] Epoch 139 | Train 99.51% | Val 66.56%\n",
      "[NORMAL] Epoch 140 | Train 99.53% | Val 66.63%\n",
      "[NORMAL] Epoch 141 | Train 99.48% | Val 66.47%\n",
      "[NORMAL] Epoch 142 | Train 99.51% | Val 66.36%\n",
      "[NORMAL] Epoch 143 | Train 99.46% | Val 66.53%\n",
      "[NORMAL] Epoch 144 | Train 99.41% | Val 66.26%\n",
      "[NORMAL] Epoch 145 | Train 99.47% | Val 66.44%\n",
      "[NORMAL] Epoch 146 | Train 99.49% | Val 66.57%\n",
      "[NORMAL] Epoch 147 | Train 99.48% | Val 66.41%\n",
      "[NORMAL] Epoch 148 | Train 99.49% | Val 66.26%\n",
      "[NORMAL] Epoch 149 | Train 99.56% | Val 66.40%\n",
      "[NORMAL] Epoch 150 | Train 99.44% | Val 66.13%\n",
      "âœ… NORMAL FT Best Acc = 66.63%\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3A: FINE-TUNIN G NORMAL (INDEPENDENT)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nðŸš€ Fine-tuning NORMAL (from pruned snapshot)\")\n",
    "\n",
    "device = config.device\n",
    "\n",
    "# âš ï¸ QUAN TRá»ŒNG: deepcopy\n",
    "normal_ft_model = copy.deepcopy(final_pruned_model).to(device)\n",
    "normal_ft_model.train()\n",
    "\n",
    "train_loader = old_pruner.train_loader\n",
    "test_loader  = old_pruner.test_loader\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    normal_ft_model.parameters(),\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=150\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc_normal = 0.0\n",
    "\n",
    "for epoch in range(150):\n",
    "    normal_ft_model.train()\n",
    "    correct, total, train_loss = 0, 0, 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = normal_ft_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(normal_ft_model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    normal_ft_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = normal_ft_model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct / total\n",
    "\n",
    "    print(f\"[NORMAL] Epoch {epoch+1:03d} | Train {train_acc:.2f}% | Val {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_acc_normal:\n",
    "        best_acc_normal = val_acc\n",
    "        torch.save({\n",
    "            'model': normal_ft_model.state_dict(),\n",
    "            'acc': best_acc_normal\n",
    "        }, \"./checkpoint/pruned_finetuned_normal.pth\")\n",
    "\n",
    "print(f\"âœ… NORMAL FT Best Acc = {best_acc_normal:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf91dd",
   "metadata": {},
   "source": [
    "#### KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a07765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Äang khá»Ÿi táº¡o Knowledge Distillation...\n",
      "âœ… Teacher loaded (Acc: 93.62%)\n",
      "âœ… Student (Pruned Model) ready.\n",
      "\n",
      "ðŸ”¥ Báº¯t Ä‘áº§u Training KD trong 150 epochs...\n",
      "Epoch 1/150 | Loss: 3.631 | Train Acc: 74.80% | Val Acc: 71.63%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 71.63%)\n",
      "Epoch 2/150 | Loss: 2.762 | Train Acc: 80.77% | Val Acc: 75.78%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 75.78%)\n",
      "Epoch 3/150 | Loss: 2.458 | Train Acc: 82.89% | Val Acc: 71.67%\n",
      "Epoch 4/150 | Loss: 2.170 | Train Acc: 84.93% | Val Acc: 80.99%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 80.99%)\n",
      "Epoch 5/150 | Loss: 2.053 | Train Acc: 85.58% | Val Acc: 81.85%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 81.85%)\n",
      "Epoch 6/150 | Loss: 1.885 | Train Acc: 86.88% | Val Acc: 81.83%\n",
      "Epoch 7/150 | Loss: 1.764 | Train Acc: 87.84% | Val Acc: 83.19%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 83.19%)\n",
      "Epoch 8/150 | Loss: 1.663 | Train Acc: 88.35% | Val Acc: 84.56%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 84.56%)\n",
      "Epoch 9/150 | Loss: 1.600 | Train Acc: 88.96% | Val Acc: 85.89%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 85.89%)\n",
      "Epoch 10/150 | Loss: 1.519 | Train Acc: 89.54% | Val Acc: 86.79%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 86.79%)\n",
      "Epoch 11/150 | Loss: 1.472 | Train Acc: 89.80% | Val Acc: 86.38%\n",
      "Epoch 12/150 | Loss: 1.404 | Train Acc: 90.30% | Val Acc: 86.46%\n",
      "Epoch 13/150 | Loss: 1.366 | Train Acc: 90.58% | Val Acc: 85.75%\n",
      "Epoch 14/150 | Loss: 1.304 | Train Acc: 91.05% | Val Acc: 87.20%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 87.20%)\n",
      "Epoch 15/150 | Loss: 1.277 | Train Acc: 91.21% | Val Acc: 86.86%\n",
      "Epoch 16/150 | Loss: 1.242 | Train Acc: 91.48% | Val Acc: 87.62%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 87.62%)\n",
      "Epoch 17/150 | Loss: 1.198 | Train Acc: 91.84% | Val Acc: 86.96%\n",
      "Epoch 18/150 | Loss: 1.158 | Train Acc: 92.04% | Val Acc: 88.47%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 88.47%)\n",
      "Epoch 19/150 | Loss: 1.124 | Train Acc: 92.22% | Val Acc: 88.21%\n",
      "Epoch 20/150 | Loss: 1.105 | Train Acc: 92.44% | Val Acc: 86.43%\n",
      "Epoch 21/150 | Loss: 1.056 | Train Acc: 92.82% | Val Acc: 88.17%\n",
      "Epoch 22/150 | Loss: 1.059 | Train Acc: 92.76% | Val Acc: 87.51%\n",
      "Epoch 23/150 | Loss: 1.037 | Train Acc: 92.83% | Val Acc: 88.49%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 88.49%)\n",
      "Epoch 24/150 | Loss: 1.009 | Train Acc: 93.15% | Val Acc: 88.34%\n",
      "Epoch 25/150 | Loss: 0.985 | Train Acc: 93.22% | Val Acc: 87.25%\n",
      "Epoch 26/150 | Loss: 0.961 | Train Acc: 93.43% | Val Acc: 88.76%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 88.76%)\n",
      "Epoch 27/150 | Loss: 0.936 | Train Acc: 93.63% | Val Acc: 88.25%\n",
      "Epoch 28/150 | Loss: 0.901 | Train Acc: 93.97% | Val Acc: 89.41%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 89.41%)\n",
      "Epoch 29/150 | Loss: 0.881 | Train Acc: 93.98% | Val Acc: 87.92%\n",
      "Epoch 30/150 | Loss: 0.874 | Train Acc: 94.11% | Val Acc: 87.05%\n",
      "Epoch 31/150 | Loss: 0.874 | Train Acc: 94.10% | Val Acc: 87.96%\n",
      "Epoch 32/150 | Loss: 0.871 | Train Acc: 94.02% | Val Acc: 86.76%\n",
      "Epoch 33/150 | Loss: 0.825 | Train Acc: 94.33% | Val Acc: 88.40%\n",
      "Epoch 34/150 | Loss: 0.826 | Train Acc: 94.40% | Val Acc: 89.22%\n",
      "Epoch 35/150 | Loss: 0.813 | Train Acc: 94.51% | Val Acc: 89.19%\n",
      "Epoch 36/150 | Loss: 0.787 | Train Acc: 94.69% | Val Acc: 89.64%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 89.64%)\n",
      "Epoch 37/150 | Loss: 0.773 | Train Acc: 94.70% | Val Acc: 88.52%\n",
      "Epoch 38/150 | Loss: 0.753 | Train Acc: 94.94% | Val Acc: 89.62%\n",
      "Epoch 39/150 | Loss: 0.744 | Train Acc: 95.07% | Val Acc: 90.14%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.14%)\n",
      "Epoch 40/150 | Loss: 0.736 | Train Acc: 95.12% | Val Acc: 88.12%\n",
      "Epoch 41/150 | Loss: 0.734 | Train Acc: 95.04% | Val Acc: 89.09%\n",
      "Epoch 42/150 | Loss: 0.714 | Train Acc: 95.18% | Val Acc: 89.23%\n",
      "Epoch 43/150 | Loss: 0.698 | Train Acc: 95.35% | Val Acc: 89.70%\n",
      "Epoch 44/150 | Loss: 0.687 | Train Acc: 95.39% | Val Acc: 89.78%\n",
      "Epoch 45/150 | Loss: 0.662 | Train Acc: 95.53% | Val Acc: 89.86%\n",
      "Epoch 46/150 | Loss: 0.673 | Train Acc: 95.48% | Val Acc: 89.09%\n",
      "Epoch 47/150 | Loss: 0.667 | Train Acc: 95.44% | Val Acc: 89.67%\n",
      "Epoch 48/150 | Loss: 0.633 | Train Acc: 95.86% | Val Acc: 89.81%\n",
      "Epoch 49/150 | Loss: 0.613 | Train Acc: 95.90% | Val Acc: 89.68%\n",
      "Epoch 50/150 | Loss: 0.622 | Train Acc: 95.84% | Val Acc: 89.26%\n",
      "Epoch 51/150 | Loss: 0.604 | Train Acc: 95.98% | Val Acc: 88.52%\n",
      "Epoch 52/150 | Loss: 0.593 | Train Acc: 96.05% | Val Acc: 88.67%\n",
      "Epoch 53/150 | Loss: 0.573 | Train Acc: 96.20% | Val Acc: 89.27%\n",
      "Epoch 54/150 | Loss: 0.566 | Train Acc: 96.25% | Val Acc: 90.32%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.32%)\n",
      "Epoch 55/150 | Loss: 0.564 | Train Acc: 96.24% | Val Acc: 89.96%\n",
      "Epoch 56/150 | Loss: 0.570 | Train Acc: 96.26% | Val Acc: 90.27%\n",
      "Epoch 57/150 | Loss: 0.547 | Train Acc: 96.35% | Val Acc: 90.69%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 90.69%)\n",
      "Epoch 58/150 | Loss: 0.533 | Train Acc: 96.50% | Val Acc: 90.32%\n",
      "Epoch 59/150 | Loss: 0.518 | Train Acc: 96.60% | Val Acc: 89.71%\n",
      "Epoch 60/150 | Loss: 0.510 | Train Acc: 96.68% | Val Acc: 90.20%\n",
      "Epoch 61/150 | Loss: 0.509 | Train Acc: 96.76% | Val Acc: 90.07%\n",
      "Epoch 62/150 | Loss: 0.496 | Train Acc: 96.81% | Val Acc: 90.29%\n",
      "Epoch 63/150 | Loss: 0.478 | Train Acc: 96.85% | Val Acc: 90.52%\n",
      "Epoch 64/150 | Loss: 0.482 | Train Acc: 96.88% | Val Acc: 89.72%\n",
      "Epoch 65/150 | Loss: 0.444 | Train Acc: 97.16% | Val Acc: 90.23%\n",
      "Epoch 66/150 | Loss: 0.456 | Train Acc: 97.02% | Val Acc: 91.08%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.08%)\n",
      "Epoch 67/150 | Loss: 0.457 | Train Acc: 97.04% | Val Acc: 89.54%\n",
      "Epoch 68/150 | Loss: 0.440 | Train Acc: 97.19% | Val Acc: 90.79%\n",
      "Epoch 69/150 | Loss: 0.432 | Train Acc: 97.22% | Val Acc: 90.29%\n",
      "Epoch 70/150 | Loss: 0.421 | Train Acc: 97.29% | Val Acc: 90.39%\n",
      "Epoch 71/150 | Loss: 0.406 | Train Acc: 97.44% | Val Acc: 90.84%\n",
      "Epoch 72/150 | Loss: 0.402 | Train Acc: 97.52% | Val Acc: 90.74%\n",
      "Epoch 73/150 | Loss: 0.387 | Train Acc: 97.57% | Val Acc: 91.02%\n",
      "Epoch 74/150 | Loss: 0.387 | Train Acc: 97.57% | Val Acc: 90.45%\n",
      "Epoch 75/150 | Loss: 0.375 | Train Acc: 97.63% | Val Acc: 90.67%\n",
      "Epoch 76/150 | Loss: 0.368 | Train Acc: 97.66% | Val Acc: 90.48%\n",
      "Epoch 77/150 | Loss: 0.378 | Train Acc: 97.62% | Val Acc: 90.71%\n",
      "Epoch 78/150 | Loss: 0.356 | Train Acc: 97.81% | Val Acc: 90.67%\n",
      "Epoch 79/150 | Loss: 0.341 | Train Acc: 97.84% | Val Acc: 90.02%\n",
      "Epoch 80/150 | Loss: 0.341 | Train Acc: 97.90% | Val Acc: 90.50%\n",
      "Epoch 81/150 | Loss: 0.321 | Train Acc: 98.06% | Val Acc: 90.66%\n",
      "Epoch 82/150 | Loss: 0.320 | Train Acc: 98.03% | Val Acc: 90.81%\n",
      "Epoch 83/150 | Loss: 0.316 | Train Acc: 98.08% | Val Acc: 90.92%\n",
      "Epoch 84/150 | Loss: 0.303 | Train Acc: 98.19% | Val Acc: 91.05%\n",
      "Epoch 85/150 | Loss: 0.302 | Train Acc: 98.19% | Val Acc: 90.61%\n",
      "Epoch 86/150 | Loss: 0.268 | Train Acc: 98.48% | Val Acc: 91.07%\n",
      "Epoch 87/150 | Loss: 0.277 | Train Acc: 98.35% | Val Acc: 91.01%\n",
      "Epoch 88/150 | Loss: 0.281 | Train Acc: 98.36% | Val Acc: 91.00%\n",
      "Epoch 89/150 | Loss: 0.264 | Train Acc: 98.50% | Val Acc: 90.35%\n",
      "Epoch 90/150 | Loss: 0.255 | Train Acc: 98.54% | Val Acc: 91.05%\n",
      "Epoch 91/150 | Loss: 0.249 | Train Acc: 98.58% | Val Acc: 91.14%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.14%)\n",
      "Epoch 92/150 | Loss: 0.248 | Train Acc: 98.60% | Val Acc: 91.05%\n",
      "Epoch 93/150 | Loss: 0.233 | Train Acc: 98.71% | Val Acc: 91.07%\n",
      "Epoch 94/150 | Loss: 0.227 | Train Acc: 98.73% | Val Acc: 91.26%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.26%)\n",
      "Epoch 95/150 | Loss: 0.224 | Train Acc: 98.77% | Val Acc: 91.28%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.28%)\n",
      "Epoch 96/150 | Loss: 0.211 | Train Acc: 98.86% | Val Acc: 91.40%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.40%)\n",
      "Epoch 97/150 | Loss: 0.209 | Train Acc: 98.85% | Val Acc: 91.08%\n",
      "Epoch 98/150 | Loss: 0.203 | Train Acc: 98.92% | Val Acc: 91.53%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.53%)\n",
      "Epoch 99/150 | Loss: 0.189 | Train Acc: 98.99% | Val Acc: 91.44%\n",
      "Epoch 100/150 | Loss: 0.188 | Train Acc: 99.04% | Val Acc: 91.81%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.81%)\n",
      "Epoch 101/150 | Loss: 0.183 | Train Acc: 99.08% | Val Acc: 91.65%\n",
      "Epoch 102/150 | Loss: 0.167 | Train Acc: 99.21% | Val Acc: 91.86%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.86%)\n",
      "Epoch 103/150 | Loss: 0.173 | Train Acc: 99.14% | Val Acc: 91.56%\n",
      "Epoch 104/150 | Loss: 0.163 | Train Acc: 99.23% | Val Acc: 91.58%\n",
      "Epoch 105/150 | Loss: 0.158 | Train Acc: 99.25% | Val Acc: 91.67%\n",
      "Epoch 106/150 | Loss: 0.157 | Train Acc: 99.28% | Val Acc: 91.50%\n",
      "Epoch 107/150 | Loss: 0.148 | Train Acc: 99.35% | Val Acc: 91.63%\n",
      "Epoch 108/150 | Loss: 0.141 | Train Acc: 99.41% | Val Acc: 91.95%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 91.95%)\n",
      "Epoch 109/150 | Loss: 0.129 | Train Acc: 99.47% | Val Acc: 91.63%\n",
      "Epoch 110/150 | Loss: 0.134 | Train Acc: 99.43% | Val Acc: 91.89%\n",
      "Epoch 111/150 | Loss: 0.123 | Train Acc: 99.51% | Val Acc: 92.00%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 92.00%)\n",
      "Epoch 112/150 | Loss: 0.117 | Train Acc: 99.57% | Val Acc: 91.62%\n",
      "Epoch 113/150 | Loss: 0.112 | Train Acc: 99.61% | Val Acc: 91.89%\n",
      "Epoch 114/150 | Loss: 0.114 | Train Acc: 99.60% | Val Acc: 92.33%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 92.33%)\n",
      "Epoch 115/150 | Loss: 0.108 | Train Acc: 99.62% | Val Acc: 92.18%\n",
      "Epoch 116/150 | Loss: 0.102 | Train Acc: 99.69% | Val Acc: 91.94%\n",
      "Epoch 117/150 | Loss: 0.108 | Train Acc: 99.64% | Val Acc: 92.06%\n",
      "Epoch 118/150 | Loss: 0.103 | Train Acc: 99.69% | Val Acc: 91.96%\n",
      "Epoch 119/150 | Loss: 0.103 | Train Acc: 99.67% | Val Acc: 92.04%\n",
      "Epoch 120/150 | Loss: 0.095 | Train Acc: 99.74% | Val Acc: 92.05%\n",
      "Epoch 121/150 | Loss: 0.097 | Train Acc: 99.70% | Val Acc: 92.32%\n",
      "Epoch 122/150 | Loss: 0.092 | Train Acc: 99.76% | Val Acc: 91.94%\n",
      "Epoch 123/150 | Loss: 0.089 | Train Acc: 99.79% | Val Acc: 92.19%\n",
      "Epoch 124/150 | Loss: 0.091 | Train Acc: 99.77% | Val Acc: 92.22%\n",
      "Epoch 125/150 | Loss: 0.085 | Train Acc: 99.81% | Val Acc: 92.22%\n",
      "Epoch 126/150 | Loss: 0.087 | Train Acc: 99.80% | Val Acc: 92.27%\n",
      "Epoch 127/150 | Loss: 0.081 | Train Acc: 99.83% | Val Acc: 92.23%\n",
      "Epoch 128/150 | Loss: 0.082 | Train Acc: 99.84% | Val Acc: 92.36%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 92.36%)\n",
      "Epoch 129/150 | Loss: 0.081 | Train Acc: 99.85% | Val Acc: 92.23%\n",
      "Epoch 130/150 | Loss: 0.079 | Train Acc: 99.86% | Val Acc: 92.36%\n",
      "Epoch 131/150 | Loss: 0.080 | Train Acc: 99.84% | Val Acc: 92.47%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 92.47%)\n",
      "Epoch 132/150 | Loss: 0.077 | Train Acc: 99.87% | Val Acc: 92.31%\n",
      "Epoch 133/150 | Loss: 0.080 | Train Acc: 99.85% | Val Acc: 92.44%\n",
      "Epoch 134/150 | Loss: 0.074 | Train Acc: 99.89% | Val Acc: 92.38%\n",
      "Epoch 135/150 | Loss: 0.078 | Train Acc: 99.86% | Val Acc: 92.47%\n",
      "Epoch 136/150 | Loss: 0.076 | Train Acc: 99.85% | Val Acc: 92.49%\n",
      "   ---> ðŸ’¾ Saving Best Student (Acc: 92.49%)\n",
      "Epoch 137/150 | Loss: 0.076 | Train Acc: 99.88% | Val Acc: 92.42%\n",
      "Epoch 138/150 | Loss: 0.075 | Train Acc: 99.88% | Val Acc: 92.35%\n",
      "Epoch 139/150 | Loss: 0.076 | Train Acc: 99.87% | Val Acc: 92.37%\n",
      "Epoch 140/150 | Loss: 0.075 | Train Acc: 99.87% | Val Acc: 92.48%\n",
      "Epoch 141/150 | Loss: 0.073 | Train Acc: 99.90% | Val Acc: 92.39%\n",
      "Epoch 142/150 | Loss: 0.074 | Train Acc: 99.89% | Val Acc: 92.47%\n",
      "Epoch 143/150 | Loss: 0.075 | Train Acc: 99.89% | Val Acc: 92.47%\n",
      "Epoch 144/150 | Loss: 0.072 | Train Acc: 99.92% | Val Acc: 92.44%\n",
      "Epoch 145/150 | Loss: 0.073 | Train Acc: 99.90% | Val Acc: 92.45%\n",
      "Epoch 146/150 | Loss: 0.073 | Train Acc: 99.90% | Val Acc: 92.45%\n",
      "Epoch 147/150 | Loss: 0.072 | Train Acc: 99.92% | Val Acc: 92.39%\n",
      "Epoch 148/150 | Loss: 0.073 | Train Acc: 99.89% | Val Acc: 92.47%\n",
      "Epoch 149/150 | Loss: 0.074 | Train Acc: 99.89% | Val Acc: 92.46%\n",
      "Epoch 150/150 | Loss: 0.077 | Train Acc: 99.87% | Val Acc: 92.42%\n",
      "\n",
      "ðŸŽ‰ HOÃ€N Táº¤T DISTILLATION! Káº¿t quáº£ tá»‘t nháº¥t: 92.49%\n",
      "File saved at: ./checkpoint/pruned_student_distilled.pth\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: FINE-TUNING Vá»šI KNOWLEDGE DISTILLATION (KD)\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# --- 1. Cáº¥u hÃ¬nh KD ---\n",
    "# Hyper-parameters\n",
    "KD_EPOCHS = 150           # Sá»‘ epoch train láº¡i (nÃªn train ká»¹)\n",
    "KD_LR = 0.01              # Learning rate (tháº¥p hÆ¡n train from scratch)\n",
    "KD_TEMP = 4.0             # Temperature (Ä‘á»™ má»m cá»§a xÃ¡c suáº¥t)\n",
    "KD_ALPHA = 0.9            # 90% há»c tá»« Teacher, 10% há»c tá»« Label tháº­t\n",
    "TEACHER_PATH = \"./checkpoint/vgg16_cifar10_baseline.pth\" # ÄÆ°á»ng dáº«n model gá»‘c\n",
    "# TEACHER_PATH = \"./checkpoint/vgg16_cifar100_baseline.pth\" # ÄÆ°á»ng dáº«n model gá»‘c\n",
    "\n",
    "# HÃ m Loss Distillation\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=4.0, alpha=0.9):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.T = temperature\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Loss 1: Hard Loss (Cross Entropy vá»›i nhÃ£n tháº­t)\n",
    "        hard_loss = F.cross_entropy(student_logits, labels)\n",
    "        \n",
    "        # Loss 2: Soft Loss (KL Divergence vá»›i Teacher)\n",
    "        soft_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits / self.T, dim=1),\n",
    "            F.softmax(teacher_logits / self.T, dim=1),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.T * self.T)\n",
    "        \n",
    "        return self.alpha * soft_loss + (1. - self.alpha) * hard_loss\n",
    "\n",
    "# --- 2. Chuáº©n bá»‹ Teacher & Student ---\n",
    "print(\"ðŸš€ Äang khá»Ÿi táº¡o Knowledge Distillation...\")\n",
    "\n",
    "# A. Load Teacher (MÃ´ hÃ¬nh gá»‘c chÆ°a cáº¯t)\n",
    "if not os.path.exists(TEACHER_PATH):\n",
    "    raise FileNotFoundError(f\"âŒ KhÃ´ng tÃ¬m tháº¥y file checkpoint Teacher táº¡i: {TEACHER_PATH}\")\n",
    "\n",
    "teacher_model = ModifiedVGG16Model(config).to(config.device)\n",
    "ckpt_t = torch.load(TEACHER_PATH, map_location=config.device)\n",
    "teacher_model.load_state_dict(ckpt_t['model'])\n",
    "teacher_model.eval() # Teacher chá»‰ dáº¡y, khÃ´ng há»c\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False # ÄÃ³ng bÄƒng Teacher\n",
    "\n",
    "print(f\"âœ… Teacher loaded (Acc: {ckpt_t.get('acc', 'N/A')}%)\")\n",
    "\n",
    "# B. Student (LÃ  model Ä‘ang náº±m trong bá»™ nhá»› sau khi cháº¡y Cell 3)\n",
    "# LÆ°u Ã½: Biáº¿n 'model' á»Ÿ Ä‘Ã¢y lÃ  model Ä‘Ã£ bá»‹ cáº¯t tá»‰a váº­t lÃ½ á»Ÿ cell trÆ°á»›c\n",
    "student_model = copy.deepcopy(final_pruned_model).to(config.device) \n",
    "student_model.train()\n",
    "print(\"âœ… Student (Pruned Model) ready.\")\n",
    "\n",
    "# --- 3. Thiáº¿t láº­p Training ---\n",
    "criterion_kd = DistillationLoss(temperature=KD_TEMP, alpha=KD_ALPHA)\n",
    "optimizer_kd = optim.SGD(student_model.parameters(), lr=KD_LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_kd = optim.lr_scheduler.CosineAnnealingLR(optimizer_kd, T_max=KD_EPOCHS)\n",
    "\n",
    "# Táº­n dá»¥ng Data Loader cÃ³ sáºµn tá»« old_pruner\n",
    "train_loader = old_pruner.train_loader\n",
    "test_loader = old_pruner.test_loader\n",
    "\n",
    "# --- 4. VÃ²ng láº·p Training KD ---\n",
    "best_acc_kd = 0\n",
    "save_name_kd = 'pruned_student_distilled.pth'\n",
    "# save_name_kd = 'pruned_student_distilled_100.pth'\n",
    "\n",
    "print(f\"\\nðŸ”¥ Báº¯t Ä‘áº§u Training KD trong {KD_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(KD_EPOCHS):\n",
    "    student_model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
    "        \n",
    "        optimizer_kd.zero_grad()\n",
    "        \n",
    "        # Forward Teacher (No grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(inputs)\n",
    "            \n",
    "        # Forward Student\n",
    "        student_logits = student_model(inputs)\n",
    "        \n",
    "        # TÃ­nh Loss tá»•ng há»£p\n",
    "        loss = criterion_kd(student_logits, teacher_logits, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Káº¹p gradient Ä‘á»ƒ trÃ¡nh lá»—i exploding (quan trá»ng cho máº¡ng Ä‘Ã£ cáº¯t)\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=5.0)\n",
    "        optimizer_kd.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = student_logits.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    # Update LR\n",
    "    scheduler_kd.step()\n",
    "    \n",
    "    # --- Test Student ---\n",
    "    student_model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
    "            outputs = student_model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    acc_test = 100. * test_correct / test_total\n",
    "    \n",
    "    # Log gá»n gÃ ng\n",
    "    print(f\"Epoch {epoch+1}/{KD_EPOCHS} | Loss: {train_loss/(batch_idx+1):.3f} | Train Acc: {100.*correct/total:.2f}% | Val Acc: {acc_test:.2f}%\")\n",
    "    \n",
    "    # LÆ°u Best Checkpoint\n",
    "    if acc_test > best_acc_kd:\n",
    "        best_acc_kd = acc_test\n",
    "        print(f\"   ---> ðŸ’¾ Saving Best Student (Acc: {best_acc_kd:.2f}%)\")\n",
    "        torch.save({\n",
    "            'model': student_model.state_dict(),\n",
    "            'acc': best_acc_kd,\n",
    "            'epoch': epoch\n",
    "        }, f'./checkpoint/{save_name_kd}')\n",
    "\n",
    "print(f\"\\nðŸŽ‰ HOÃ€N Táº¤T DISTILLATION! Káº¿t quáº£ tá»‘t nháº¥t: {best_acc_kd:.2f}%\")\n",
    "print(f\"File saved at: ./checkpoint/{save_name_kd}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
